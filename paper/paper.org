* Config :ignore:
#+LATEX_CLASS: two-side-article
#+OPTIONS:  H:3 num:t toc:nil title:nil author:nil date:nil
# #+CALL: config-latex-class() # TODO make this call function work
** LaTeX class :noexport:
#+NAME: config-latex-class
#+begin_src emacs-lisp :exports none  :results none
;; (setq org-latex-with-hyperref nil)
;; TODO remove this when not submitting as blind for reviewing
;; (set (make-local-variable 'org-latex-with-hyperref) nil)
;; ("latexmk -f -silent -output-directory=./tex %f \n cp ./tex/%b.pdf ./%b.pdf")
;; (setq org-latex-pdf-process "latexmk -f -silent -output-directory=./tex %f \n cp ./tex/%b.pdf ./%b.pdf")
(unless (boundp 'org-latex-classes)
  (setq org-latex-classes nil))
(add-to-list 'org-latex-classes
             '("two-side-article"
               "\\documentclass[twoside]{article}
    [NO-DEFAULT-PACKAGES]
    [PACKAGES]
    [EXTRA]
    \\newcommand{\\mboxparagraph}[1]{\\paragraph{#1}\\mbox{}\\\\}
    \\newcommand{\\mboxsubparagraph}[1]{\\subparagraph{#1}\\mbox{}\\\\}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")))
#+end_src
** Title & Authors :ignore:
#+BEGIN_EXPORT latex
% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
\runningtitle{Mode-constrained Model-based Reinforcement Learning via Gaussian Processes}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Scannell, Ek, Richards}

\twocolumn[
\aistatstitle{Mode-constrained Model-based Reinforcement Learning via \\ Gaussian Processes}
\aistatsauthor{ Aidan Scannell \And Carl Henrik Ek \And  Arthur Richards }
\aistatsaddress{ Aalto University \And  University of Cambridge \And University of Bristol } ]
#+END_EXPORT

** AISTATS Config :ignore:
# If your paper is accepted, change the options for the package
# aistats2022 as follows:
# #+LATEX_HEADER: \usepackage{aistats2023}
# #+LATEX_HEADER: \usepackage[accepted]{aistats-template/aistats2023}
# #+LATEX_HEADER: \usepackage{aistats-template/aistats2023}
# #+LATEX_HEADER: \usepackage[accepted]{aistats-template/aistats2023}
#+LATEX_HEADER: \usepackage[accepted]{aistats2023}

# If you set papersize explicitly, activate the following three lines:
# #+LATEX_HEADER: \special{papersize = 8.5in, 11in}
# #+LATEX_HEADER: \setlength{\pdfpageheight}{11in}
# #+LATEX_HEADER: \setlength{\pdfpagewidth}{8.5in}
# If you use natbib package, activate the following three lines:
# #+LATEX_HEADER: \usepackage[round]{natbib}

** Bibliography :noexport:
# General configuration.
# # #+latex_header: \usepackage[autocite=plain, backend=biber, doi=true, url=true, hyperref=true,uniquename=false, maxbibnames=99, maxcitenames=2, sortcites=true, style=authoryear-comp]{biblatex}
# # #+LATEX_HEADER: \usepackage[citestyle=authoryear-comp, maxcitenames=2, maxbibnames=99, doi=false, isbn=false, eprint=false, backend=bibtex, hyperref=true, url=false, natbib=true, style=authoryear-comp]{biblatex}
#+LATEX_HEADER: \usepackage[citestyle=authoryear-comp, maxcitenames=2, maxbibnames=99, doi=false, isbn=false, eprint=false, backend=bibtex, hyperref=true, url=false, natbib=true, style=authoryear-comp]{biblatex}
# # #+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/mendeley/library.bib}
# #+LATEX_HEADER: \addbibresource{~/Dropbox/org/ref/zotero-library.bib}
#+LATEX_HEADER: \addbibresource{zotero-library.bib}

# #+LATEX_HEADER: \renewcommand{\bibname}{References}
# #+LATEX_HEADER: \renewcommand{\bibsection}{\subsubsection*{\bibname}}

** My packages :ignore:noexport:
*** maths/general :ignore:
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{amsmath,amssymb,amsfonts}
#+LATEX_HEADER: \usepackage[makeroom]{cancel}
#+LATEX_HEADER: \usepackage[disable]{todonotes}
# #+LATEX_HEADER: \usepackage{todonotes}
*** Captions :ignore:
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \captionsetup[figure]{font=footnotesize}
*** Equation Definitions :ignore:
#+LATEX_HEADER: \usepackage{mathtools}
*** Algorithms :ignore:
#+LATEX_HEADER: \usepackage{algpseudocode}
#+LATEX_HEADER: \usepackage{algorithm}
*** TikZ :ignore:
#+LATEX_HEADER: \usepackage{tikz,pgfplots}

and bayesnet for graphical models
#+LATEX_HEADER: \usetikzlibrary{bayesnet}

#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \DeclareUnicodeCharacter{2212}{âˆ’}
#+LATEX_HEADER: \usepgfplotslibrary{groupplots,dateplot}
#+LATEX_HEADER: \usetikzlibrary{patterns,shapes.arrows}
#+LATEX_HEADER: \pgfplotsset{compat=newest}

*** fontawesome :ignore:
#+LATEX_HEADER: \usepackage{fontawesome5}
*** hyperref :ignore:
#+LATEX_HEADER: \usepackage[colorlinks=true,linkcolor=blue,allcolors=blue]{hyperref}
*** Cleverref :ignore:
#+latex_header: \usepackage[capitalise,nameinlink]{cleveref}
** Settings :ignore:

#+LATEX_HEADER: \newcommand{\defeq}{\vcentcolon=}
*** Create a Definition theorem :ignore:
#+LATEX_HEADER: \newtheorem{definition}{Definition}[section]
#+LATEX_HEADER: \newtheorem{assumption}{Assumption}[section]
#+LATEX_HEADER: \newtheorem{theorem}{Theorem}[section]
#+LATEX_HEADER: \newtheorem{lemma}{Lemma}[section]
# #+LATEX_HEADER: \newtheorem*{remark}{Remark}
*** Cleverref :ignore:
#+latex_header: \crefname{section}{Sec.}{Secs.}
#+latex_header: \crefname{algorithm}{Alg.}{Algs.}
#+latex_header: \crefname{appendix}{App.}{Apps.}
#+latex_header: \crefname{definition}{Def.}{Defs.}
#+latex_header: \crefname{table}{Tab.}{Tabs}
* Maths :ignore:
** Math Variables :noexport:
#+LATEX_HEADER: \DeclareMathOperator{\R}{\mathbb{R}}
#+LATEX_HEADER: \DeclareMathOperator{\E}{\mathbb{E}}
#+LATEX_HEADER: \DeclareMathOperator{\V}{\mathbb{V}}
#+LATEX_HEADER: \DeclareMathOperator{\K}{\mathbf{K}}

*** Num Data / Mode / State Dimension / Control Dimension (k, d, t/n)
#+LATEX_HEADER: \newcommand{\numData}{\ensuremath{t}}
# #+LATEX_HEADER: \newcommand{\numData}{\ensuremath{n}}
#+LATEX_HEADER: \newcommand{\numEpisodes}{\ensuremath{e}}
#+LATEX_HEADER: \newcommand{\numTimesteps}{\ensuremath{t}}
#+LATEX_HEADER: \newcommand{\numInd}{\ensuremath{m}}
#+LATEX_HEADER: \newcommand{\stateDim}{\ensuremath{d}}
#+LATEX_HEADER: \newcommand{\controlDim}{\ensuremath{f}}
#+LATEX_HEADER: \newcommand{\modeInd}{\ensuremath{k}}
#+LATEX_HEADER: \newcommand{\modeDesInd}{\ensuremath{\text{des}}}
#+LATEX_HEADER: \newcommand{\testInd}{\ensuremath{*}}
#+LATEX_HEADER: \newcommand{\NumData}{\ensuremath{\MakeUppercase{\numData}}}
#+LATEX_HEADER: \newcommand{\NumInd}{\ensuremath{\MakeUppercase{\numInd}}}
# #+LATEX_HEADER: \newcommand{\StateDim}{\ensuremath{\MakeUppercase{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\ControlDim}{\ensuremath{\MakeUppercase{\controlDim}}}
#+LATEX_HEADER: \newcommand{\StateDim}{\ensuremath{{D_x}}}
#+LATEX_HEADER: \newcommand{\ControlDim}{\ensuremath{{D_u}}}
#+LATEX_HEADER: \newcommand{\ModeInd}{\ensuremath{\MakeUppercase{\modeInd}}}
#+LATEX_HEADER: \newcommand{\NumEpisodes}{\MakeUppercase{\numEpisodes}}
#+LATEX_HEADER: \newcommand{\NumTimesteps}{\MakeUppercase{\numTimesteps}}

# Macros for single/all data notation
#+LATEX_HEADER: \newcommand{\singleData}[1]{\ensuremath{#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{\MakeUppercase{#1}}}
# #+LATEX_HEADER: \newcommand{\singleData}[1]{\ensuremath{#1_{\numData}}}
# #+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{#1}}
# #+LATEX_HEADER: \newcommand{\allData}[1]{\ensuremath{#1_{1:\NumData}}}
#+LATEX_HEADER: \newcommand{\singleOutputK}{\ensuremath{\mode{\singleOutput}}}

# Macros for data dimensions
# #+LATEX_HEADER: \newcommand{\singleDataDim}[1]{\ensuremath{#1_{\stateDim, \numData}}}
#+LATEX_HEADER: \newcommand{\singleDataDim}[1]{\ensuremath{_{\stateDim}#1_{\numData}}}
#+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{#1_{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{_{\stateDim}#1}}
# #+LATEX_HEADER: \newcommand{\singleDimi}[2]{\ensuremath{\tensor*[_{#2}]{#1}{}}}
# #+LATEX_HEADER: \newcommand{\singleDim}[1]{\ensuremath{\singleDimi{#1}{\stateDim}}}

# Macros for mode k notation
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1^{(\modeInd)}}}
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1^{\modeInd}}}
# #+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{\tensor*[^{\modeInd}]{#1}{}}}
#+LATEX_HEADER: \newcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
#+LATEX_HEADER: \newcommand{\modeDes}[1]{\ensuremath{#1^{\modeDesInd}}}

#+LATEX_HEADER: \newcommand{\singleDimiMode}[2]{\ensuremath{\tensor*[_#2^\modeInd]{#1}{}}}
#+LATEX_HEADER: \newcommand{\singleDimMode}[1]{\ensuremath{\singleDimiMode{#1}{\stateDim}}}
#+LATEX_HEADER: \newcommand{\singleDimModeData}[1]{\ensuremath{\tensor*[_\stateDim^\modeInd]{#1}{_\numData}}}

*** Data set
# Dataset/inputs/outputs
#+LATEX_HEADER: \newcommand{\state}{\ensuremath{\mathbf{x}}}
#+LATEX_HEADER: \newcommand{\control}{\ensuremath{\mathbf{u}}}
# #+LATEX_HEADER: \newcommand{\control}{\ensuremath{\mathbf{a}}}

#+LATEX_HEADER: \newcommand{\x}{\ensuremath{\mathbf{x}}}
#+LATEX_HEADER: \newcommand{\y}{\ensuremath{y}}
#+LATEX_HEADER: \newcommand{\dataset}{\ensuremath{\mathcal{D}}}

# Single/all input/output notation
#+LATEX_HEADER: \newcommand{\singleInput}{\ensuremath{\x_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\singleOutput}{\ensuremath{\singleData{\y}}}
#+LATEX_HEADER: \newcommand{\allInput}{\ensuremath{\allData{\x}}}
#+LATEX_HEADER: \newcommand{\allOutput}{\ensuremath{\MakeUppercase{\y}}}

# Single/all state/control notation
#+LATEX_HEADER: \newcommand{\singleState}{\ensuremath{\state_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\singleControl}{\ensuremath{\control_{\numData-1}}}
#+LATEX_HEADER: \newcommand{\allState}{\ensuremath{\allData{\state}}}
#+LATEX_HEADER: \newcommand{\allControl}{\ensuremath{\allData{\control}}}

*** Noise Vars
#+LATEX_HEADER: \newcommand{\noiseVar}{\ensuremath{\sigma}}
#+LATEX_HEADER: \newcommand{\noiseVarK}{\ensuremath{\mode{\noiseVar}}}
#+LATEX_HEADER: \newcommand{\noiseVarOneK}{\ensuremath{\singleDimiMode{\noiseVar}{1}}}
#+LATEX_HEADER: \newcommand{\noiseVarDK}{\ensuremath{\singleDimiMode{\noiseVar}{\StateDim}}}
#+LATEX_HEADER: \newcommand{\noiseVardK}{\ensuremath{\singleDimMode{\noiseVar}}}
# #+LATEX_HEADER: \newcommand{\noiseVarOneK}{\ensuremath{\noiseVarK_{1}}}
# #+LATEX_HEADER: \newcommand{\noiseVarDK}{\ensuremath{\noiseVarK_{\StateDim}}}
# #+LATEX_HEADER: \newcommand{\noiseVardK}{\ensuremath{\noiseVarK_{\stateDim}}}
# #+LATEX_HEADER: \newcommand{\noiseVardK2}{\ensuremath{\left(\noiseVardK\right)^2}}

*** Mode Indicator Variable
#+LATEX_HEADER: \newcommand{\modeVar}{\ensuremath{\alpha}}
#+LATEX_HEADER: \newcommand{\modeVarn}{\ensuremath{\singleData{\modeVar}}}
#+LATEX_HEADER: \newcommand{\ModeVar}{\ensuremath{\bm{\modeVar}}}
# #+LATEX_HEADER: \newcommand{\ModeVar}{\ensuremath{\allData{\bm{\modeVar}}}}
#+LATEX_HEADER: \newcommand{\modeVarK}{\ensuremath{\modeVarn=\modeInd}}
# #+LATEX_HEADER: \newcommand{\ModeVarK}{\ensuremath{\mode{\bm{\modeVar}}}}
#+LATEX_HEADER: \newcommand{\ModeVarK}{\ensuremath{\ModeVar_{\modeInd}}}

*** Gating Network New
# Function notation
#+LATEX_HEADER: \newcommand{\gatingFunc}{\ensuremath{h}}

# All inputs set/vector/tensor notation
#+LATEX_HEADER: \newcommand{\GatingFunc}{\ensuremath{\mathbf{\gatingFunc}}}

*** Experts New
# Function notation
#+LATEX_HEADER: \newcommand{\latentFunc}{\ensuremath{f}}
#+LATEX_HEADER: \newcommand{\LatentFunc}{\ensuremath{\mathbf{\latentFunc}}}

# Vector/Matrix/Tensor notation
#+LATEX_HEADER: \newcommand{\F}{\ensuremath{\MakeUppercase{\mathbf{\latentFunc}}}}

*** Params
#+LATEX_HEADER: \newcommand{\gatingParams}{\ensuremath{\bm\phi}}
#+LATEX_HEADER: \newcommand{\expertParams}{\ensuremath{\bm\theta}}
#+LATEX_HEADER: \newcommand{\gatingParamsK}{\ensuremath{\mode{\bm\phi}}}
#+LATEX_HEADER: \newcommand{\expertParamsK}{\ensuremath{\mode{\bm\theta}}}
*** Sparse GPs
**** Misc
#+LATEX_HEADER: \newcommand{\Z}{\ensuremath{\mathbf{Z}}}
*** Continuous
#+LATEX_HEADER: \newcommand{\derivative}[1]{\ensuremath{\dot{#1}}}
#+LATEX_HEADER: \newcommand{\stateDerivative}{\ensuremath{\derivative{\state}}}
# #+LATEX_HEADER: \newcommand{\stateDerivative}{\ensuremath{\dot{\mathbf{x}}}}

*** Prob Dists :noexport:
#+LATEX_HEADER: \newcommand{\pFk}{\ensuremath{p\left(\Fk \mid \allInput, \expertParams\right)}}

#+LATEX_HEADER: \newcommand{\pF}{\ensuremath{p\left(\F \mid \allInput, \expertParams\right)}}
#+LATEX_HEADER: \newcommand{\pfk}{\ensuremath{p\left(\fk \mid \allInput, \expertParamsK \right)}}
#+LATEX_HEADER: \newcommand{\pfknd}{\ensuremath{p\left(\fknd \mid \allInput\right)}}

#+LATEX_HEADER: \newcommand{\pFkGivenUk}{\ensuremath{p\left(\Fk \mid \uFk \right)}}
# #+LATEX_HEADER: \newcommand{\pYkGivenUk}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenFku}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \uFk \right)}}

#+LATEX_HEADER: \newcommand{\qF}{\ensuremath{q\left(\F \right)}}
#+LATEX_HEADER: \newcommand{\qFu}{\ensuremath{q\left(\uF \right)}}
#+LATEX_HEADER: \newcommand{\qFku}{\ensuremath{q\left(\uFk \right)}}
#+LATEX_HEADER: \newcommand{\pFku}{\ensuremath{p\left(\uFk \mid \zFk \right)}}
#+LATEX_HEADER: \newcommand{\pFkuGivenX}{\ensuremath{p\left(\uFk \mid \zFk \right)}}
#+LATEX_HEADER: \newcommand{\pFuGivenX}{\ensuremath{p\left(\uF \mid \zF \right)}}
#+LATEX_HEADER: \newcommand{\qFk}{\ensuremath{q\left(\Fk \right)}}
#+LATEX_HEADER: \newcommand{\qfk}{\ensuremath{q\left(\fk \right)}}
#+LATEX_HEADER: \newcommand{\qfkn}{\ensuremath{q\left(\fkn \right)}}
#+LATEX_HEADER: \newcommand{\qfn}{\ensuremath{q\left(\fn \right)}}
#+LATEX_HEADER: \newcommand{\pFkGivenFku}{\ensuremath{p\left(\Fk \mid \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pfkGivenFku}{\ensuremath{p\left(\fkn \mid \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenFku}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \uFk \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenUX}{\ensuremath{p\left(\allOutput \mid \uF, \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenU}{\ensuremath{p\left(\allOutput \mid \uF \right)}}


#+LATEX_HEADER: \newcommand{\pY}{\ensuremath{p\left(\allOutput \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenx}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenxNegF}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \singleInput, \neg\Fk \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenfk}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \fkn \right)}}
#+LATEX_HEADER: \newcommand{\pykGivenfkd}{\ensuremath{p\left(\singleOutput \mid \modeVarK, \fknd \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenFk}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \Fk \right)}}
#+LATEX_HEADER: \newcommand{\pYkGivenX}{\ensuremath{p\left(\allOutput \mid \ModeVarK, \allInput \right)}}
#+LATEX_HEADER: \newcommand{\pYGivenX}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

**** Gating network
#+LATEX_HEADER: \newcommand{\PrA}{\ensuremath{\Pr\left(\ModeVarK \right)}}
#+LATEX_HEADER: \newcommand{\Pra}{\ensuremath{\Pr\left(\modeVarK \right)}}
#+LATEX_HEADER: \newcommand{\PaGivenhx}{\ensuremath{P\left(\modeVarn \mid \hn, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenx}{\ensuremath{\Pr\left(\modeVarn \mid \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenhx}{\ensuremath{\Pr\left(\modeVarK \mid \hn, \singleInput \right)}}
#+LATEX_HEADER: \newcommand{\PraGivenxNegH}{\ensuremath{\Pr\left(\modeVarK \mid \singleInput, \neg\Hall \right)}}
#+LATEX_HEADER: \newcommand{\PrAGivenX}{\ensuremath{\Pr\left(\ModeVarK \mid \allInput \right)}}

#+LATEX_HEADER: \newcommand{\pHGivenX}{\ensuremath{p\left(\Hall \mid \allInput\right)}}
#+LATEX_HEADER: \newcommand{\pHkGivenX}{\ensuremath{p\left(\Hk \mid \allInput\right)}}

*** Kernels
# #+LATEX_HEADER: \newcommand{\Kkxx}{\mode{\mathbf{K}}_{\allInput\allInput}}
#+LATEX_HEADER: \newcommand{\Kkxx}{\mode{\mathbf{K}}_{d, \allInput\allInput}}

# TO derivative kernels
#+LATEX_HEADER: \newcommand{\ddK}{\ensuremath{\partial^2\K_{**}}}
#+LATEX_HEADER: \newcommand{\dK}{\ensuremath{\partial\K_{*}}}
#+LATEX_HEADER: \newcommand{\Kxx}{\ensuremath{\K_{}}}
#+LATEX_HEADER: \newcommand{\iKxx}{\ensuremath{\Kxx^{-1}}}

#+LATEX_HEADER: \newcommand{\dKz}{\ensuremath{\partial\K_{*\zH}}}
#+LATEX_HEADER: \newcommand{\Kzz}{\ensuremath{\K_{\zH\zH}}}
#+LATEX_HEADER: \newcommand{\iKzz}{\ensuremath{\Kzz^{-1}}}
*** Desired Mode
# Function notation
#+LATEX_HEADER: \newcommand{\HDes}{\ensuremath{\MDes{\GatingFunc}}}
#+LATEX_HEADER: \newcommand{\uHDes}{\ensuremath{\MDes{\uH}}}

# Inducing points
#+LATEX_HEADER: \newcommand{\pDes}{\ensuremath{p\left( \uHDes \mid \zHDes \right)}}
#+LATEX_HEADER: \newcommand{\qDes}{\ensuremath{q\left( \uHDes \right)}}
#+LATEX_HEADER: \newcommand{\mDes}{\ensuremath{\MDes{\mathbf{m}}}}
#+LATEX_HEADER: \newcommand{\SDes}{\ensuremath{\MDes{\mathbf{S}}}}

*** Jacobian
# Single data notation
#+LATEX_HEADER: \newcommand{\singleTest}[1]{\ensuremath{#1_{\testInd}}}
#+LATEX_HEADER: \newcommand{\testInput}{\ensuremath{\singleTest{\state}}}

# Jacobian notation
#+LATEX_HEADER: \newcommand{\Jac}{\ensuremath{\mathbf{J}}}
#+LATEX_HEADER: \newcommand{\testJac}{\ensuremath{\singleTest{\Jac}}}
#+LATEX_HEADER: \newcommand{\muJac}{\ensuremath{\mu_{\Jac}}}
#+LATEX_HEADER: \newcommand{\covJac}{\ensuremath{\Sigma_{\Jac}}}

*** Old :noexport:
**** Gating Network Old
# # Function notation
# #+LATEX_HEADER: \newcommand{\gatingFunc}{\ensuremath{h}}
# #+LATEX_HEADER: \newcommand{\hk}{\ensuremath{\mode{\gatingFunc}}}

# # Single data notation
# #+LATEX_HEADER: \newcommand{\hkn}{\ensuremath{\singleData{\hk}}}
# #+LATEX_HEADER: \newcommand{\hn}{\ensuremath{\singleData{\mathbf{\gatingFunc}}}}

# # All inputs set/vector/tensor notation
# #+LATEX_HEADER: \newcommand{\GatingFunc}{\ensuremath{\mathbf{\gatingFunc}}}
# #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\GatingFunc}}
# #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\mode{\GatingFunc}}}
# # #+LATEX_HEADER: \newcommand{\Hall}{\ensuremath{\allData{\GatingFunc}}}
# # #+LATEX_HEADER: \newcommand{\Hk}{\ensuremath{\allData{\mode{\GatingFunc}}}}
**** Desired Mode Old
# #+LATEX_HEADER: \newcommand{\HDes}{\ensuremath{\modeDes{\GatingFunc}}}
# #+LATEX_HEADER: \newcommand{\HuDes}{\ensuremath{\modeDes{\Hu}}}
# #+LATEX_HEADER: \newcommand{\mDes}{\ensuremath{\modeDes{\mathbf{m}}}}
# #+LATEX_HEADER: \newcommand{\SDes}{\ensuremath{\modeDes{\mathbf{S}}}}

**** Experts Old
# # Function notation
# #+LATEX_HEADER: \newcommand{\latentFunc}{\ensuremath{f}}
# #+LATEX_HEADER: \newcommand{\f}{\ensuremath{f}}
# #+LATEX_HEADER: \newcommand{\fk}{\ensuremath{\mode{\latentFunc}}}
# # #+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\singleDim{\fk}}}
# #+LATEX_HEADER: \newcommand{\fkd}{\ensuremath{\singleDimMode{\f}}}

# # Single input notation
# #+LATEX_HEADER: \newcommand{\fn}{\ensuremath{\singleData{\mathbf{\latentFunc}}}}
# #+LATEX_HEADER: \newcommand{\fkn}{\ensuremath{\singleData{\mode{\mathbf{\latentFunc}}}}}
# # #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDim{\singleData{\fk}}}}
# # #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDimMode{\singleData{\f}}}}
# #+LATEX_HEADER: \newcommand{\fknd}{\ensuremath{\singleDimModeData{\f}}}

# # All inputs set/vector/tensor notation
# # #+LATEX_HEADER: \newcommand{\F}{\ensuremath{\allData{\mathbf{\f}}}}
# #+LATEX_HEADER: \newcommand{\F}{\ensuremath{\mathbf{\f}}}
# #+LATEX_HEADER: \newcommand{\Fk}{\ensuremath{\mode{\F}}}
# # #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\singleDim{\Fk}}}
# #+LATEX_HEADER: \newcommand{\Fkd}{\ensuremath{\singleDimMode{\F}}}

** Maths commands :ignore:
*** argmin/argmax :ignore:
#+LATEX_HEADER: \DeclareMathOperator*{\argmax}{argmax}
*** Maths diag :ignore:
#+LATEX_HEADER: \newcommand{\diag}{\mathop{\mathrm{diag}}}
** lit review maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\timeInd}{\ensuremath{t}}
\newcommand{\TimeInd}{\ensuremath{\MakeUppercase{\timeInd}}}

\renewcommand{\allInput}{\ensuremath{\hat{\state}_{1:\TimeInd}}}
\renewcommand{\allOutput}{\ensuremath{{\Delta\state}_{1:\TimeInd}}}


\newcommand{\dynamicsFunc}{\ensuremath{f}}

\newcommand{\costFunc}{\ensuremath{c}}
\newcommand{\terminalCostFunc}{\ensuremath{C_T}}
\newcommand{\integralCostFunc}{\ensuremath{C}}
\newcommand{\constraintsFunc}{\ensuremath{g}}

\newcommand{\stateTraj}{\ensuremath{\bar{\state}}}
\newcommand{\controlTraj}{\ensuremath{\bar{\control}}}

\newcommand{\policySpace}{\ensuremath{\Pi}}
\newcommand{\policy}{\ensuremath{\pi}}

\renewcommand{\u}{\ensuremath{\mathbf{u}}}
#+END_EXPORT

** Maths :ignore:noexport:
*** Domains :ignore:
#+BEGIN_EXPORT latex
\newcommand{\modeDomain}{\ensuremath{\mathcal{A}}}
%\renewcommand{\inputDomain}{\ensuremath{\mathcal{X}}}
%\newcommand{\inputDomain}{\ensuremath{\mathcal{X}}}

%\renewcommand{\state}{\ensuremath{\mathbf{s}}}
\renewcommand{\state}{\ensuremath{\mathbf{x}}}

%\renewcommand{\nominalDynamics}{\ensuremath{\mathbf{n}}}
\newcommand{\inputDim}{\ensuremath{d}}
\newcommand{\InputDim}{\ensuremath{D}}
#+END_EXPORT

*** Bounds :ignore:
#+BEGIN_EXPORT latex
\newcommand{\tightBound}{\ensuremath{\mathcal{L}_{\text{tight}}}}
\newcommand{\furtherBound}{\ensuremath{\mathcal{L}_{\text{further}}}}
\newcommand{\furtherBoundTwo}{\ensuremath{\mathcal{L}_{\text{further}^2}}}
#+END_EXPORT
*** maths :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
%\renewcommand{\singleOutput}{\ensuremath{\Delta x_{\numData}}}
%\renewcommand{\allOutput}{\ensuremath{\Delta\mathbf{\state}}}
\newcommand{\singleModeVar}{\ensuremath{\singleData{\modeVar}}}
\newcommand{\allModeVar}{\ensuremath{\bm{\modeVar}}}
\newcommand{\singleModeVarK}{\ensuremath{\singleModeVar = \modeInd}}
\newcommand{\allModeVarK}{\ensuremath{\bm{\modeVar}_{\modeInd}}}
%\newcommand{\allModeVarK}{\ensuremath{\{\singleModeVarK\}_{\numData=1}^\NumData}}
\newcommand{\modeVarnk}{\ensuremath{\modeVar_{\numData,\modeInd}}}

% new
\renewcommand{\numData}{\ensuremath{n}}
\renewcommand{\NumData}{\ensuremath{N}}
\renewcommand{\singleOutput}{\ensuremath{y_{\numData}}}
\renewcommand{\singleInput}{\ensuremath{\mathbf{x}_{\numData}}}
\renewcommand{\allInput}{\ensuremath{\mathbf{X}}}
\renewcommand{\allOutput}{\ensuremath{\mathbf{y}}}
\renewcommand{\allOutput}{\ensuremath{\mathbf{y}}}
%\renewcommand{\allInputK}{\ensuremath{\{\singleInput : \singleModeVarK \}}}
\renewcommand{\singleInputK}{\ensuremath{\mathbf{x}_{\numData, \modeInd}}}
\renewcommand{\allInputK}{\ensuremath{\mode{\allInput}}}
\newcommand{\allOutputK}{\ensuremath{\mode{\allOutput}}}

%\renewcommand{\x}{\ensuremath{\mathbf{z}}}
%\renewcommand{\y}{\ensuremath{y}}
%\renewcommand{\singleInput}{\ensuremath{\mathbf{z}_{\numData}}}
%\renewcommand{\allInput}{\ensuremath{\mathbf{Z}}}
%\renewcommand{\singleInputK}{\ensuremath{\mathbf{z}_{\numData, \modeInd}}}
%\renewcommand{\allInputK}{\ensuremath{\mode{\allInput}}}

%\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInput) \right)}}
\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInputK) \right)}}
\newcommand{\expertsPrior}{\ensuremath{p\left(\LatentFunc(\allInput) \right)}}
\newcommand{\expertMeanFunc}{\ensuremath{\mode{\mu}}}
\newcommand{\expertCovFunc}{\ensuremath{\mode{k}}}
\newcommand{\expertLikelihood}{\ensuremath{p\left(\allOutput \mid \mode{f}(\allInput)\right)}}
\newcommand{\singleExpertLikelihood}{\ensuremath{p(\singleOutput \mid \mode{f}(\singleInput))}}
%\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutput \mid \mode{f}(\allInput))}}
%\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutputK \mid \mode{f}(\allInputK))}}
\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutputK \mid \mode{f}(\allInputK))}}
\newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK, \allInput \right)}}
\newcommand{\singleExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \allInput \right)}}
% \newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK \right)}}

\newcommand{\gatingPrior}{\ensuremath{p\left(\GatingFunc(\allInput ) \right)}}
\newcommand{\gatingMeanFunc}{\ensuremath{\mode{\hat{\mu}}}}
\newcommand{\gatingCovFunc}{\ensuremath{\mode{\hat{k}}}}
\newcommand{\singleGatingLikelihood}{\ensuremath{\Pr\left(\singleModeVarK \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\allGatingLikelihood}{\ensuremath{\Pr\left(\allModeVarK \mid \GatingFunc(\allInput) \right)}}
\newcommand{\allGatingLikelihood}{\ensuremath{p\left(\allModeVar \mid \GatingFunc(\allInput) \right)}}
\newcommand{\gatingLikelihood}{\ensuremath{P\left(\singleModeVar \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \singleModeVar \mid \singleInput \right)}}
\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \allModeVarK \mid \allInput \right)}}
\newcommand{\singleGatingPosterior}{\ensuremath{\Pr\left( \singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\evidence}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

\newcommand{\moeExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \singleInput, \expertParamsK \right)}}
\newcommand{\moeGatingPosterior}{\ensuremath{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\moeEvidence}{\ensuremath{p\left(\allOutput \mid \allInput, \expertParams, \gatingParams \right)}}
\newcommand{\singleMoeEvidence}{\ensuremath{p\left(\singleOutput \mid \singleInput, \expertParams, \gatingParams \right)}}

\newcommand{\npmoeExpertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVar, \allInput, \expertParams \right)}}
\newcommand{\npmoeGatingPosterior}{\ensuremath{p\left(\allModeVar \mid \allInput, \gatingParams \right)}}

\newcommand{\moeLikelihood}{\ensuremath{p\left(\allOutput \mid \LatentFunc(\allInput), \GatingFunc (\allInput) \right)}}
\newcommand{\singleMoeLikelihood}{\ensuremath{p\left(\singleOutput \mid \mode{\latentFunc}(\allInput), \GatingFunc (\allInput) \right)}}

#+END_EXPORT
*** kernels :ignore:
#+BEGIN_EXPORT latex
%\renewcommand{\expertKernelnn}{\ensuremath{k_{\singleInput\singleInput}}}
%\renewcommand{\expertKernelnM}{\ensuremath{\mathbf{k}_{\singleInput \expertInducingInput}}}
%\renewcommand{\expertKernelMM}{\ensuremath{\mathbf{K}_{\expertInducingInput\expertInducingInput}}}
%\renewcommand{\expertKernelMn}{\ensuremath{\mathbf{k}_{\expertInducingInput \singleInput}}}
\renewcommand{\expertKernelnn}{\ensuremath{k_{\modeInd \numData \numData}}}
\renewcommand{\expertKernelNN}{\ensuremath{\mathbf{K}_{\modeInd \NumData \NumData}}}
\renewcommand{\expertKernelnM}{\ensuremath{\mathbf{k}_{\modeInd \numData \NumInducing}}}
\renewcommand{\expertKernelNM}{\ensuremath{\mathbf{K}_{\modeInd \NumData \NumInducing}}}
\renewcommand{\expertKernelMM}{\ensuremath{\mathbf{K}_{\modeInd \NumInducing \NumInducing}}}
\renewcommand{\expertKernelMn}{\ensuremath{\mathbf{k}_{\modeInd \NumInducing \numData}}}
\renewcommand{\expertKernelMN}{\ensuremath{\mathbf{K}_{\modeInd \NumInducing \NumData}}}
\renewcommand{\expertKernelsM}{\ensuremath{\mathbf{k}_{\modeInd * \NumInducing}}}
\renewcommand{\expertKernelss}{\ensuremath{k_{\modeInd **}}}
\renewcommand{\expertKernelSM}{\ensuremath{\mathbf{K}_{\modeInd * \NumInducing}}}
\renewcommand{\expertKernelSS}{\ensuremath{\mathbf{K}_{\modeInd **}}}

%\renewcommand{\gatingKernelnn}{\ensuremath{\hat{k}_{\singleInput\singleInput}}}
%\renewcommand{\gatingKernelnM}{\ensuremath{\hat{\mathbf{k}}_{\singleInput \gatingInducingInput}}}
%\renewcommand{\gatingKernelMM}{\ensuremath{\hat{\mathbf{K}}_{\gatingInducingInput\gatingInducingInput}}}
%\renewcommand{\gatingKernelMn}{\ensuremath{\hat{\mathbf{k}}_{\gatingInducingInput \singleInput}}}
\renewcommand{\gatingKernelnn}{\ensuremath{\hat{k}_{\modeInd \numData \numData}}}
\renewcommand{\gatingKernelNN}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumData \NumData}}}
\renewcommand{\gatingKernelnM}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \numData \NumInducing}}}
\renewcommand{\gatingKernelNM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumData \NumInducing}}}
\renewcommand{\gatingKernelMM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumInducing \NumInducing}}}
\renewcommand{\gatingKernelMn}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \NumInducing \numData}}}
\renewcommand{\gatingKernelss}{\ensuremath{\hat{k}_{\modeInd **}}}
\renewcommand{\gatingKernelsM}{\ensuremath{\hat{\mathbf{k}}_{\modeInd * \NumInducing}}}
\renewcommand{\gatingKernelMs}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \NumInducing *}}}
\renewcommand{\gatingKernelSM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd * \NumInducing}}}
\renewcommand{\gatingKernelMS}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumInducing *}}}
\renewcommand{\gatingKernelSS}{\ensuremath{\hat{\mathbf{K}}_{\modeInd **}}}

\renewcommand{\expertA}{\ensuremath{\mode{\mathbf{A}}}}
\renewcommand{\gatingA}{\ensuremath{\mode{\hat{\mathbf{A}}}}}
#+END_EXPORT

*** inference :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\renewcommand{\output}{\ensuremath{\Delta \state}}

\newcommand{\kernel}{\ensuremath{k}}
\newcommand{\expertKernel}{\ensuremath{\mode{\kernel}}}
\newcommand{\gatingKernel}{\ensuremath{\mode{\hat{\kernel}}}}

\newcommand{\numInducing}{\ensuremath{m}}
\newcommand{\NumInducing}{\ensuremath{\MakeUppercase{\numInducing}}}
%\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingInput}{\ensuremath{\bm{\zeta}}}
\newcommand{\inducingOutput}{\ensuremath{\mathbf{u}}}

%\newcommand{\expertInducingInput}{\ensuremath{\mode{\inducingInput}}}
%\newcommand{\expertsInducingInput}{\ensuremath{\inducingInput}}}
\newcommand{\expertInducingInput}{\ensuremath{\mode{\bm{\zeta}}}}
\newcommand{\expertsInducingInput}{\ensuremath{\bm{\zeta}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\inducingOutput}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\MakeUppercase{\inducingOutput}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\hat{\mathbf{\latentFunc}}}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\mathbf{\latentFunc}}}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\tilde{\mathbf{\latentFunc}}}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\tilde{\MakeUppercase{\mathbf{\latentFunc}}}}}
\newcommand{\expertInducingOutput}{\ensuremath{\mode{\latentFunc}(\expertInducingInput)}}
\newcommand{\expertsInducingOutput}{\ensuremath{\mathbf{\latentFunc}(\expertsInducingInput)}}

%\newcommand{\gatingInducingInput}{\ensuremath{\hat{\inducingInput}}}
\newcommand{\gatingInducingInput}{\ensuremath{\bm{\xi}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\hat{\inducingOutput}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\inducingOutput}}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\hat{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\tilde{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\tilde{\MakeUppercase{\mathbf{\gatingFunc}}}}}
\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\gatingFunc}(\gatingInducingInput)}}
\newcommand{\gatingsInducingOutput}{\ensuremath{\mathbf{\gatingFunc}(\gatingInducingInput)}}

%\newcommand{\expertInducingPrior}{\ensuremath{p(\mode{\latentFunc}(\expertInducingInput))}}
%\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput \mid \expertInducingInput)}}
%\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput \mid \expertsInducingInput)}}
\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
%\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertInducingVariational}{\ensuremath{q(\expertInducingOutput)}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\expertsInducingOutput)}}
\newcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput))}}
\newcommand{\expertsVariational}{\ensuremath{q(\LatentFunc(\singleInput))}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \expertInducingOutput)}}
\newcommand{\allExpertGivenInducing}{\ensuremath{p(\allOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \expertInducingOutput)}}
\newcommand{\allLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\allInput) \mid \expertInducingOutput)}}


%\newcommand{\gatingInducingPrior}{\ensuremath{p(\GatingFunc(\gatingInducingInput))}}
%\newcommand{\gatingInducingPrior}{\ensuremath{p(\gatingInducingOutput \mid \gatingInducingInput)}}
%\newcommand{\gatingsInducingPrior}{\ensuremath{p(\gatingsInducingOutput \mid \gatingInducingInput)}}
\newcommand{\gatingInducingPrior}{\ensuremath{p(\gatingInducingOutput)}}
\newcommand{\gatingsInducingPrior}{\ensuremath{p(\gatingsInducingOutput)}}
%\newcommand{\gatingInducingVariational}{\ensuremath{q(\GatingFunc(\gatingInducingInput))}}
\newcommand{\gatingInducingVariational}{\ensuremath{q(\gatingInducingOutput)}}
\newcommand{\gatingsInducingVariational}{\ensuremath{q(\gatingsInducingOutput)}}
\newcommand{\gatingsVariational}{\ensuremath{q(\GatingFunc(\singleInput))}}
\newcommand{\singleGatingGivenInducing}{\ensuremath{\Pr(\singleModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\allGatingGivenInducing}{\ensuremath{\Pr(\allModeVarK \mid \gatingInducingOutput)}}
\newcommand{\allGatingsGivenInducing}{\ensuremath{\Pr(\allModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingsGivenInducing}{\ensuremath{p(\GatingFunc(\singleInput) \mid \gatingsInducingOutput)}}
\newcommand{\allLatentGatingsGivenInducing}{\ensuremath{p(\GatingFunc(\allInput) \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingGivenInducing}{\ensuremath{p(\mode{\gatingFunc}(\singleInput) \mid \gatingInducingOutput)}}

\newcommand{\expertKL}{\ensuremath{\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\expertsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\gatingKL}{\ensuremath{\text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
\newcommand{\gatingsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd \text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
#+END_EXPORT
** Intro Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\targetState}{\ensuremath{\state_f}}

\newcommand{\nominalStateTraj}{\ensuremath{\stateTraj_*}}
\newcommand{\nominalControlTraj}{\ensuremath{\controlTraj_*}}
\newcommand{\fixedControl}{\ensuremath{\control_{*}}}
\newcommand{\velocity}{\ensuremath{v}}

\newcommand{\trajectory}{\ensuremath{\bar{\state}}}
\newcommand{\stateControlTraj}{\ensuremath{\bm\tau}}
\newcommand{\jacTraj}{\ensuremath{\bar{\mathbf{J}}}}
\newcommand{\modeVarTraj}{\ensuremath{\modeVar_{0:\TimeInd}=\desiredMode}}

\newcommand{\stateDiffTraj}{\ensuremath{\Delta\bar{\state}}}
\newcommand{\stateCol}{\ensuremath{\mathbf{z}}}

%\renewcommand{\modeInd}{\ensuremath{\modeVar}}

\newcommand{\desiredMode}{\ensuremath{\modeInd^{*}}}
\renewcommand{\modeDes}[1]{\ensuremath{#1_{\desiredMode}}}
\newcommand{\desiredGatingFunction}{\ensuremath{\modeDes{\gatingFunc}}}
%\newcommand{\desiredDynamicsFunc}{\ensuremath{\mode{\latentFunc}}}
\newcommand{\desiredDynamicsFunc}{\ensuremath{\modeDes{\latentFunc}}}
%\newcommand{\desiredDynamicsFunc}{\ensuremath{\latentFunc_{\modeVar_{\timeInd}}}}
\newcommand{\desiredStateDomain}{\ensuremath{\modeDes{\stateDomain}}}
%\newcommand{\desiredStateDomain}{\ensuremath{\mode{\stateDomain}}}

%\newcommand{\controlledDynamicsFunc}{\ensuremath{\modeDes{\latentFunc}}}
\newcommand{\controlledDynamicsFunc}{\ensuremath{\latentFunc_{\controlTraj}}}

\newcommand{\valueFunc}{\ensuremath{V}}

\newcommand{\controlledPolicyDist}{\ensuremath{q_\policy}}

\newcommand{\satisfactionProb}{\ensuremath{p_{\modeVar}}}
#+END_EXPORT
# *** Geometry Maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\manifold}{\ensuremath{\mathcal{M}}}
\newcommand{\manifoldFunction}{\ensuremath{h}}
\newcommand{\manifoldDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\manifoldCodomain}{\ensuremath{\mathcal{Z}}}
\newcommand{\ManifoldDim}{\ensuremath{D}}
\newcommand{\manifoldDim}{\ensuremath{d}}
\newcommand{\manifoldDomainDim}{\ensuremath{d_{\manifoldDomain}}}
\newcommand{\manifoldCodomainDim}{\ensuremath{d_{\manifoldCodomain}}}
\newcommand{\manifoldInput}{\ensuremath{\mathbf{x}}}

% \newcommand{\jacobian}{\ensuremath{\mathbf{J}_{\mathbf{x}_t}}}
\newcommand{\jacobian}{\ensuremath{\mathbf{J}(\state(t))}}
\newcommand{\metricTensor}{\ensuremath{\mathbf{G}}}
\newcommand{\metricTensorTraj}{\ensuremath{\bar{\mathbf{G}}}}

\newcommand{\geodesicFunction}{\ensuremath{f_G}}

%\newcommand{\gatingDomain}{\ensuremath{\hat{\mathcal{X}}}}
%\newcommand{\gatingCodomain}{\ensuremath{\mathcal{A}}}
\newcommand{\gatingDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\gatingCodomain}{\ensuremath{\mathcal{Z}}}

\newcommand{\desiredManifold}{\ensuremath{\mathcal{M}_{k^*}}}
%\newcommand{\desiredMetricTensor}{\ensuremath{\mathbf{G}_{k^*}}}
\newcommand{\desiredMetricTensor}{\ensuremath{\mathbf{G}}}
%\newcommand{\desiredJacobian}{\ensuremath{\mathbf{J}_{k^*}(\state(t))}}
\newcommand{\desiredJacobian}{\ensuremath{\mathbf{J}}}
%\newcommand{\GatingDim}{\ensuremath{D_{x+u}}}
\newcommand{\GatingDim}{\ensuremath{D}}
\newcommand{\gatingDim}{\ensuremath{d}}

% Manfiold kernels
\newcommand{\manifoldKernelMM}{\ensuremath{\mathbf{K}_{\NumInducing \NumInducing}}}
\newcommand{\jacManifoldKernelsM}{\ensuremath{\partial \mathbf{K}_{* \NumInducing}}}
\newcommand{\jacManifoldKernelMs}{\ensuremath{\partial \mathbf{K}_{\NumInducing *}}}
\newcommand{\hessManifoldKernel}{\ensuremath{\partial^2 \mathbf{K}_{**}}}
\newcommand{\manifoldKernelNN}{\ensuremath{\mathbf{K}_{\NumData \NumData}}}
\newcommand{\jacManifoldKernelsN}{\ensuremath{\partial \mathbf{K}_{* \NumData}}}
\newcommand{\jacManifoldKernelNs}{\ensuremath{\partial \mathbf{K}_{\NumData *}}}
\newcommand{\hessManifoldKerneldd}{\ensuremath{\partial^2 k(\cdot, \cdot')}}
\newcommand{\jacManifoldKerneldN}{\ensuremath{\partial \mathbf{K}_{\cdot \NumData}}}
\newcommand{\jacManifoldKernelNd}{\ensuremath{\partial \mathbf{K}_{\NumData \cdot}}}

\newcommand{\manifoldInducingInput}{\ensuremath{\bm\xi}}
%\newcommand{\manifoldInducingOutput}{\ensuremath{\mathbf{u}}}
\newcommand{\manifoldInducingOutput}{\ensuremath{\manifoldFunction(\manifoldInducingInput)}}
\newcommand{\manifoldInducingVariational}{\ensuremath{q(\mathbf{u})}}
\newcommand{\manifoldInducingOutputMean}{\ensuremath{\mathbf{m}}}
\newcommand{\manifoldInducingOutputCov}{\ensuremath{\mathbf{S}}}
\newcommand{\manifoldMeanFunc}{\ensuremath{\mu}}


%\newcommand{\manifoldFunc}{\ensuremath{\mathbf{h}}}
%\newcommand{\desiredMeanFunc}{\ensuremath{\mu}}
\renewcommand{\muJac}{\ensuremath{\bm\mu_{\mathbf{J}}}}
\renewcommand{\covJac}{\ensuremath{\bm\Sigma_{\mathbf{J}}}}
\renewcommand{\testInput}{\ensuremath{\mathbf{x}_*}}

\newcommand{\stateCostMatrix}{\ensuremath{\mathbf{Q}}}
\newcommand{\controlCostMatrix}{\ensuremath{\mathbf{R}}}
\newcommand{\terminalStateCostMatrix}{\ensuremath{\mathbf{H}}}
\newcommand{\approxExpectedCost}{\ensuremath{J(\stateTraj, \controlTraj)}}

\newcommand{\terminalState}{\ensuremath{\state_{\TimeInd}}}

\newcommand{\stateMean}{\ensuremath{\bm\mu_{\state_\timeInd}}}
\newcommand{\stateCov}{\ensuremath{\bm\Sigma_{\state_\timeInd}}}
\newcommand{\terminalStateMean}{\ensuremath{\bm\mu_{\state_\TimeInd}}}
\newcommand{\terminalStateCov}{\ensuremath{\bm\Sigma_{\state_\TimeInd}}}
\newcommand{\controlMean}{\ensuremath{\bm\mu_{\control_\timeInd}}}
\newcommand{\controlCov}{\ensuremath{\bm\Sigma_{\control_\timeInd}}}
\newcommand{\stateDiff}{\ensuremath{\Delta \state}}
\newcommand{\stateDiffMean}{\ensuremath{\bm\mu_{\stateDiff_\timeInd}}}
\newcommand{\stateDiffCov}{\ensuremath{\bm\Sigma_{\stateDiff_\timeInd}}}

\newcommand{\transitionDistK}{\ensuremath{p(\state_{\timeInd+1} \mid \state_\timeInd, \control_\timeInd, \modeVar_{\timeInd}=\modeInd)}}
#+END_EXPORT
** Background maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\gpDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\dynamicsModel}{\ensuremath{p_{\theta}}}
\newcommand{\constraintFunc}{\ensuremath{c}}
\newcommand{\safeSet}{\ensuremath{\mathcal{X}_{\text{feasible}}}}
#+END_EXPORT
** Prob inf maths :ignore:
*** Domains :ignore:
#+BEGIN_EXPORT latex
\newcommand{\stateDomain}{\ensuremath{\mathcal{S}}}
\newcommand{\controlDomain}{\ensuremath{\mathcal{A}}}
\newcommand{\modeDomain}{\ensuremath{\mathcal{A}}}
%\renewcommand{\inputDomain}{\ensuremath{\mathcal{X}}}
%\renewcommand{\inputDomain}{\ensuremath{\mathcal{X}}}

\renewcommand{\state}{\ensuremath{\mathbf{s}}}

\newcommand{\inputDim}{\ensuremath{d}}
\newcommand{\InputDim}{\ensuremath{D}}
#+END_EXPORT

*** maths :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\mode}[1]{\ensuremath{#1_{\modeInd}}}
\newcommand{\singleModeVar}{\ensuremath{\singleData{\modeVar}}}
\newcommand{\allModeVar}{\ensuremath{\bm{\modeVar}}}
\newcommand{\singleModeVarK}{\ensuremath{\singleModeVar = \modeInd}}
\newcommand{\allModeVarK}{\ensuremath{\bm{\modeVar}_{\modeInd}}}
%\newcommand{\allModeVarK}{\ensuremath{\{\singleModeVarK\}_{\numData=1}^\NumData}}
\newcommand{\modeVarnk}{\ensuremath{\modeVar_{\numData,\modeInd}}}

% new
\renewcommand{\numData}{\ensuremath{n}}
\renewcommand{\NumData}{\ensuremath{N}}
\renewcommand{\singleOutput}{\ensuremath{y_{\numData}}}
\renewcommand{\singleInput}{\ensuremath{\mathbf{x}_{\numData}}}
\renewcommand{\allInput}{\ensuremath{\mathbf{X}}}
\renewcommand{\allOutput}{\ensuremath{\mathbf{y}}}
\renewcommand{\allOutput}{\ensuremath{\mathbf{y}}}
%\renewcommand{\allInputK}{\ensuremath{\{\singleInput : \singleModeVarK \}}}
%\renewcommand{\allOutputK}{\ensuremath{\{\singleOutput : \singleModeVarK\}}}
%\renewcommand{\allInputK}{\ensuremath{\allInput^{\modeInd}}}
%\renewcommand{\allOutputK}{\ensuremath{\allOutput^{\modeInd}}}
\newcommand{\singleInputK}{\ensuremath{\mathbf{x}_{\numData, \modeInd}}}
\newcommand{\allInputK}{\ensuremath{\mode{\allInput}}}

%\renewcommand{\x}{\ensuremath{\mathbf{z}}}
%\renewcommand{\y}{\ensuremath{y}}
%\renewcommand{\singleInput}{\ensuremath{\mathbf{z}_{\numData}}}
%\renewcommand{\allInput}{\ensuremath{\mathbf{Z}}}
%\renewcommand{\singleInputK}{\ensuremath{\mathbf{z}_{\numData, \modeInd}}}
%\renewcommand{\allInputK}{\ensuremath{\mode{\allInput}}}

%\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInput) \right)}}
\newcommand{\expertPrior}{\ensuremath{p\left(\mode{f}(\allInputK) \right)}}
\newcommand{\expertsPrior}{\ensuremath{p\left(\LatentFunc(\allInput) \right)}}
\newcommand{\expertMeanFunc}{\ensuremath{\mode{\mu}}}
\newcommand{\expertCovFunc}{\ensuremath{\mode{k}}}
\newcommand{\expertLikelihood}{\ensuremath{p\left(\allOutput \mid \mode{f}(\allInput)\right)}}
\newcommand{\singleExpertLikelihood}{\ensuremath{p(\singleOutput \mid \mode{f}(\singleInput))}}
%\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutput \mid \mode{f}(\allInput))}}
%\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutputK \mid \mode{f}(\allInputK))}}
\newcommand{\allExpertLikelihood}{\ensuremath{p(\allOutputK \mid \mode{f}(\allInputK))}}
\newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK, \allInput \right)}}
\newcommand{\singleExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \allInput \right)}}
% \newcommand{\expertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVarK \right)}}

\newcommand{\gatingPrior}{\ensuremath{p\left(\GatingFunc(\allInput ) \right)}}
\newcommand{\gatingMeanFunc}{\ensuremath{\mode{\hat{\mu}}}}
\newcommand{\gatingCovFunc}{\ensuremath{\mode{\hat{k}}}}
\newcommand{\singleGatingLikelihood}{\ensuremath{\Pr\left(\singleModeVarK \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\allGatingLikelihood}{\ensuremath{\Pr\left(\allModeVarK \mid \GatingFunc(\allInput) \right)}}
\newcommand{\allGatingLikelihood}{\ensuremath{p\left(\allModeVar \mid \GatingFunc(\allInput) \right)}}
\newcommand{\gatingLikelihood}{\ensuremath{p\left(\singleModeVar \mid \GatingFunc(\singleInput) \right)}}
%\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \singleModeVar \mid \singleInput \right)}}
\newcommand{\gatingPosterior}{\ensuremath{\Pr\left( \allModeVarK \mid \allInput \right)}}
\newcommand{\singleGatingPosterior}{\ensuremath{\Pr\left( \singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\evidence}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}

\newcommand{\moeExpertPosterior}{\ensuremath{p\left(\singleOutput \mid \singleModeVarK, \singleInput, \expertParamsK \right)}}
\newcommand{\moeGatingPosterior}{\ensuremath{\Pr\left(\singleModeVarK \mid \singleInput, \gatingParams \right)}}
\newcommand{\moeEvidence}{\ensuremath{p\left(\allOutput \mid \allInput \right)}}
\newcommand{\singleMoeEvidence}{\ensuremath{p\left(\singleOutput \mid \singleInput, \expertParams, \gatingParams \right)}}

\newcommand{\npmoeExpertPosterior}{\ensuremath{p\left(\allOutput \mid \allModeVar, \allInput \right)}}
\newcommand{\npmoeGatingPosterior}{\ensuremath{p\left(\allModeVar \mid \allInput \right)}}

\newcommand{\moeLikelihood}{\ensuremath{p\left(\allOutput \mid \LatentFunc(\allInput), \GatingFunc (\allInput) \right)}}
\newcommand{\singleMoeLikelihood}{\ensuremath{p\left(\singleOutput \mid \mode{\latentFunc}(\allInput), \GatingFunc (\allInput) \right)}}

#+END_EXPORT
*** kernels :ignore:
#+BEGIN_EXPORT latex
%\renewcommand{\expertKernelnn}{\ensuremath{k_{\singleInput\singleInput}}}
%\renewcommand{\expertKernelnM}{\ensuremath{\mathbf{k}_{\singleInput \expertInducingInput}}}
%\renewcommand{\expertKernelMM}{\ensuremath{\mathbf{K}_{\expertInducingInput\expertInducingInput}}}
%\renewcommand{\expertKernelMn}{\ensuremath{\mathbf{k}_{\expertInducingInput \singleInput}}}
\newcommand{\expertKernelnn}{\ensuremath{k_{\modeInd \numData \numData}}}
\newcommand{\expertKernelNN}{\ensuremath{\mathbf{K}_{\modeInd \NumData \NumData}}}
\newcommand{\expertKernelnM}{\ensuremath{\mathbf{k}_{\modeInd \numData \NumInducing}}}
\newcommand{\expertKernelNM}{\ensuremath{\mathbf{K}_{\modeInd \NumData \NumInducing}}}
\newcommand{\expertKernelMM}{\ensuremath{\mathbf{K}_{\modeInd \NumInducing \NumInducing}}}
\newcommand{\expertKernelMn}{\ensuremath{\mathbf{k}_{\modeInd \NumInducing \numData}}}
\newcommand{\expertKernelMN}{\ensuremath{\mathbf{K}_{\modeInd \NumInducing \NumData}}}
\newcommand{\expertKernelsM}{\ensuremath{\mathbf{k}_{\modeInd * \NumInducing}}}
\newcommand{\expertKernelss}{\ensuremath{k_{\modeInd **}}}
\newcommand{\expertKernelSM}{\ensuremath{\mathbf{K}_{\modeInd * \NumInducing}}}
\newcommand{\expertKernelSS}{\ensuremath{\mathbf{K}_{\modeInd **}}}

%\renewcommand{\gatingKernelnn}{\ensuremath{\hat{k}_{\singleInput\singleInput}}}
%\renewcommand{\gatingKernelnM}{\ensuremath{\hat{\mathbf{k}}_{\singleInput \gatingInducingInput}}}
%\renewcommand{\gatingKernelMM}{\ensuremath{\hat{\mathbf{K}}_{\gatingInducingInput\gatingInducingInput}}}
%\renewcommand{\gatingKernelMn}{\ensuremath{\hat{\mathbf{k}}_{\gatingInducingInput \singleInput}}}
\newcommand{\gatingKernelnn}{\ensuremath{\hat{k}_{\modeInd \numData \numData}}}
\newcommand{\gatingKernelNN}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumData \NumData}}}
\newcommand{\gatingKernelnM}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \numData \NumInducing}}}
\newcommand{\gatingKernelNM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumData \NumInducing}}}
\newcommand{\gatingKernelMM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumInducing \NumInducing}}}
\newcommand{\gatingKernelMn}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \NumInducing \numData}}}
\newcommand{\gatingKernelss}{\ensuremath{\hat{k}_{\modeInd **}}}
\newcommand{\gatingKernelsM}{\ensuremath{\hat{\mathbf{k}}_{\modeInd * \NumInducing}}}
\newcommand{\gatingKernelMs}{\ensuremath{\hat{\mathbf{k}}_{\modeInd \NumInducing *}}}
\newcommand{\gatingKernelSM}{\ensuremath{\hat{\mathbf{K}}_{\modeInd * \NumInducing}}}
\newcommand{\gatingKernelMS}{\ensuremath{\hat{\mathbf{K}}_{\modeInd \NumInducing *}}}
\newcommand{\gatingKernelSS}{\ensuremath{\hat{\mathbf{K}}_{\modeInd **}}}

\newcommand{\expertA}{\ensuremath{\mode{\mathbf{A}}}}
\newcommand{\gatingA}{\ensuremath{\mode{\hat{\mathbf{A}}}}}
#+END_EXPORT

*** inference :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\renewcommand{\output}{\ensuremath{\Delta \state}}

\newcommand{\kernel}{\ensuremath{k}}
\newcommand{\expertKernel}{\ensuremath{\mode{\kernel}}}
\newcommand{\gatingKernel}{\ensuremath{\mode{\hat{\kernel}}}}

\newcommand{\numInducing}{\ensuremath{m}}
\newcommand{\NumInducing}{\ensuremath{\MakeUppercase{\numInducing}}}
%\newcommand{\inducingInput}{\ensuremath{\mathbf{Z}}}
\newcommand{\inducingInput}{\ensuremath{\bm{\zeta}}}
\newcommand{\inducingOutput}{\ensuremath{\mathbf{u}}}

%\newcommand{\expertInducingInput}{\ensuremath{\mode{\inducingInput}}}
%\newcommand{\expertsInducingInput}{\ensuremath{\inducingInput}}}
\newcommand{\expertInducingInput}{\ensuremath{\mode{\bm{\zeta}}}}
\newcommand{\expertsInducingInput}{\ensuremath{\bm{\zeta}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\inducingOutput}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\MakeUppercase{\inducingOutput}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\hat{\mathbf{\latentFunc}}}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\mathbf{\latentFunc}}}}}
%\newcommand{\expertInducingOutput}{\ensuremath{\mode{\tilde{\mathbf{\latentFunc}}}}}
%\newcommand{\expertsInducingOutput}{\ensuremath{\tilde{\MakeUppercase{\mathbf{\latentFunc}}}}}
\newcommand{\expertInducingOutput}{\ensuremath{\mode{\latentFunc}(\expertInducingInput)}}
\newcommand{\expertsInducingOutput}{\ensuremath{\mathbf{\latentFunc}(\expertsInducingInput)}}

%\newcommand{\gatingInducingInput}{\ensuremath{\hat{\inducingInput}}}
\newcommand{\gatingInducingInput}{\ensuremath{\bm{\xi}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\hat{\inducingOutput}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\inducingOutput}}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\hat{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\hat{\MakeUppercase{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\tilde{\mathbf{\gatingFunc}}}}}
%\newcommand{\gatingsInducingOutput}{\ensuremath{\tilde{\MakeUppercase{\mathbf{\gatingFunc}}}}}
\newcommand{\gatingInducingOutput}{\ensuremath{\mode{\gatingFunc}(\gatingInducingInput)}}
\newcommand{\gatingsInducingOutput}{\ensuremath{\mathbf{\gatingFunc}(\gatingInducingInput)}}

%\newcommand{\expertInducingPrior}{\ensuremath{p(\mode{\latentFunc}(\expertInducingInput))}}
%\newcommand{\expertInducingPrior}{\ensuremath{p(\expertInducingOutput \mid \expertInducingInput)}}
%\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput \mid \expertsInducingInput)}}
\newcommand{\expertsInducingPrior}{\ensuremath{p(\expertsInducingOutput)}}
\newcommand{\expertVariational}{\ensuremath{q(\mode{\latentFunc}(\singleInput))}}
%\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\allExpertGivenInducing}{\ensuremath{p(\allOutput \mid \expertInducingOutput)}}
%\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\singleInput) \mid \expertInducingOutput)}}
\newcommand{\allLatentExpertGivenInducing}{\ensuremath{p(\mode{\latentFunc}(\allInput) \mid \expertInducingOutput)}}


%\newcommand{\gatingInducingPrior}{\ensuremath{p(\GatingFunc(\gatingInducingInput))}}
%\newcommand{\gatingInducingPrior}{\ensuremath{p(\gatingInducingOutput \mid \gatingInducingInput)}}
%\newcommand{\gatingsInducingPrior}{\ensuremath{p(\gatingsInducingOutput \mid \gatingInducingInput)}}
\newcommand{\gatingInducingPrior}{\ensuremath{p(\gatingInducingOutput)}}
\newcommand{\gatingsInducingPrior}{\ensuremath{p(\gatingsInducingOutput)}}
%\newcommand{\gatingInducingVariational}{\ensuremath{q(\GatingFunc(\gatingInducingInput))}}
\newcommand{\gatingInducingVariational}{\ensuremath{q(\gatingInducingOutput)}}
\newcommand{\gatingsInducingVariational}{\ensuremath{q(\gatingsInducingOutput)}}
\newcommand{\gatingsVariational}{\ensuremath{q(\GatingFunc(\singleInput))}}
\newcommand{\singleGatingGivenInducing}{\ensuremath{\Pr(\singleModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\allGatingGivenInducing}{\ensuremath{\Pr(\allModeVarK \mid \gatingInducingOutput)}}
\newcommand{\allGatingsGivenInducing}{\ensuremath{\Pr(\allModeVarK \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingsGivenInducing}{\ensuremath{p(\GatingFunc(\singleInput) \mid \gatingsInducingOutput)}}
\newcommand{\allLatentGatingsGivenInducing}{\ensuremath{p(\GatingFunc(\allInput) \mid \gatingsInducingOutput)}}
\newcommand{\singleLatentGatingGivenInducing}{\ensuremath{p(\mode{\gatingFunc}(\singleInput) \mid \gatingInducingOutput)}}

\newcommand{\expertKL}{\ensuremath{\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\expertsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd\text{KL}\left( \expertInducingVariational \mid\mid \expertInducingPrior \right)}}
\newcommand{\gatingKL}{\ensuremath{\text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
\newcommand{\gatingsKL}{\ensuremath{\sum_{\modeInd=1}^\ModeInd \text{KL}\left( \gatingInducingVariational \mid\mid \gatingInducingPrior \right)}}
#+END_EXPORT
** Experiments maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\deltaTime}{\ensuremath{\Delta \timeInd}}
\newcommand{\env}[1]{\ensuremath{\hat{#1}}}
\newcommand{\modeProbTraj}{\ensuremath{\Pr(\allModeVarK \mid \stateTraj)}}

\newcommand{\windDrift}[1]{\ensuremath{\bm\omega_{#1}}}
\newcommand{\windTurbulence}[1]{\ensuremath{\bm\epsilon_{#1}}}
\newcommand{\windTurbulenceNoise}[1]{\ensuremath{\bm\Sigma_{\windTurbulence{#1}}}}
#+END_EXPORT
** Exploration maths :ignore:
#+BEGIN_EXPORT latex
\newcommand{\explorativeController}{\ensuremath{\pi_{\text{explore}}}}
\newcommand{\modeController}{\ensuremath{\pi_{\text{mode}}}}
\renewcommand{\dynamicsModel}{\ensuremath{p_{\theta}}}

\newcommand{\desiredGatingKernel}{\ensuremath{\hat{k}_{\desiredMode}}}
\newcommand{\desiredGatingVariance}{\ensuremath{\hat{\sigma}^2_{\timeInd}}}
\newcommand{\desiredGatingMean}{\ensuremath{\hat{\mu}_{\timeInd}}}
\newcommand{\desiredGatingCovFunc}{\ensuremath{\bm\Sigma^2_{\desiredMode}}}
\newcommand{\desiredGatingMeanFunc}{\ensuremath{\bm\mu_{\desiredMode}}}

\newcommand{\initialStateDomain}{\ensuremath{\stateDomain_0}}


\newcommand{\utraj}{\ensuremath{\bar{\control}}}
\newcommand{\policies}{\ensuremath{\Pi}}


\renewcommand{\dataset}{\ensuremath{\mathcal{D}}}
\renewcommand{\input}{\ensuremath{\hat{\state}}}
\renewcommand{\output}{\ensuremath{\modeVar}}
%\renewcommand{\inputDomain}{\ensuremath{\mathcal{X}}}
\newcommand{\outputDomain}{\ensuremath{\mathcal{A}}}

\newcommand{\outputGivenInputParams}{\ensuremath{p(\output \mid \input, \params)}}
\newcommand{\outputGivenInputData}{\ensuremath{p(\output \mid \input, \mathcal{D})}}
#+END_EXPORT
** override :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\control}{\ensuremath{\mathbf{a}}}
\newcommand{\action}{\ensuremath{\control}}

\renewcommand{\numData}{\ensuremath{t}}

%\renewcommand{\stateTraj}{\ensuremath{\state_{0:\TimeInd}}}

\newcommand{\allAction}{\ensuremath{\mathbf{A}}}
\renewcommand{\allState}{\ensuremath{\mathbf{S}}}


\renewcommand{\allInput}{\ensuremath{\hat{\mathbf{S}}}}
\renewcommand{\allInputK}{\ensuremath{\mode{\hat{\mathbf{S}}}}}
\renewcommand{\singleInput}{\ensuremath{\hat{\mathbf{s}}_{\timeInd}}}
\renewcommand{\singleOutput}{\ensuremath{\Delta\mathbf{s}_{\timeInd+1}}}

\renewcommand{\allOutput}{\ensuremath{\Delta\mathbf{S}}}
\newcommand{\allOutputK}{\ensuremath{\Delta\mode{\mathbf{S}}}}


\renewcommand{\controlDim}{\ensuremath{D_{a}}}


\renewcommand{\controlDim}{\ensuremath{D_{a}}}
\renewcommand{\stateDim}{\ensuremath{D_{s}}}

\newcommand{\rewardFunc}{\ensuremath{r}}
\newcommand{\dynamicsModelK}{\ensuremath{p_{\mode{\theta}}}}

\newcommand{\expertInducingPrior}{\ensuremath{p(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\singleExpertGivenInducing}{\ensuremath{p(\singleOutput \mid \mode{\latentFunc}(\expertInducingInput))}}

\newcommand{\expertInducingVariational}{\ensuremath{q(\mode{\latentFunc}(\expertInducingInput))}}
\newcommand{\expertsInducingVariational}{\ensuremath{q(\mathbf{\latentFunc}(\expertsInducingInput))}}

%\renewcommand{\singleGatingGivenInducing}{\ensuremath{\Pr(\modeVar_{\timeInd} =\modeInd \mid \mode{\gatingFunc}(\gatingInducingInput))}}
\renewcommand{\singleModeVar}{\ensuremath{\modeVar_{\timeInd}}}
%\singleGatingLikelihood

#+END_EXPORT

* Abstract :ignore:
# In the pursuit of safety and exploration guarantees for
# model-based RL algorithms it is common to only consider stationary environments so that
# Lipschitz continuity assumptions can be made on the dynamics function.
# \todo{is this true about exploration}
# This limits their general applicability.

# Our work extends the applicability of model-based RL to environments subject to multiple dynamic modes that vary over
# the state space.
#+BEGIN_EXPORT latex
\begin{abstract}
%Over the last decade, model-based RL has become a popular paradigm for controlling dynamical systems.
%Although recent algorithms can find high-performance controllers, they typically only consider stationary environments
%and do not perform well in environments subject to multiple dynamic modes.
%Our main contribution is a model-based RL algorithm that can solve tasks --
%For example, flying a quadcopter to a target state whilst avoiding turbulent dynamic modes induced by strong wind fields.
% and do not perform well in environments subject to multiple dynamic modes.
%-- whilst remaining in a desired dynamic modewith high probability.
%We present a model-based RL algorithm that can solve tasks --
%Most model-based RL algorithms do not consider environments subject to multiple dynamic modes.
%Moreover, they do not consider environments where it is beneficial to avoid \textit{inoperable or undesirable} dynamic modes.
%model-based RL algorithms do not typically consider environments -- subject to multiple dynamic modes --
%where it is beneficial to avoid \textit{inoperable or undesirable} dynamic modes.
%This is a particularly difficult problem because the mode constraint is \textit{unknown a priori}.
%We present a model-based RL algorithm that avoids entering such \textit{inoperable or undesirable} dynamic modes,
%by remaining in a single dynamic modewith high probability.
%We propose to jointly infer the mode constraint, along with the underlying dynamic modes, via a novel Mixtures of Gaussian Process Experts method.
%Importantly, our method infers latent structure that our planning scheme leverages to 1) enforce the mode constraint with high probability,
%and to 2) target exploration where the mode constraint's \textit{epistemic uncertainty} is high.
%We validate our method by showing that it can navigate a simulated quadcopter -- subject to a turbulent dynamic mode-- to a target state, whilst remaining in
%the desired dynamic modewith high probability.
%We provide a TensorFlow/GPflow implementation at \url{https://github.com/aidanscannell/ModeRL}.
% We present a MBRL algorithm that avoids entering such \textit{inoperable or undesirable} dynamic modes,
%by constraining exploration to a single dynamic mode with high probability.

%Model-based reinforcement learning (RL) algorithms do not typically consider environments -- subject to multiple dynamic modes -- where it is beneficial to avoid \textit{inoperable or undesirable} dynamic modes. To this end, we present a model-based RL algorithm that constrains training to a single dynamic mode with high probability. This is a particularly difficult problem because the mode constraint is a hidden variable associated with the environment's dynamics. As such, it is 1) \textit{unknown a priori} and 2) we do not observe its output from the environment, so cannot learn it with supervised learning. We present a nonparametric dynamic model which learns the mode constraint -- as a latent variable -- alongside the dynamic modes. Importantly, it learns latent structure that our planning scheme leverages to 1) enforce the mode constraint with high probability, and 2) escape local optima induced by the mode constraint. We validate our method by showing that it can navigate a simulated quadcopter -- subject to a turbulent dynamic mode -- to a target state, whilst remaining in the desired dynamic mode with high probability.

Model-based reinforcement learning (RL) algorithms do not typically consider environments with multiple dynamic modes, where it is beneficial to avoid inoperable or undesirable modes. We present a model-based RL algorithm that constrains training to a single dynamic mode with high probability. This is a difficult problem because the mode constraint is a hidden variable associated with the environment's dynamics. As such, it is 1) unknown a priori and 2) we do not observe its output from the environment, so cannot learn it with supervised learning. We present a nonparametric dynamic model which learns the mode constraint alongside the dynamic modes. Importantly, it learns latent structure that our planning scheme leverages to 1) enforce the mode constraint with high probability, and 2) escape local optima induced by the mode constraint. We validate our method by showing that it can solve a simulated quadcopter navigation task whilst providing a level of constraint satisfaction both during and after training.
% Model-based reinforcement learning (RL) algorithms do not typically consider environments with multiple dynamic modes, where it is beneficial to avoid inoperable or undesirable modes. We present a model-based RL algorithm that constrains training to a single dynamic mode with high probability. This is a difficult problem because the mode constraint is a hidden variable associated with the environment's dynamics. As such, it is 1) unknown a priori and 2) we do not observe its output from the environment, so cannot learn it with supervised learning. We present a nonparametric dynamic model which learns the mode constraint alongside the dynamic modes. Importantly, it learns latent structure that our planning scheme leverages to 1) enforce the mode constraint with high probability, and 2) escape local optima induced by the mode constraint. We validate our method by showing that it can solve a simulated quadcopter navigation task whilst constraining training to a single dynamic mode with high probability.

% We propose to jointly learn the mode constraint, along with the underlying dynamic modes.

%regarding the mode constraint to
%We overcome issues with constrained exploration by introducing a novel exploration mechanism which prevents
%Based on the observation that the mode constraint hinders exploration, we introduce a novel exploration mechanism which prevents
%We validate our method in a simulated quadcopter environment by showing that it can navigate to a target state whilst remaining in a desired dynamics
%mode with high probability.
%We provide a TensorFlow/GPflow implementation at \url{https://github.com/aidanscannell/ModeRL}.

%Further to this, we observe that the mode constraint hinders exploration and stops the quadcopter from navigating to the target state.
%We handle the latent mode constraint by jointly inferring the underlying dynamic modes and the latent mode constraint via
%a novel Mixtures of Gaussian Process Experts method built upon sparse approximations and a stochastic variational inference scheme.
%We overcome issues with constrained exploration by introducing a novel exploration mechanism which prevents
%Based on the observation that the mode constraint hinders exploration, we introduce a novel exploration mechanism which prevents
%We validate our method in a simulated quadcopter environment by showing that it can navigate to a target state whilst remaining in a desired dynamics
%mode with high probability.
%We provide a TensorFlow/GPflow implementation at \url{https://github.com/aidanscannell/ModeRL}.

%mode remaining planning algorithm leverages the latent structure of our dynamics
%model to explore the environment whilst simultaneously guaranteeing that the agent remains in the desired dynamic mode
%with high probability.
%Our mode remaining planning algorithm leverages the latent structure of our dynamics
%model to explore the environment whilst simultaneously guaranteeing that the agent remains in the desired dynamic mode
%with high probability.
%We validate our method in a simulated quadcopter environment
%and provide a TensorFlow/GPflow implementation at XXXX.

%We introduce a trajectory optimisation algorithm that leverages the latent structure of our dynamics
%We introduce a novel Mixtures of Gaussian Process Experts method built upon sparse approximations and stochastic variational inference and use it to
%infer both the underlying dynamic modes, as well as information regarding the latent mode constraint.



%The main contribution of this paper is a model-based RL algorithm that can solve tasks
%in environments subject to multiple dynamic modes, whilst \textit{avoiding inoperable or undesirable dynamic modes}.

%The main contribution of this paper is a model-based RL algorithm that can navigate an agent to a target state
%-- in environments subject to multiple dynamic modes -- whilst \textit{avoiding inoperable or undesirable dynamic modes}.
%For example, flying a quadcopter to a target state whilst avoiding turbulent dynamic modes induced by strong wind fields.
%We introduce a trajectory optimisation algorithm that leverages the latent structure of our dynamics
%model to explore the environment whilst simultaneously guaranteeing that the agent remains in the desired dynamic mode
%with high probability.
%We validate our method in a simulated quadcopter environment
%and provide a TensorFlow/GPflow implementation at \url{https://github.com/aidanscannell/ModeRL}.
%The main goal of this paper is to autonomously navigate an agent to a target state whilst
%\textit{avoiding inoperable or undesirable dynamic modes}.
%For example, flying a quadcopter to a target state whilst avoiding regions of the state
%space that are subject to turbulent dynamic modes induced by strong wind fields.
%Motivated by synergising model learning and control, we introduce a Mixtures of Gaussian process experts method for learning
%dynamic models, which infers latent structure regarding how systems switch between their underlying dynamic modes.
%Importantly,
%We introduce a trajectory optimisation algorithm that explores the environment whilst guaranteeing that the agent remains in
%the desired dynamic mode with high probability.
%We validate our method in a simulated quadcopter environment where the goal is to avoid entering regions of the state
%space with strong wind fields and thus turbulent dynamic modes.
%Our model-based RL algorithm is implemented in TensorFlow/GPflow and is available at \url{https://github.com/aidanscannell/ModeOpt}.
\end{abstract}
#+END_EXPORT
# We consider the model-based RL setting, where an explicit dynamic model -- that includes uncertainties -- is used
# to plan trajectories to a target state.
# We introduce an explorative trajectory optimisation algorithm that explicitly reasons about
# the uncertainties in the dynamic model.
# As a result, it can explore the environment whilst guaranteeing that the agent remains in
# the desired dynamic mode with high probability.

# Our work extends these methods to non-stationary environments subject to \textit{multimodal} dynamic modes,
# where some of the dynamic modes are believed to be \textit{inoperable or undesirable}.

# In particular, we introduce a model-based RL algorithms for  in environments
# subject to \textit{multimodal} dynamic modes, where some of the dynamic modes are believed to be
# \textit{inoperable or undesirable}.

# In this work, we are interested in environments where non-stationarity is induced by different dynamic modes
# in different regions of the state space.
# The main goal of our work is to control \textit{unknown},
# \textit{multimodal} dynamical systems, to a target state, whilst \textit{avoiding inoperable or undesirable dynamic modes}.

# Over the last decade, \textit{learning-based control} has become a popular paradigm for controlling dynamical systems.
# Although recent algorithms can find high-performance controllers, they typically only consider unimodal systems and
# cannot correctly identify multimodal dynamical systems.
# The main goal of our work is to control \textit{unknown},
# \textit{multimodal} dynamical systems, to a target state, whilst \textit{avoiding inoperable or undesirable dynamic modes}.
# Further to this, deploying learning algorithms in the real world requires handling the uncertainties
# inherent to the system, as well as the uncertainties arising from learning from observations.
# To this end, we consider the model-based RL setting, where an explicit dynamic model -- that includes uncertainties -- is used to plan trajectories to a target state.
# Motivated by synergising model learning and control, we introduce a Mixtures of Gaussian process experts method for learning
# dynamic models, which infers latent structure regarding how systems switch between their underlying dynamic modes.
# Initially, the agentâ€™s dynamic model will be highly \textit{uncertain} â€” due to a lack of training observations â€” so
# it is not possible find mode remaining trajectories to the target state with high confidence.
# When this is the case, our algorithm actively explores its environment, collects data and updates its dynamic model.
# We introduce an explorative trajectory optimisation algorithm that explicitly reasons about
# the uncertainties in the dynamic model.
# As a result, it can explore the environment whilst guaranteeing that the agent remains in
# the desired dynamic mode with high probability.
# We embed our mode remaining exploration algorithm into a model-based RL algorithm, which solves the mode
# remaining navigation problem,
# whilst guaranteeing that the controlled system remains in the desired dynamic mode with a high probability.
# We validate our method in a simulated quadcopter environment where the goal is to avoid entering regions of the state
# space with strong wind fields and thus turbulent dynamic modes.
# Our model-based RL algorithm is implemented in TensorFlow/GPflow and is available at \url{https://github.com/aidanscannell/ModeOpt}.

* Abstract :ignore:noexport:
# In the pursuit of safety and exploration guarantees for
# model-based RL algorithms it is common to only consider stationary environments so that
# Lipschitz continuity assumptions can be made on the dynamics function.
# \todo{is this true about exploration}
# This limits their general applicability.

# Our work extends the applicability of model-based RL to environments subject to multiple dynamic modes that vary over
# the state space.
#+BEGIN_EXPORT latex
\begin{abstract}
%Over the last decade, model-based RL has become a popular paradigm for controlling dynamical systems.
%Although recent algorithms can find high-performance controllers, they typically only consider stationary environments
%and do not perform well in environments subject to multiple dynamic modes.
%Our main contribution is a model-based RL algorithm that can solve tasks --
%For example, flying a quadcopter to a target state whilst avoiding turbulent dynamic modes induced by strong wind fields.
% and do not perform well in environments subject to multiple dynamic modes.
%-- whilst remaining in a desired dynamic mode with high probability.
%We present a model-based RL algorithm that can solve tasks --
%Most model-based RL algorithms do not consider environments subject to multiple dynamic modes.
%Moreover, they do not consider environments where it is beneficial to avoid \textit{inoperable or undesirable} dynamic modes.
%model-based RL algorithms do not typically consider environments -- subject to multiple dynamic modes --
%where it is beneficial to avoid \textit{inoperable or undesirable} dynamic modes.
%This is a particularly difficult problem because the mode constraint is \textit{unknown a priori}.
%We present a model-based RL algorithm that avoids entering such \textit{inoperable or undesirable} dynamic modes,
%by remaining in a single dynamic mode with high probability.
%We propose to jointly infer the mode constraint, along with the underlying dynamic modes, via a novel Mixtures of Gaussian Process Experts method.
%Importantly, our method infers latent structure that our planning scheme leverages to 1) enforce the mode constraint with high probability,
%and to 2) target exploration where the mode constraint's \textit{epistemic uncertainty} is high.
%We validate our method by showing that it can navigate a simulated quadcopter -- subject to a turbulent dynamic mode -- to a target state, whilst remaining in
%the desired dynamic mode with high probability.
%We provide a TensorFlow/GPflow implementation at \url{https://github.com/aidanscannell/ModeRL}.

Model-based reinforcement learning (MBRL) algorithms do not typically consider environments -- subject to multiple dynamic modes --
where it is beneficial to avoid \textit{inoperable or undesirable} dynamic modes.
We present a MBRL algorithm that avoids entering such \textit{inoperable or undesirable} dynamic modes,
by constraining the controlled system to remain in a single dynamic mode with high probability.
This is a particularly difficult problem because the mode constraint is \textit{unknown a priori}.
We propose to jointly infer the mode constraint, along with the underlying dynamic modes.
Importantly, our method infers latent structure that our planning scheme leverages to 1) enforce the mode constraint with high probability,
and to 2) escape the local optima induced by the mode constraint by
targeting exploration where the mode constraint's \textit{epistemic uncertainty} is high.
We validate our method by showing that it can navigate a simulated quadcopter -- subject to a turbulent dynamic mode -- to a target state, whilst remaining in the desired dynamic mode with high probability.
\todo{add links correctly}
More information and code available at \url{https://www.aidanscannell.com/publication/mode-constrained-mbrl/}.


%regarding the mode constraint to
%We overcome issues with constrained exploration by introducing a novel exploration mechanism which prevents
%Based on the observation that the mode constraint hinders exploration, we introduce a novel exploration mechanism which prevents
%We validate our method in a simulated quadcopter environment by showing that it can navigate to a target state whilst remaining in a desired dynamics
%mode with high probability.
%We provide a TensorFlow/GPflow implementation at \url{https://github.com/aidanscannell/ModeRL}.

%Further to this, we observe that the mode constraint hinders exploration and stops the quadcopter from navigating to the target state.
%We handle the latent mode constraint by jointly inferring the underlying dynamic modes and the latent mode constraint via
%a novel Mixtures of Gaussian Process Experts method built upon sparse approximations and a stochastic variational inference scheme.
%We overcome issues with constrained exploration by introducing a novel exploration mechanism which prevents
%Based on the observation that the mode constraint hinders exploration, we introduce a novel exploration mechanism which prevents
%We validate our method in a simulated quadcopter environment by showing that it can navigate to a target state whilst remaining in a desired dynamics
%mode with high probability.
%We provide a TensorFlow/GPflow implementation at \url{https://github.com/aidanscannell/ModeRL}.

%mode remaining planning algorithm leverages the latent structure of our dynamics
%model to explore the environment whilst simultaneously guaranteeing that the agent remains in the desired dynamic mode
%with high probability.
%Our mode remaining planning algorithm leverages the latent structure of our dynamics
%model to explore the environment whilst simultaneously guaranteeing that the agent remains in the desired dynamic mode
%with high probability.
%We validate our method in a simulated quadcopter environment
%and provide a TensorFlow/GPflow implementation at XXXX.

%We introduce a trajectory optimisation algorithm that leverages the latent structure of our dynamics
%We introduce a novel Mixtures of Gaussian Process Experts method built upon sparse approximations and stochastic variational inference and use it to
%infer both the underlying dynamic modes, as well as information regarding the latent mode constraint.



%The main contribution of this paper is a model-based RL algorithm that can solve tasks
%in environments subject to multiple dynamic modes, whilst \textit{avoiding inoperable or undesirable dynamic modes}.

%The main contribution of this paper is a model-based RL algorithm that can navigate an agent to a target state
%-- in environments subject to multiple dynamic modes -- whilst \textit{avoiding inoperable or undesirable dynamic modes}.
%For example, flying a quadcopter to a target state whilst avoiding turbulent dynamic modes induced by strong wind fields.
%We introduce a trajectory optimisation algorithm that leverages the latent structure of our dynamics
%model to explore the environment whilst simultaneously guaranteeing that the agent remains in the desired dynamic mode
%with high probability.
%We validate our method in a simulated quadcopter environment
%and provide a TensorFlow/GPflow implementation at \url{https://github.com/aidanscannell/ModeRL}.
%The main goal of this paper is to autonomously navigate an agent to a target state whilst
%\textit{avoiding inoperable or undesirable dynamic modes}.
%For example, flying a quadcopter to a target state whilst avoiding regions of the state
%space that are subject to turbulent dynamic modes induced by strong wind fields.
%Motivated by synergising model learning and control, we introduce a Mixtures of Gaussian process experts method for learning
%dynamic models, which infers latent structure regarding how systems switch between their underlying dynamic modes.
%Importantly,
%We introduce a trajectory optimisation algorithm that explores the environment whilst guaranteeing that the agent remains in
%the desired dynamic mode with high probability.
%We validate our method in a simulated quadcopter environment where the goal is to avoid entering regions of the state
%space with strong wind fields and thus turbulent dynamic modes.
%Our model-based RL algorithm is implemented in TensorFlow/GPflow and is available at \url{https://github.com/aidanscannell/ModeOpt}.
\end{abstract}
#+END_EXPORT
# We consider the model-based RL setting, where an explicit dynamic model -- that includes uncertainties -- is used
# to plan trajectories to a target state.
# We introduce an explorative trajectory optimisation algorithm that explicitly reasons about
# the uncertainties in the dynamic model.
# As a result, it can explore the environment whilst guaranteeing that the agent remains in
# the desired dynamic mode with high probability.

# Our work extends these methods to non-stationary environments subject to \textit{multimodal} dynamic modes,
# where some of the dynamic modes are believed to be \textit{inoperable or undesirable}.

# In particular, we introduce a model-based RL algorithms for  in environments
# subject to \textit{multimodal} dynamic modes, where some of the dynamic modes are believed to be
# \textit{inoperable or undesirable}.

# In this work, we are interested in environments where non-stationarity is induced by different dynamic modes
# in different regions of the state space.
# The main goal of our work is to control \textit{unknown},
# \textit{multimodal} dynamical systems, to a target state, whilst \textit{avoiding inoperable or undesirable dynamic modes}.

# Over the last decade, \textit{learning-based control} has become a popular paradigm for controlling dynamical systems.
# Although recent algorithms can find high-performance controllers, they typically only consider unimodal systems and
# cannot correctly identify multimodal dynamical systems.
# The main goal of our work is to control \textit{unknown},
# \textit{multimodal} dynamical systems, to a target state, whilst \textit{avoiding inoperable or undesirable dynamic modes}.
# Further to this, deploying learning algorithms in the real world requires handling the uncertainties
# inherent to the system, as well as the uncertainties arising from learning from observations.
# To this end, we consider the model-based RL setting, where an explicit dynamic model -- that includes uncertainties -- is used to plan trajectories to a target state.
# Motivated by synergising model learning and control, we introduce a Mixtures of Gaussian process experts method for learning
# dynamic models, which infers latent structure regarding how systems switch between their underlying dynamic modes.
# Initially, the agentâ€™s dynamic model will be highly \textit{uncertain} â€” due to a lack of training observations â€” so
# it is not possible find mode remaining trajectories to the target state with high confidence.
# When this is the case, our algorithm actively explores its environment, collects data and updates its dynamic model.
# We introduce an explorative trajectory optimisation algorithm that explicitly reasons about
# the uncertainties in the dynamic model.
# As a result, it can explore the environment whilst guaranteeing that the agent remains in
# the desired dynamic mode with high probability.
# We embed our mode remaining exploration algorithm into a model-based RL algorithm, which solves the mode
# remaining navigation problem,
# whilst guaranteeing that the controlled system remains in the desired dynamic mode with a high probability.
# We validate our method in a simulated quadcopter environment where the goal is to avoid entering regions of the state
# space with strong wind fields and thus turbulent dynamic modes.
# Our model-based RL algorithm is implemented in TensorFlow/GPflow and is available at \url{https://github.com/aidanscannell/ModeOpt}.

* INTRODUCTION
** Figure 1 constraint expanding :ignore:
#+begin_export latex
\begin{figure}[!t]
  \centering
    \begin{tikzpicture}[outer sep=0]
    \node at (0, 0) {\includegraphics[width=0.92\columnwidth]{../experiments/figures/moderl_constraint_expanding.pdf}};
    %\node at (0, 0) {hey};
    \node at (3, 0.4) {\LARGE\faSkull};
    \end{tikzpicture}
    \caption{\textbf{Mode-constrained quadcopter navigation}.
    Top-down view of a quadcopter subject to
    1) an \textit{operable} dynamic mode (blue) and 2) an \textit{inoperable}, turbulent dynamic mode induced by a strong wind field (red).
    The goal is to navigate to the target whilst remaining in the \textit{operable} dynamic mode (blue).
    We achieve this by gradually expanding the $\delta\text{-mode-constrained}$ region (\cref{def-delta-mode-remaining}) at each episode $i$,
    i.e. improving our knowledge of the latent mode constraint, by training our dynamic model on new data $\mathcal{D}_{0:i}$.}
    %and reducing our un under the well-calibrated uncertainty of our dynamic model.}
    %by updating our learned dynamic model (and thus our knowledge of the latent mode constraint).}
    %However, the mode constraint is \textit{unknown a priori}.
  \label{fig-constraint-expanding}
\end{figure}
#+end_export
** intro :ignore:


# Most model-based RL algorithms do not consider environments subject to multiple dynamic modes.
# Moreover, they do not consider environments where we would like to avoid \textit{inoperable or undesirable dynamic modes}.
# We present a model-based RL algorithm that avoids entering \textit{inoperable or undesirable dynamic modes}
# by remaining in a desired dynamic mode with high probability.
# For example, flying a quadcopter to a target state whilst avoiding turbulent dynamic modes.
# This problem is particularly difficult because the mode constraint is latent and must be inferred via interaction with the environment.
# We handle this latent mode constraint by jointly inferring the underlying dynamic modes and the latent mode constraint via
# a novel Mixtures of Gaussian Process Experts method built upon sparse approximations and a stochastic variational inference scheme.
# Our planning scheme then leverages the model's latent structure to target exploration in regions where the mode constraint's
# \textit{epistemic uncertainty} is high.
# We validate our method by showing that it can navigate a simulated quadcopter -- subject to a turbulent dynamic mode -- to a target state, whilst remaining in
# the desired dynamic mode.
# # We provide a TensorFlow/GPflow implementation at \url{https://github.com/aidanscannell/ModeRL}.



# Over the last decade, reinforcement learning (RL) has become a popular paradigm for controlling dynamical systems.
# However, most \acrshort{rl} algorithms do not consider environments subject to multiple dynamic modes.
# Moreover, they do not consider environments where we would like to avoid \textit{inoperable or undesirable} dynamic modes.
# For example, flying a quadcopter to a target state whilst avoiding turbulent dynamic modes, or driving a car whilst avoiding dangerous road surfaces.
# In these examples, we seek agents that can learn efficiently whilst avoiding these \textit{inoperable or undesirable} dynamic modes.

# Over the last decade, reinforcement learning (RL) has become a popular paradigm for controlling dynamical systems \citep{sutton2018reinforcement,hewingLearningBased2020}.
# However, \acrshort{rl} algorithms to not typical prevent agents entering \textit{inoperable or undesirable} dynamic modes.
# This is a desirable behaviour when flying a quadcopter to a target state whilst avoiding turbulent dynamic modes, or driving a car whilst avoiding dangerous road surfaces.
# For example, flying a quadcopter to a target state whilst avoiding turbulent dynamic modes, or driving a car whilst avoiding dangerous road surfaces.
# In these examples, we seek agents that can learn sample efficiently, whilst avoiding these \textit{inoperable or undesirable} dynamic modes.
# One approach to solving this problem is to constrain the agent to a single dynamic mode during learning.

# In some scenarios, we would like to use \acrshort{rl} to control systems, whilst avoiding \textit{inoperable or undesirable} dynamic modes.
Over the last decade, reinforcement learning (RL) has become a popular paradigm for controlling dynamical systems \citep{sutton2018reinforcement,hewingLearningBased2020}.
However, RL algorithms do not typically prevent agents from entering \textit{inoperable} or \textit{undesirable} dynamic modes (cref:def-dynamics-mode).
This would be desirable when flying a quadcopter to a target state whilst avoiding turbulent dynamic modes,
or driving a car whilst avoiding dangerous road surfaces.
In these examples, we seek agents that can learn sample-efficiently, whilst avoiding these \textit{inoperable} or \textit{undesirable} dynamic modes.

One approach to solving this problem is to constrain the agent to a single dynamic mode during training.
However, in this mode-constrained setting, the agent does not observe the constraint.
Instead, the mode constraint is a hidden variable associated with the environment's dynamics, which are /unknown a priori/.
As a result, our constraint must be simultaneously learned and enforced.
# Instead, the mode constraint depends on the dynamic modes, which are /unknown a priori/.
# learning.
# To the best of our knowledge, there are currently no RL algorithms that attempt to do this.
# Perhaps due to the difficulty of simultaneously enforcing a constraint whilst learning it.

When simultaneously learning and enforcing a constraint in RL, it is impossible to guarantee constraint satisfaction.
This emphasises the need to learn sample efficiently, as each interaction with the environment could result in a constraint violation.
Further to this, constraints can prevent an agent from solving the main task [cite:@royDirectBehaviorSpecification2022].
This is because they can introduce local optima that make the space of feasible policies hard to navigate.

# Further to this, constraints can induce local optimas, making the space of feasible policies hard to navigate, which can prevent an agent from

# Further to this, in constrained RL, the space of feasible policies can become hard to navigate, which can prevent an agent from

# ModeRL's planning scheme combats issues with local optima by using the model's /epistemic uncertainty/ in the mode constraint to guide exploration.
#+begin_export latex
In this paper, we present ModeRL\footnote{Code @ \href{https://github.com/aidanscannell/moderl/}{https://github.com/aidanscannell/moderl/}},
a Bayesian model-based RL algorithm, which simultaneously learns and enforces the mode constraint using
well-calibrated uncertainty estimates from a learned dynamic model.
#+end_export
Our main contributions are as follows:
1. A method for jointly inferring the mode constraint alongside the underlying dynamic modes.
2. A planning algorithm that leverages the dynamic model's well-calibrated uncertainty estimates to 1) enforce the mode constraint up to a given probability, and 2) combat local optima induced by the constraint.
3. We validate ModeRL in a simulated quadcopter navigation task.
# 3. We validate ModeRL in a simulated quadcopter navigation task by showing that it can solve tasks where a constrained greedy strategy fails.
# 2. A planning algorithm that leverages the dynamic model's well-calibrated uncertainty estimates to 1) enforce the mode constraint up to a given probability, 2) combat local optima induced by the mode constraint, and 3) improve sample efficiency via non-myopic exploration.
# 3. Experiments in a simulated quadcopter navigation task that validates ModeRL solves tasks where a constrained greedy strategy fails.

# 1. A method for jointly inferring the mode constraint alongside the underlying dynamic modes by treating the mode constraint as a latent variable.

# 1. We introduce a method for jointly inferring the mode constraint alongside the underlying dynamic modes,
#    which treats the mode constraint as a latent variable and principally disentangles sources of uncertainty.
# 2. We introduce a planning algorithm that leverages the dynamic model's well-calibrated uncertainty estimates to constrain the controlled system to
#    to the desired dynamic mode up to a given probability.
# 3. We also leverage the dynamic model's well-calibrated uncertainty estimates to combat issues with local optima induced by the mode constraint.
#    More specifically, we guide exploration to states where the agent's /epistemic uncertainty/ in the mode constraint is high.
# 4. We improve sample efficiency via a non-myopic exploration strategy which leverages the latent structure of our dynamic model
# 5. Finally, we validate ModeRL in a simulated quadcopter navigation task, where we show that ModeRL can solve the task, whislt a
#    constrained greedy strategy cannot.




# 4. Finally, we leverage the GPs modelling our mode constraint to improve sample efficiency via a non-myopic exploration strategy.
   # We highlight some potential issues with objectives that are commonly used for exploration in RL.
# 4. We combat issues with local optima induced by the mode constraint, by targeting exploration
#    where the agent's /epistemic uncertainty/ in the mode constraint is high.


# targeting exploration where the
# We propose to  integrating prior knowledge about the mode constraint,

# Further to this, when simultaneously enforcing and learning a constraint, it is important to be sample efficient.
# In this paper, we present ModeRL, a Bayesian model-based RL algorithm which leverages a learned dynamics model, with well-calibrated uncertainty
# estimates, to plan mode-constrained trajectories.


# principally disentangles the /epistemic/ and /aleatoric uncertainties/

# which treat the mode constraint as a latent variable and infers it jointly with the underlying dynamic modes.
# ModeRL's planning algorithm lever

# In contrast to previous MoGPE methods, our approach principally disentangles the /epistemic/ and /aleatoric uncertainties/
# associated with the mode constraint.
# Further to this, our variational lower bound provides scalability as it can be optimised with stochastic gradient methods.

# *Main contributions*
# Our main contributions are as follows:
# 1. We

# 1. Motivated by synergising model learning and planning, we introduce a novel Mixtures of Gaussian process experts method for learning representations of multimodal dynamical systems.
#    It treats the mode constraint as a latent variable and infers it jointly with the underlying dynamic modes.
#    In contrast to previous MoGPE methods, our approach principally disentangles the /epistemic/ and /aleatoric uncertainties/
#    associated with the mode constraint.
#    Further to this, our variational lower bound provides scalability as it can be optimised with stochastic gradient methods.
# 2. Our planning algorithm leverages the well-calibrated uncertainty estimates associated with our dynamic model to constrain the controlled system to
#    to the desired dynamic mode with high probability.
#    We observe that exploration subject to unknown constraints is hard and propose an exploration strategy that targets exploration where the mode constraint's
#    /epistemic uncertainty/ is high.
# 3. We highlight some potential issues with objectives that are commonly used for exploration in RL.

# 3. We visually inspect multiple exploration objectives commonly used in RL and observe some potential issues.
   # provide insights into why some common exploration strategies may fail with long horizons.
# 3. We capitalise on our easy to visualise environment to provide visual insights on multiple exploration strategies.

    # algorithm that can explore multimodal environments with a high probability of remaining in a desired dynamic mode.
# 2. An explorative trajectory optimisation algorithm that can explore multimodal environments with a high probability of remaining in a desired dynamic mode.

# Further to this, when deploying RL algorithms in the real world, it is important to handle the uncertainties
# inherent to the system, as well as the uncertainties arising from learning from observations.

# When deploying RL algorithms in the real world, it is important to be sample efficient.
# This is exacerbated when considering constrained RL

# When deploying a mode-constrained RL algorithm in the real world,
# i.e. simultaneously enforcing a constraint whilst learning it,  it is important to be sample efficient.




# The main goal of our work is to control /unknown/, or /partially unknown/,
# \textit{multimodal} dynamical systems, to a target state, whilst avoiding \textit{inoperable or undesirable} dynamic modes.
# Further to this, deploying learning algorithms in the real world requires handling the uncertainties
# inherent to the system, as well as the uncertainties arising from learning from observations.
# To this end, we consider the model-based RL setting, where an explicit dynamic model -- that includes uncertainties -- is used to plan trajectories to a target state.


# Over the last decade, reinforcement learning (RL) has become a popular paradigm for controlling dynamical systems.
# However, no  not consider environments where we would like to avoid \textit{inoperable or undesirable} dynamic modes.

# Moreover, they do not consider environments where we would like to avoid \textit{inoperable or undesirable} dynamic modes.
# For example, flying a quadcopter to a target state whilst avoiding turbulent dynamic modes, or driving a car whilst avoiding dangerous road surfaces.
# In these examples, we seek agents that can learn efficiently whilst avoiding these \textit{inoperable or undesirable} dynamic modes.


# One approach to solving this problem is to constrain the agent to a single dynamic mode.
# A common paradigm when considering constraints in RL is to consier constrained Markov decision processes (CMDP) [cite:@altmanConstrained1999].
# In this setting, the agent must satisfy constraints on expectations of auxiliary cost functions which indicate constraint violations.
# However, in contrast to the CMDP setting, we do not have access to a cost function as the mode constraints are /unknown a priori/.



# principally disentangles the /epistemic/ and /aleatoric uncertainties/

# which treat the mode constraint as a latent variable and infers it jointly with the underlying dynamic modes.
# ModeRL's planning algorithm lever

# In contrast to previous MoGPE methods, our approach principally disentangles the /epistemic/ and /aleatoric uncertainties/
# associated with the mode constraint.
# Further to this, our variational lower bound provides scalability as it can be optimised with stochastic gradient methods.

# *Main contributions*
# Our main contributions are as follows:
# 1. We

# 1. Motivated by synergising model learning and planning, we introduce a novel Mixtures of Gaussian process experts method for learning representations of multimodal dynamical systems.
#    It treats the mode constraint as a latent variable and infers it jointly with the underlying dynamic modes.
#    In contrast to previous MoGPE methods, our approach principally disentangles the /epistemic/ and /aleatoric uncertainties/
#    associated with the mode constraint.
#    Further to this, our variational lower bound provides scalability as it can be optimised with stochastic gradient methods.
# 2. Our planning algorithm leverages the well-calibrated uncertainty estimates associated with our dynamic model to constrain the controlled system to
#    to the desired dynamic mode with high probability.
#    We observe that exploration subject to unknown constraints is hard and propose an exploration strategy that targets exploration where the mode constraint's
#    /epistemic uncertainty/ is high.
# 3. We highlight some potential issues with objectives that are commonly used for exploration in RL.

# 3. We visually inspect multiple exploration objectives commonly used in RL and observe some potential issues.
   # provide insights into why some common exploration strategies may fail with long horizons.
# 3. We capitalise on our easy to visualise environment to provide visual insights on multiple exploration strategies.

    # algorithm that can explore multimodal environments with a high probability of remaining in a desired dynamic mode.
# 2. An explorative trajectory optimisation algorithm that can explore multimodal environments with a high probability of remaining in a desired dynamic mode.

# The rest of this paper is organised as follows.
# \todo{add para on paper structure}
# cref:problem-statement-explore formally states the problem and the assumptions that are made in this chapter.
# cref:sec-exploration then details the exploration strategy proposed in this work and
# cref:sec-moderl presents mode-constrained model-based reinforcement learning, a model-based RL algorithm for
# solving the mode remaining navigation problem in cref:problem-statement-main.
# Finally, cref:sec-preliminary-results presents preliminary results in a simulated
# quadcopter environment and cref:sec-future-work-explore discusses ModeRL and
# details directions for future work.

** after fig 1 :ignore:


# The difficulties associated with constructing mathematical representations of dynamical systems
# can be overcome by learning from observations citep:ljungSystem1999.
# Learning dynamic models has the added benefit that it
# alleviates the dependence on domain experts for specifying accurate models, making it easier to
# deploy more general techniques.
# However, learning dynamic models for control introduces other difficulties.
# For example, it is important to know
# where the model cannot predict confidently due to a lack of training observations.
# This concept is known as /epistemic uncertainty/ and is reduced in the limit of infinite data.
# Correctly quantifying uncertainty is crucial for intelligent decision-making.
# #+BEGIN_EXPORT latex
# \begin{myquote}
# \textbf{Epistemic uncertainty}
# is the uncertainty attributed to incomplete knowledge about a phenomenon that limits our ability to model it.
# It represents knowledge about a phenomenon that we could know but we do \textit{not know a priori}.
# It is reduced through the accumulation of additional information.
# \end{myquote}
# #+END_EXPORT

# Bounding the expected cost function gives bounds on the probability of harmful events.
# the underlying dynamic modes and how the system switches between them

# the underlying dynamic modes and how the system switches between them are /unknown a priori/.

# In this setting, the reward function is augmented with an additional cost function which indicates when the constraint is violated.


# #+BEGIN_EXPORT latex
# \begin{myquote}
# \textbf{Epistemic uncertainty}
# is the uncertainty attributed to incomplete knowledge about a phenomenon that limits our ability to model it.
# It represents knowledge about a phenomenon that we could know but we do \textit{not know a priori}.
# It is reduced through the accumulation of additional information.
# \end{myquote}
# #+END_EXPORT

# The main goal of our work is to control /unknown/, or /partially unknown/,
# \textit{multimodal} dynamical systems, to a target state, whilst \textit{avoiding inoperable or undesirable dynamic modes}.


# The main goal of our work is to control /unknown/, or /partially unknown/,
# \textit{multimodal} dynamical systems, to a target state, whilst \textit{avoiding inoperable or undesirable dynamic modes}.
# Further to this, deploying learning algorithms in the real world requires handling the uncertainties
# inherent to the system, as well as the uncertainties arising from learning from observations.
# To this end, we consider the model-based RL setting, where an explicit dynamic model -- that includes uncertainties -- is used to plan trajectories to a target state.


# We present a model-based RL algorithm that avoids entering \textit{inoperable or undesirable dynamic modes}
# by remaining in a desired dynamic mode with high probability.

# Although recent algorithms can find high-performance controllers, they typically only consider unimodal systems and
# cannot correctly identify multimodal dynamical systems.

# Motivated by synergising model learning and control, we introduce a Mixtures of Gaussian process experts method for learning
# dynamic models, which infers latent structure regarding how systems switch between their underlying dynamic modes.
# We then present three trajectory optimisation algorithms which, given this learned dynamic model, find trajectories
# to a target state with \textit{mode remaining guarantees}.
# Initially, the agentâ€™s dynamic model will be highly \textit{uncertain} â€” due to a lack of training observations â€” so these algorithms cannot guarantee mode remaining navigation with high confidence.
# When this is the case, the agent actively explores its environment, collects data and updates its dynamic model.
# We introduce an explorative trajectory optimisation algorithm that explicitly reasons about
# the uncertainties in the dynamic model.
# As a result, it can explore the environment whilst guaranteeing that the agent remains in
# the desired dynamic mode with high probability.
# Finally, we consolidate the work in this thesis into a model-based RL algorithm, which solves the mode
# remaining navigation problem,
# whilst guaranteeing that the controlled system remains in the desired dynamic mode with a high probability.

# This paper is concerned with controlling /unknown/ or /partially unknown/, multimodal dynamical systems,
# from an initial state $\state_0$ â€“ in a desired dynamic mode $\desiredMode$ â€“ to a target state $\targetState$,
# whilst guaranteeing that the controlled system remains in the desired dynamic mode.
# However, in contrast to previous chapters, it does not assume prior access to the environment. That is,
# it considers the more realistic scenario, where the agent must iteratively explore its
# environment, collect data and update itâ€™s dynamic model â€“ whilst remaining in
# the desired dynamic mode â€“ until it can confidently navigate to the target state
# $\targetState$. Following previous chapters, this chapter also considers model-based approaches
# where the dynamic model is learned from observations.
* RELATED WORK
To the best of our knowledge, there is no prior work addressing mode-constrained RL (see cref:eq-main-problem).
However, we can compare our method to related works, which we detail here.
# However, we can compare how our method differs from related works, which we detail here.
# However, we review relevant work, highlighting their similarities and differences.
# In particular, how mode-constrained RL differs.
# similarities to our own.

# In contrast to constrained RL where the constraint function is 1) /unknown a priori/, 2) its output is not observed from the environment,
# and
# However, we review the relevant work with similarities to our own.

# when both the dynamic modes and the mode constraint are /unknown a priori/.
# However, we review the relevant work with similarities to our own.

# \todo{Add more citations from rebuttal}
# [cite:@buisson-fenetActively2020]
# [cite:@huActiveUncertaintyReduction2023] [cite:@kirschnerAdaptiveSafeBayesian2019]

*Constrained Markov decision processes (CMDPs)*
A common paradigm when considering constraints in RL is to consider CMDPs citep:altmanConstrained1999,wachiSafeExplorationOptimization2018.
In this setting, the agent must satisfy a set of constraints defined by additional cost functions, whose output is observed from the environment.
In contrast, the output of the constraint function is not observed in mode-constrained RL, so we cannot learn it using supervised learning.
Our work has similarities to cite:schreiterSafe2015 as they use a Gaussian process (GP) classifier to identify safe and unsafe regions
when learning dynamic models in an active learning setting.
However, they also assume that they observe the output of their constraint function.

*Safe model predictive control (MPC)*
\cite{kollerLearningBased2018} and \cite{hewingLearningBased2020} consider safe MPC schemes which use GP dynamic models to
certify the safety of actions.
However, they do not consider environments with multimodal dynamics.
cite:arcariDualStochasticMPC2020 is the most similar work to our own, as they consider safe stochastic MPC in dynamical systems with multiple
operating modes (synonymous to our dynamic mode in cref:def-dynamics-mode).
Our work differs in that we use nonparametric methods to jointly identify the mode constraint and the dynamic modes.


# *Constrained Markov decision processes (CMDPs)*
# A common paradigm when considering constraints in RL is to consider CMDPs citep:altmanConstrained1999,wachiSafeExplorationOptimization2018.
# In this setting, the agent must satisfy a set of constraints defined by additional cost functions, whose output is observed from the environment.
# In contrast, the output of the constraint function is not observed in mode-constrained RL, so we cannot learn it using supervised learning.
# Our work has similarities to cite:schreiterSafe2015 as they use a Gaussian process (GP) classifier to identify safe and unsafe regions
# when learning dynamic models in an active learning setting.
# However, they also assume that they observe the output of their constraint function.

# However, they also assume that they observe if a state transition is safe (or not).
# Instead, we must simultaneously learn and enforce the mode constraint whilst interacting with the environment.
# In contrast, we consider the mode-constrained setting, where the agent does not directly observe the constraint in the form of a cost from the environment.
# As such, we cannot learn the constraint function using supervised learning.

*Mode remaining planning*
cite:scannellTrajectory2021 present a mode remaining trajectory optimisation algorithm that uses a learned dynamic model.
However, they do not enforce any constraints.
Further to this, they assume access to the environment /a priori/.
In contrast, we consider the model-based RL setting, where we enforce the mode constraint during exploration (i.e. data collection) as well.
# However, their approach attempts to remain in a single dynamic mode, without enforcing any constraints.

# In contrast, we are interested in constraining exploration (data collection process) a single dynamic mode.
# However, they assume access to the environment /a priori/ so do not consider exploration in model-based RL setting.
# consider mode-constrained exploration in a model-based RL setting.


# *Safe active learning*
# In contrast, we cannot directly observe the mode constraint and must infer it using a dynamic model.

*Safety as stability*
In low-dimensional continuous-control problems, cite:berkenkampSafe2017 propose to encode safety as stability via a learned dynamic model.
However, their method assumes that the environment's dynamics are Lipschitz continuous.
Although we do not provide details in this paper, we believe our method could be used to remove this assumption by constraining exploration to a subset
of the dynamics that are Lipschitz continuous.

# *Safe model-based RL*
# [cite:@thomasSafeReinforcementLearning2021] also propose a safe model-based RL approach which penalises unsafe actions via
# a reward penaly based upon multi-step predictions in a learned dynamic model. However, they assume that the set of unsafe states is /known a priori/

# *Safe model-based RL*
# In low-dimensional continuous-control problems, cite:berkenkampSafe2017 propose to construct confidence intervals around Lyapunov functions
# by first learning a GP dynamic model.
# They then use their confidence intervals to optimise a policy such that it remains in a Lyapunov stable region of attraction.
# Their method encodes safety via stability guarantees but is limited to systems where the dynamics are Lipschitz continuous.
# Although we do not provide details in this paper, we believe that our method can be used as a wrapper to remove the
# assumption of Lipschitz continuous dynamics that many safe RL methods depend upon.
# This is because our method can constrain exploration to a subset of the state space where the dynamics are Lipschitz continuous.
# [cite:@thomasSafeReinforcementLearning2021] also propose a safe model-based RL approach which penalises unsafe actions via
# a reward penaly based upon multi-step predictions in a learned dynamic model. However, they assume that the set of unsafe states is /known a priori/


# In contrast to most of the methods presented here, the constraint function in mode-constrained RL is not only /unknown a priori/, but its output
In summary, the constraint function in mode-constrained RL is not only /unknown a priori/, but its output is also not observed.
This is because it is a hidden variable associated with the environment's dynamics.
As such, it cannot be learned with supervised learning.
In the remainder of this paper, we present a model-based RL method that uses a nonparametric dynamic model to infer the mode constraint -- as a latent variable --
alongside the dynamic modes.
Importantly, it simultaneously learns and enforces the mode constraint (with high probability) during training.


# and instead must be learned alongside the environment's dynamics.
# This is because the mode constraint function is

# modes and the mode constraint are /unknown a priori/.



# generalise safe RL algorithms with
# Lipschitz continuity assumptions on the environment's dynamics, to environments whose dynamics are not Lipschitz continuous, by constraining

# Although we do not provide details in this paper, we believe that our method can generalise safe RL algorithms with
# Lipschitz continuity assumptions on the environment's dynamics, to environments whose dynamics are not Lipschitz continuous, by constraining
# the algorithm to a subset of the state space where the dynamics are Lipschitz continuous.

# be used as a wrapper around safe RL
# methods which Lipschitz continuity assumptions  to environments with
# non Lipschitz continuous dynamics, by constraining the system to remain in a single dynamic mode
# (where Lipschitz continuity assumptions do hold).


# \todo[inline]{Add more related work. Namely, constrained MDP}

# [cite:@liuConstrained2021] consider

# assumptions to hold over the entire state-action space.

# Our work provides a framework to extend these meth
# In contrast, we consider a more general class of environment and do not require Lipschitz continuity assumptions to hold over the entire state-action space.

# systems with multimodal dynamic modes and as a result Lipschitz continuity assumptions do not hold over our state-action space.

* PROBLEM STATEMENT label:problem-statement
We consider environments with
states $\state_t \in \stateDomain \subseteq \R^{\StateDim}$, actions $\control_t \in \controlDomain \subseteq \R^{\ControlDim}$
and /multimodal/, /stochastic/  transition dynamics, given by
#+BEGIN_EXPORT latex
\begin{equation}  \label{eq-dynamics}
\begin{aligned}
\state_{\timeInd+1}
%&= \dynamicsFunc(\state_\timeInd, \control_\timeInd) + \bm\epsilon_{\timeInd} \\
 &= \mode{\dynamicsFunc}(\state_\timeInd, \control_\timeInd) + \bm\epsilon_{\modeInd, \timeInd},
\quad\text{if } \modeVar(\state_{\timeInd}) = \modeInd,
\end{aligned}
\end{equation}
#+END_EXPORT
where the discrete mode indicator function $\modeVar : \stateDomain \rightarrow \{1, \dots, \ModeInd\}$
indicates which of the $\ModeInd$ underlying dynamic modes
$\{\mode{\latentFunc} : \mode{\stateDomain} \times \controlDomain \rightarrow \stateDomain \}_{\modeInd=1}^\ModeInd$
and associated i.i.d. noise models $\bm\epsilon_{\modeInd, \timeInd}$
governs the environment at a given time step $\timeInd$.
We refer to the output of the mode indicator function as the mode indicator variable
$\alpha_{\timeInd} = \alpha(\state_{\timeInd}) \in \{1,\ldots,\ModeInd\}$.
# $\bm\epsilon_{\modeInd, \timeInd} \sim \mathcal{N}\left(\mathbf{0}, \bm\Sigma_{\mode{\bm\epsilon}} \right)$,

#+begin_export latex
\begin{definition}[dynamic mode] \label{def-dynamics-mode}
Let $\modeVar : \stateDomain \rightarrow \{0,\ldots, \ModeInd\}$ denote a function which partitions a dynamical system's $\dynamicsFunc$ state space $\stateDomain$ into
$\ModeInd$ pair-wise disjoint state domains
$\mode{\stateDomain} = \{ \state \in \stateDomain \mid \modeVar(\state) = \modeInd \}$.
Given a dynamical system comprising of $\ModeInd$ functions,
$\dynamicsFunc = \{\dynamicsFunc_{\modeInd} : \stateDomain_{\modeInd} \times \controlDomain \rightarrow \stateDomain\}_{\modeInd=1}^{\ModeInd}$,
where each function governs the system at a particular region of the state space $\stateDomain$,
we formally define a dynamic mode as,
\begin{align} \label{eq-dynamics-mode-def}
\dynamicsFunc_{\modeInd} : \stateDomain_{\modeInd} \times \controlDomain \rightarrow \stateDomain.
\end{align}
\end{definition}
#+end_export

# *State-dependent modes*
# We consider systems where the $\ModeInd$ underlying dynamic modes have pair-wise disjoint state domains.
# That is, each mode's state domain is given by
# $\mode{\stateDomain} = \{ \state \in \stateDomain \mid \modeVar(\state) = \modeInd \}$, with
# $\stateDomain_{i} \cap \stateDomain_{j} = \emptyset$ for distinct $i, j \in \{1, \ldots \ModeInd\}$.
# Notice from cref:def-dynamics-mode that our dynamic modes $\{\mode{\dynamicsFunc}\}_{\modeInd=1}^{\ModeInd}$ are free to leave
# Notice from cref:def-dynamics-mode that our dynamic modes $\{\mode{\dynamicsFunc}\}_{\modeInd=1}^{\ModeInd}$ are free to leave
Notice from cref:def-dynamics-mode that our dynamic modes are free to leave
their state spaces $\mode{\stateDomain}$ and enter other modes $\stateDomain$.

# We consider general deterministic policies $\pi(\state_{\timeInd}, \timeInd)$,
# which encapsulates both closed-loop policies $\pi: \stateDomain \rightarrow \controlDomain$ and open-loop policies $\pi: \R \rightarrow \controlDomain$.
*Problem statement*
We consider controlling the stochastic system in cref:eq-dynamics in an episodic setting, over a horizon $\TimeInd$.
We assume that after each episode the system is reset to a known initial state $\state_{0}$.
We consider general deterministic policies $\pi \in \Pi$,
which encapsulates both closed-loop policies $\pi(\state_{\timeInd})$ and open-loop policies $\pi(\timeInd)$.
For a known transition dynamic model $\dynamicsFunc: \stateDomain \times \controlDomain \rightarrow \stateDomain$,
the performance of a policy $\pi$ is the sum of rewards over the horizon, in expectation over the transition noise,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-objective}
J(\pi, \dynamicsFunc) = \E_{\bm\epsilon_{0:\TimeInd}} \left[ \sum_{\timeInd=0}^{\TimeInd} \rewardFunc(\state_{\timeInd}, \control_{\timeInd}) \mid \state_0 \right].
\end{align}
#+END_EXPORT
The goal of our work is to find the optimal policy $\pi^{*}$ whilst remaining in a desired dynamic mode $\desiredMode$,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-main-problem}
\argmax_{\pi \in \Pi} J(\pi, \dynamicsFunc) \quad \text{s.t. } \modeVar(\state_{\timeInd}) = \desiredMode \quad \forall \timeInd \in \{0, \ldots, \TimeInd\}.
\end{align}
#+END_EXPORT
Formally, a mode-constrained system is defined as follows.
#+begin_export latex
\begin{definition}[mode-constrained] \label{def-mode-remaining-main}
Let $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ denote a multimodal dynamical system
and $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar(\state) = \desiredMode \}$
denote the state domain of the desired dynamic mode $\desiredMode$.
Given an initial state $\state_0 \in \desiredStateDomain$ and a policy $\pi \in \Pi$,
the controlled system is said to be mode-constrained under the policy $\pi$ iff:
\begin{align} \label{eq-mode-remaining-def-main}
%\modeVar(\state_{\timeInd}) &\in \desiredMode \quad \forall \timeInd \in \{0, \ldots, \TimeInd\}
\dynamicsFunc(\state_{\timeInd}, \policy(\state_{\timeInd}, \timeInd)) + \epsilon_{\timeInd} &\in \stateDomain_{\desiredMode} \quad \forall \timeInd \in \{0, \ldots, \TimeInd\}
\end{align}
\end{definition}
#+end_export
Given that neither the underlying dynamic modes $\{\mode{\latentFunc}\}_{\modeInd=1}^{\ModeInd}$, nor how the system
switches between them $\modeVar$, are /known a priori/,
it is not possible to solve cref:eq-main-problem with the mode constraint in cref:def-mode-remaining-main.
Therefore, we relax the requirement to finding a mode-constrained policy with high probability.
We formally define a $\delta\text{-mode-constrained}$ policy as follows.
#+BEGIN_EXPORT latex
\begin{definition}[$\delta$-mode-constrained] \label{def-delta-mode-remaining}
Let $\dynamicsFunc : \stateDomain \times \controlDomain \rightarrow \stateDomain$ denote a multimodal dynamical system
%and $\desiredMode$ a desired dynamic mode defined by its state domain $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar(\state) = \desiredMode \}$.
and $\desiredStateDomain = \{ \state \in \stateDomain \mid \modeVar(\state) = \desiredMode \}$
denote the state domain of the desired dynamic mode $\desiredMode$.
Given an initial state $\state_0 \in \desiredStateDomain$ and $\delta \in (0,1]$,
a controlled system is said to be $\delta$-mode-constrained under the policy $\pi$ iff:
\begin{align} \label{eq-mode-remaining-def-explore}
\Pr( \forall \timeInd \in \{0,\ldots, \TimeInd \} :
%\modeVar(\state_{\timeInd}) = \desiredMode,
\dynamicsFunc(\state_{\timeInd}, \policy(\state_{\timeInd}, \timeInd)) + \epsilon_{\timeInd} &\in \stateDomain_{\desiredMode}) \geq 1 - \delta.
%\control_{\timeInd} \in \controlDomain) \geq 1 - \delta
%\Pr( \forall \timeInd \in \{0,\ldots,\TimeInd-1\} : \dynamicsFunc(\state_{\timeInd}, \policy(\state_{\timeInd}, \timeInd))
%&\in \desiredStateDomain, \policy(\state_{\timeInd}, \timeInd) \in \controlDomain) \geq 1 - \delta
\end{align}
\end{definition}
#+END_EXPORT
Policies satisfying this $\delta\text{-mode-constrained}$ definition
should remain in the desired dynamic mode with probability up to $1-\delta$.
It is worth noting that the agent would not be able to explore the environment without relaxing the
mode constraint from cref:def-mode-remaining-main.
Increasing $\delta$ promotes exploration but also increases the chance of violating the mode constraint.
Intuitively, $\delta$, which we refer to as the /constraint level/, makes the mode-constrained problem
feasible, whilst still providing some level of constraint satisfaction during training.
# problem feasible.
# the conservativeness of the agent's exploration.
# As such, $\delta$ can be viewed as setting the conservativeness of the agent's exploration.

# However, well-calibrated uncertainty estimates associated with a learned dynamic model make it possible to
# find mode-constrained trajectories with high probability.

# Note that the objective function in cref:eq-main-problem is not of primary interest as the novelty of our problem arises from the
# mode constraint on the dynamics.

# Intuitively, $\delta$ controls the level of conservativeness in the agent's exploration,
# as increasing $\delta$ promotes exploration at the cost of increase chance in violating the mode remaining constraint.

# as the more the $\delta\text{-mode remaining}$ constraint is relaxed (promoting exploration),
# the higher the chance in violating the mode remaining constraint.
# Our goal is to expand the that is known to belong to the desired dynamic mode
# towards the target state $\targetState$, without violating the $\delta\text{-mode remaining}$ constraint.
# much the agent wil exp  is  there
# the algorithm should expand the region that is known to belong to the desired dynamic mode
# towards the target state $\targetState$, without violating the $\delta\text{-mode remaining}$ constraint.
# Thus, the algorithm should expand the region that is known to belong to the desired dynamic mode
# towards the target state $\targetState$, without violating the $\delta\text{-mode remaining}$ constraint.
# Therefore, smaller $\delta$ values correspond to a higher confidence of remaining in the desired
# dynamic mode.


*Initial mode remaining controller*
In robotics applications, an initial set of poor-performing controllers can normally be obtained via
simulation or domain knowledge.
We assume access to an initial data set of state transitions
$\dataset_0 = \{(\state_{\timeInd}, \control_{\timeInd}), \state_{\timeInd+1}\}_{\timeInd=1}^{\TimeInd N_{0}}$
from $N_{0}$ episodes of length $\TimeInd$.
We use it to learn a predictive dynamic model $p(\state_{t+1} \mid \state_{\timeInd}, \action_{\timeInd}, \dataset_{0})$ which is locally accurate
around the start state $\state_0$.
#+BEGIN_EXPORT latex
\begin{assumption} \label{}
A state transition data set has been collected
$\dataset_0 = \{(\state_{\timeInd}, \control_{\timeInd}), \state_{\timeInd+1}\}_{\timeInd=1}^{\TimeInd N_{0}}$
from an initial region of the state space $\initialStateDomain \subseteq \desiredStateDomain$, which belongs to the desired
dynamic mode $\desiredMode$ and contains the start state $\state_0 \in \initialStateDomain$.
\end{assumption}
#+END_EXPORT
Although such a model can be used to learn an
initial policy, it will not work outside of the initial state domain $\initialStateDomain$ and may
not be able to find a $\delta\text{-mode-constrained}$ policy, due to the model having high /epistemic uncertainty/.
For this reason, we adopt a model-based RL strategy which incrementally explores the environment subject to a $\delta\text{-mode constraint}$.
At each episode, it collects data and uses it to train its dynamic model.
This reduces the dynamic model's /epistemic uncertainty/ and expands the $\delta\text{-mode-constrained}$ region.
See cref:fig-constraint-expanding.
# Each episode collects data, reducing the dynamic model's /epistemic uncertainty/, in turn
# expanding the $\delta\text{-mode-constrained}$ region.
# We further assume that we can itentify the desired dynamic mode $\desiredMode$ from the lear

# We further assume that a desired dynamic mode $\desiredMode$ is /known a priori/.
# #+BEGIN_EXPORT latex
# \begin{assumption} \label{}
# A desired dynamic mode $\desiredMode$ is either known, or can be identified from the learned dynamics.
# \end{assumption}
# #+END_EXPORT
# This is a realistic assumption as the parameters associated with each dynamic mode can be used to identify different behaviours.
# For example, the noise variance associated with a GP regression model represents the transition noise.

* Alg :ignore:
#+BEGIN_EXPORT latex
\begin{algorithm}[!t]
\caption{ModeRL}\label{alg-mode-opt}
\begin{algorithmic}[1]
\Require{Start state $\state_0$, desired dynamic mode $\desiredMode$, initial data set $\dataset_0$, policy $\pi_{0}$, dynamic model ${p(\state_{\timeInd+1} \mid \singleInput, \dataset_{0})}$}
%\For{$i = 0, 1, \ldots $}
\For{$i  \in \{0, 1, \ldots, \text{num episodes} \}$}
    \While{not converged}
        \State Sample $N_{b}$ state transitions $\mathcal{B} \sim \dataset_{0:i}$
        \State Update dynamics using \cref{eq-lower-bound} with $\mathcal{B}$
    \EndWhile
    %\State Sample $N_{b}$ state transitions from $\dataset_{i}$ dynamics using \cref{eq-lower-bound} and $\dataset_i$
    %\State Train dynamics using \cref{eq-lower-bound} and $\dataset_i$
    \State Optimise policy $\pi_{i+1}$ using \cref{eq-joint-entropy-objective}
    %\State Collect data $\dataset_{i+1}$ from environment using $\pi_{i+1}$; add to data set ${\dataset_{0:i+1} = \dataset_{i+1} \cup \dataset_{0:i}}$
    \State Collect data $\dataset_{i+1}$ using $\pi_{i+1}$
    \State Update agent's data set ${\dataset_{0:i+1} = \dataset_{i+1} \cup \dataset_{0:i}}$
\EndFor
\end{algorithmic}
\end{algorithm}
#+END_EXPORT

* MODE-CONSTRAINED MODEL-BASED RL label:sec-mode-optimisation
** intro :ignore:
We propose to solve the mode-constrained RL problem in cref:eq-main-problem by synergising model learning and planning in a
model-based RL algorithm we name mode-constrained model-based reinforcement learning (ModeRL).
Our approach is detailed in cref:alg-mode-opt.
** Probabilistic Dynamic Model

ModeRL learns a single-step dynamic model and we adopt the delta state formulation to regularise the predictive distribution.
We denote a state difference output as $\singleOutput = \state_{t+1} - \state_{t}$ and the set of all state difference outputs as  $\allOutput$.
We further denote a state-action input as $\singleInput = (\state_{\timeInd}, \action_{\timeInd})$, the set of all state-action inputs
as $\allInput$, the set of all state inputs as $\allState$ and the state transition data set at episode $i$ as $\dataset_{0:i}$.

# In particular, we seek to formulate a prior over our mode constraint which we can use to encode prior knowledge.
# In turn, this should enable ModeRL to potentially find plans that satisfy the mode constraint.
# In practice, this is unrealistic,
The main goal of our dynamic model is to jointly infer the mode constraint along with the underlying dynamic modes.
In particular, we would like our model to,
1. Formulate a prior over the mode constraint where we can encode prior knowledge, potentially enabling ModeRL to find a policy without ever violating the mode constraint.
   In practice, encoding prior knowledge allows us to exploit Bayesian interpolation [cite:@mackayBayesian1992] to reduce the number of constraint violations during training.
2. Disentangle the sources of uncertainty in the mode constraint so that ModeRL can escape local optima induced by the constraint (via intrinsic exploration with the mode constraint's /epistemic uncertainty/).

*Marginal likelihood*
Mixtures of Gaussian process experts (MoGPE) models are a natural choice for modelling multimodal systems as they automatically infer the assignment of observations to
dynamic modes (experts).
Let us start by introducing the MoGPE marginal likelihood,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-np-moe-marginal-likelihood-main}
%\moeEvidence
\evidence
%&= \sum_{\allModeVar} \npmoeGatingPosterior \npmoeExpertPosterior  \\
&= \sum_{\allModeVar}
\underbrace{p \left(\bm\modeVar \mid \allState \right)}_{\text{gating network}}
\bigg[ \prod_{\modeInd=1}^\ModeInd
\underbrace{p\left(\allOutputK \mid \allInputK, \expertParamsK \right)}_{\text{dynamic mode } \modeInd}
\bigg]
\end{align}
#+END_EXPORT
where $\allInputK$ denotes the set of $\NumData_{\modeInd}$ inputs assigned to dynamic mode $\modeInd$, i.e.
$\allInputK = \{\singleInput \in \allInput \mid \modeVar(\state_{\timeInd}) = \modeInd\}$.
Similarly for the outputs we have $\allOutputK = \{\singleOutput \in \allOutput \mid \modeVar(\state_{\timeInd+1}) = \modeInd\}$.
Note that there is a joint distribution corresponding to every possible combination of assignments
of observations to dynamic modes.
Hence, cref:eq-np-moe-marginal-likelihood-main is a sum over exponentially many ($\ModeInd^{\NumData}$) sets of assignments,
where $\allModeVar = \{\modeVar_1, \ldots, \modeVar_\NumData \}$ represents a set of assignments for all observations.
This distribution factors into the product over modes, where each mode models the joint Gaussian distribution
over the observations assigned to it.

# whilst providing scalability via sparse GP approximations \cite{snelsonSparse2005a,bauerUnderstanding2016,quinonero-candelaUnifying2005}.
*Learning the mode constraint*
The gating network indicates which dynamic mode governs the system at a given state input.
It is of particular importance in our work as we use it to represent our mode constraint.
Motivated by synergising model learning and planning, we formulate our gating network using input-dependent functions --
known as gating functions -- and place GP priors over them.
In cref:sec-planning we exploit the disentangled /epistemic uncertainty/ represented in our
GP-based gating network to help ModeRL escape local optima induced by the mode constraint.
Similar to cite:trespMixtures2000a, our gating network resembles a GP classification model,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-prob-mass}
\npmoeGatingPosterior &= \E_{\underbrace{p(\GatingFunc(\allState)) }_{\text{GP prior(s)}}} \bigg[ \prod_{\numData=1}^{\NumData}
\underbrace{p \left(\modeVar_{\timeInd} \mid \GatingFunc(\state_{\timeInd}) \right)}_{\text{classification likelihood}} \bigg]
%\allModeVar \mid \GatingFunc(\allInput) &\sim \prod_{\numData=1}^{\NumData} \text{Cat} \left( \text{softmax}   \left(\GatingFunc(\singleInput)\right) \right) \\
%\GatingFunc(\allInput) &\sim \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( \gatingMeanFunc(\allInput), \gatingCovFunc(\allInput, \allInput) \right),
\end{align}
#+END_EXPORT
where $p\left(\modeVar_{\timeInd} \mid \GatingFunc(\state_{\timeInd})\right)$ represents a classification likelihood parameterised by $\ModeInd$ gating
functions $\GatingFunc: \stateDomain \rightarrow \mathbb{R}^{\ModeInd}$.
We use a Bernoulli likelihood when $\ModeInd=2$ and a softmax when $\ModeInd>2$.
We place GP priors on each of the gating functions
$p(\GatingFunc(\allInput)) = \prod_{\modeInd=1}^\ModeInd \mathcal{N}\left( p(\mode{\gatingFunc}(\allInput) \mid \gatingMeanFunc(\allInput), \gatingCovFunc(\allInput, \allInput) \right)$,
where $\gatingMeanFunc(\cdot)$ and $\gatingCovFunc(\cdot,\cdot)$ represent the mean and covariance functions
associated with the $\modeInd^\text{th}$ gating function $\mode{\gatingFunc}$.
In our gating network formulation, the GP posteriors represent the mode constraint's /epistemic uncertainty/.

# We seek to formulate our gating network so that it can disentangle the sources of uncertainty in the mode constraint.
# This is so that our planning algorithm can target exploration where the agent is not confident in
# the mode constraint, i.e. where the gating networkâ€™s /epistemic uncertainty/ is high.

# Each gating function $\mode{\gatingFunc}$ describes how its corresponding mode's mixing
# probability varies over the input space.
# In contrast to the experts, partitioning the data set is not desirable for the gating network GPs,
# as each gating function should depend on all of the training observations.

# Each mode's mixing probability $\singleGatingPosterior$ is then obtained by marginalising
# *all* of the gating functions.
# In the general case where $\singleGatingLikelihood$ uses the softmax function
# (cref:eq-softmax) this integral is intractable, so it is approximated with Monte Carlo quadrature.


# which principally handles
# uncertainty,
# In particular, is disentangles the /epistemic uncertainty/ arising from learning from limited observations from the aleatoric uncertainty arising

# Given this model, the mode remaining control methods in cref:chap-traj-opt-control
# can be used to find trajectories to the target state $\targetState$.
# The mode chance constraints from cref:eq-mode-chance-constraint
# can then be used to check if these trajectories are $\delta\text{-mode remaining}$ under the learned dynamics
# model $\dynamicsModel$.
# Initially, it will not be possible to find
# $\delta\text{-mode remaining}$ trajectories under the learned dynamic model, due to high
# /epistemic uncertainty/.
# In this case, the agent must explore the environment, collect  data and update its dynamic model.
# Eventually, the agent will reduce the model's /epistemic uncertainty/ such that
# it can find $\delta\text{-mode remaining}$ trajectories to the target state $\targetState$.
# Ideally, the exploration strategy will have some guarantee of remaining in the desired dynamic mode.
# cref:fig-mode-opt-loop illustrates this process in a flowchart.
# and cref:alg-mode-opt details the algorithm presented in this chapter.
# The algorithm presented in this chapter is detailed in cref:alg-mode-opt.

# The algorithm is named ModeRL and is detailed in cref:alg-mode-opt.
# The algorithm is initialised with a start state $\state_0$, a target state $\targetState$,
# a desired dynamic mode $\desiredMode$, a
# data set of state transitions from the desired dynamic mode $\dataset_0$  and a calibrated
# dynamic model $\dynamicsModel$.
** Dynamic Modes :ignore:
# comment
\newline

# We model the underlying dynamic modes $\{\mode{\dynamicsFunc}\}_{\modeInd=1}^{\ModeInd}$ as independent GP regression models,
*Dynamic modes*
We model the underlying dynamic modes as independent GP regression models,
#+BEGIN_EXPORT latex
\begin{align}
\underbrace{p\left(\allOutputK \mid \allInputK \right)}_{\text{dynamic mode } \modeInd}
&= \underbrace{\E_{p(\mode{\latentFunc}(\allInputK))}}_{\text{GP prior}} \bigg[
\prod_{\numData=1}^{\NumData_{\modeInd}}
\underbrace{p\left(\singleOutput \mid \mode{\latentFunc}(\singleInput)\right)}_{\text{Gaussian likelihood}}
\bigg], \nonumber
%\allOutputK \mid \mode{\latentFunc}(\allInputK) &\sim \prod_{\numData=1}^{\NumData_{\modeInd}} \mathcal{N}\left( \singleOutput \mid \mode{f}(\singleInput), \mode{\noiseVar}^2 \right) \label{eq-expert-likelihood} \\
%\mode{\latentFunc}(\allInputK) &\sim \mathcal{N}\left( \expertMeanFunc(\allInputK), \expertCovFunc(\allInputK, \allInputK) \right), \label{eq-expert-prior}
\end{align}
#+END_EXPORT
where each mode's GP prior is given by
$p(\mode{\latentFunc}(\allInputK)) = \mathcal{N} \left( \mode{\latentFunc}(\allInputK) \mid \expertMeanFunc(\allInputK), \expertCovFunc(\allInputK, \allInputK) \right)$
with $\expertMeanFunc(\cdot)$ and $\expertCovFunc(\cdot, \cdot)$ representing the mean and
covariance functions associated with the $\modeInd^{\text{th}}$ mode's GP prior respectively.
Note that as the assignment of observations to modes is /not known a priori/, we must infer the assignments from observations.
In our model, each dynamic mode's Gaussian likelihood represents the mode's /aleatoric uncertainty/ (the transition noise in this case)
whilst each mode's GP posterior represents the mode's /epistemic uncertainty/.

** Dynamic Model Learning label:sec-dynamics-learning
# Training our dynamic model amounts to performing Bayesian inference to
Performing Bayesian inference in our dynamic model involves finding the posterior over the latent variables
$p(\{\latentFunc(\allInput)\}_{\modeInd=1}^{\ModeInd}, \GatingFunc(\allState) \mid \dataset_{0:i})$,
which requires calculating the marginal likelihood in cref:eq-np-moe-marginal-likelihood-main.
As such, exact inference in our model is intractable due to the marginalisation over the set of mode
indicator variables. For this reason, we resort to a variational approximation.
# The rich structure of our model makes it hard to construct an ELBO that can be evaluated in closed form,
# whilst accurately modelling the complex dependencies.
# Further to this, the marginal likelihood is extremely expensive to evaluate, as there are $\ModeInd^{\NumData}$
# sets of assignments $\bm\modeVar$ that need to be marginalised. For each set of assignments,  GP experts need to be evaluated, each with complexity O(N 3 ). For these rea-
# sons, this work derives a variational approximation based on inducing variables, that
# provides scalability by utilising stochastic gradient-based optimisation.



Following the approach by cite:titsiasVariational2009, we augment the probability space
with a set of inducing variables for each GP.
However, instead of collapsing these inducing variables, we
represent them as variational distributions and use them to lower bound the marginal likelihood, similar to
cite:hensmanGaussian2013,hensmanScalable2015.
cref:fig-graphical-model-sparse shows the graphical model of the augmented joint probability space.

*Augmented dynamic modes*
We sidestep the hard assignment of observations to modes by augmenting each dynamics GP with a set
of $\NumInducing_{\mode{\latentFunc}}$ separate independent inducing points,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-experts-inducing-prior}
\expertInducingPrior
&=  \mathcal{N}\left( \expertInducingOutput \mid
\expertMeanFunc(\expertInducingInput),
\expertCovFunc(\expertInducingInput, \expertInducingInput) \right).
\end{align}
#+END_EXPORT
Introducing separate inducing points from each mode's GP can loosely be seen as "partitioning"
the observations between modes.
However, as the assignment of observations to modes is /not known a priori/, the inducing inputs
$\expertInducingInput$ and variables $\expertInducingOutput$, must be inferred from observations.

*Augmented gating network*
We follow a similar approach for the gating network and augment each gating function GP with a
set of $\NumInducing_{\mode{\gatingFunc}}$ inducing points,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-gatings-inducing-prior}
\gatingInducingPrior
&= \mathcal{N}\left( \gatingInducingOutput \mid
\gatingMeanFunc(\gatingInducingInput),
\gatingCovFunc(\gatingInducingInput, \gatingInducingInput) \right).
\end{align}
#+END_EXPORT
The distribution over all gating functions is denoted $\gatingsInducingPrior =\prod_{\modeInd=1}^\ModeInd \gatingInducingPrior$.
In contrast to the dynamic modes, the gating function GPs share inducing inputs $\gatingInducingInput$.

*Marginal likelihood*
We use these inducing points to approximate the true marginal likelihood with,
#+BEGIN_EXPORT latex
%\small
\begin{align} \label{eq-augmented-marginal-likelihood-main}
\evidence &\approx
\E_{\gatingsInducingPrior \expertsInducingPrior} \bigg[  \\
&\prod_{\numData=1}^{\NumData} \sum_{\modeInd=1}^{\ModeInd} \singleGatingGivenInducing  \singleExpertGivenInducing \bigg], \nonumber
\end{align}
%\normalsize
#+END_EXPORT
where the conditional distributions $\singleExpertGivenInducing$ and $\singleGatingGivenInducing$
follow from standard sparse GP methodologies.
See cref:eq-sparse-gp-methodologies-gating,eq-sparse-gp-methodologies in cref:sec-sparse-approximations.
Importantly, the factorisation over observations is outside of the marginalisation over the mode indicator variable, i.e.
the mode indicator variable can be marginalised for each data point separately.
This is not usually the case for MoGPE methods.
Our approximation assumes that the inducing variables,
$\{\expertInducingOutput\}_{\modeInd=1}^\ModeInd$, are
a sufficient statistic for their associated latent function values,
$\{\mode{\latentFunc}(\allInputK) \}_{\modeInd=1}^\ModeInd$
and the set of assignments $\allModeVar$.
It becomes exact when each mode's inducing points represent the true data partition
$\{\expertInducingInput, \expertInducingOutput\}_{\modeInd=1}^{\ModeInd} = \{\allInputK, \mode{\latentFunc}(\allInputK)\}_{\modeInd=1}^{\ModeInd}$.
# It becomes exact in the limit $\ModeInd\NumInducing=\NumData$,
# if each mode's inducing points represent the true data partition
# $\{\expertInducingInput, \expertInducingOutput\}_{\modeInd=1}^{\ModeInd} = \{\allInputK, \mode{\latentFunc}(\allInputK)\}_{\modeInd=1}^{\ModeInd}$.

** ELBO :ignore:
# hello

*Evidence lower bound (ELBO)*
Following a similar approach to cite:hensmanGaussian2013,hensmanScalable2015,
we lower bound cref:eq-augmented-marginal-likelihood-main,
#+BEGIN_EXPORT latex
%\small
\begin{align} \label{eq-lower-bound}
\mathcal{L}(\{\mode{\mathbf{m}}, &\mode{\mathbf{L}}, \mode{\hat{\mathbf{m}}}, \mode{\hat{\mathbf{L}}}, \expertInducingInput\}_{\modeInd=1}^{\ModeInd}, \gatingInducingInput) =
%\sum_{(\singleInput, \singleOutput) \in \dataset_{0:i}}
\sum_{\timeInd=1}^{\NumData}
\E_{\gatingsVariational \expertsInducingVariational}
\bigg[ \nonumber \\
&\text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood  \singleExpertGivenInducing \bigg] \nonumber \\
&- \expertsKL \nonumber \\
&- \gatingsKL
\end{align}
%\normalsize
#+END_EXPORT
# #+BEGIN_EXPORT latex
# \small
# \begin{align} \label{eq-lower-bound}
# &\mathcal{L} = \sum_{\numData=1}^\NumData \E_{\gatingsVariational \expertsInducingVariational}
# \left[ \text{log} \sum_{\modeInd=1}^{\ModeInd} \singleGatingLikelihood
# \singleExpertGivenInducing \right] \nonumber \\
# &- \expertsKL \nonumber
# - \gatingsKL
# \end{align}
# \normalsize
# #+END_EXPORT
where the dynamic mode's variational posterior is given by
$\expertsInducingVariational = \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left(\expertInducingOutput \mid \mode{\mathbf{m}}, \mode{\mathbf{L}} \mode{\mathbf{L}}^{T} \right)$
and the gating network's variational posterior is given by
$\gatingsVariational = \prod_{\modeInd=1}^{\ModeInd} \int \singleLatentGatingGivenInducing \mathcal{N}\left(\gatingInducingOutput \mid \mode{\hat{\mathbf{m}}}, \mode{\hat{\mathbf{L}}}\mode{\hat{\mathbf{L}}}^{T} \right) \text{d}\GatingFunc(\gatingInducingInput)$.

*Optimisation*
The bound in cref:eq-lower-bound induces a local factorisation over observations and has a set of global variables
-- the necessary conditions to perform stochastic variational inference (SVI) [cite:@hoffmanStochastic2013] on
$\expertsInducingVariational$ and $\gatingsInducingVariational$, i.e.
optimise $\{\mode{\mathbf{m}}, \mode{\mathbf{L}}, \mode{\hat{\mathbf{m}}}, \mode{\hat{\mathbf{L}}}\}_{\modeInd=1}^{\ModeInd}$.
We treat the inducing inputs $\gatingInducingInput,\{\expertInducingInput\}_{\modeInd=1}^{\ModeInd}$,
kernel hyperparameters and noise variances, as
variational hyperparameters and optimise them alongside the variational parameters, using Adam citep:kingmaAdam2017.
We use mini-batches and approximate the expectations over the log-likelihood using Monte Carlo samples.

# Firstly, it contains a sum of terms corresponding to input-output pairs, enabling optimisation with mini-batches.
# Secondly, the expectations over the log-likelihood are calculated using Monte Carlo samples.

Our approach can loosely be viewed as parameterising the nonparametric model in cref:eq-np-moe-marginal-likelihood-main to obtain a desirable
factorisation for 1) constructing a GP-based gating network and 2) deriving an ELBO that can be optimised with stochastic gradient methods.
Importantly, our approach still captures the complex dependencies between the gating network and dynamic modes.


*Predictions*
Given our variational approximation, we make predictions at a new input $\singleInput$ with,
#+BEGIN_EXPORT latex
\small
\begin{equation} \label{eq-dynamics-approx-predictions}
\begin{aligned}
p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \dataset_{0:i})
&\approx \int p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))
q(\mode{\latentFunc}(\expertInducingInput)) \text{d} \mode{\latentFunc}(\expertInducingInput), \nonumber \\
p(\mode{\gatingFunc}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i})
&\approx \int p(\mode{\gatingFunc}(\state_{\timeInd}) \mid \mode{\gatingFunc}(\gatingInducingInput))
q(\mode{\gatingFunc}(\gatingInducingInput))
\text{d} \mode{\gatingFunc}(\gatingInducingInput). \nonumber
\end{aligned}
\end{equation}
\normalsize
#+END_EXPORT
See cref:eq--variational-posteriors-functional-gating,eq--variational-posteriors-functional-experts in cref:sec-sparse-approximations.
# Importantly, these are Gaussian convolutions and can be calculated in closed form.

# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-dynamics-approx-predictions}
# p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \dataset_{0:i})
# &\approx \int p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))
# q(\mode{\latentFunc}(\expertInducingInput)) \text{d} \mode{\latentFunc}(\expertInducingInput) \nonumber \\
# p(\mode{\gatingFunc}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i})
# &\approx \int p(\mode{\gatingFunc}(\state_{\timeInd}) \mid \mode{\gatingFunc}(\gatingInducingInput))
# q(\mode{\gatingFunc}(\gatingInducingInput))
# \text{d} \mode{\gatingFunc}(\gatingInducingInput).
# \end{align}
# #+END_EXPORT
# Importantly, these are Gaussian convolutions and can be calculated in closed form.
# #+BEGIN_EXPORT latex
# \begin{align} \label{eq-dynamics-approx-predictions}
# \underbrace{p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \dataset_{0:i})}_{\text{Dynamics GP posterior } \modeInd}
# &\approx \int p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))
# q(\mode{\latentFunc}(\expertInducingInput)) \text{d} \mode{\latentFunc}(\expertInducingInput) \nonumber \\
# \underbrace{p(\mode{\gatingFunc}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i})}_{\text{Gating GP posterior } \modeInd}
# &\approx \int p(\mode{\gatingFunc}(\state_{\timeInd}) \mid \mode{\gatingFunc}(\gatingInducingInput))
# q(\mode{\gatingFunc}(\gatingInducingInput))
# \text{d} \mode{\gatingFunc}(\gatingInducingInput).
# \end{align}
# #+END_EXPORT
# #+BEGIN_EXPORT latex
# \begin{align}
# p(\mode{\latentFunc}(\singleInput) &\mid \singleInput, \dataset_{0:i}) \approx
# q(\mode{\latentFunc}(\singleInput))  \\
# &= \int p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))
# q(\mode{\latentFunc}(\expertInducingInput)) \text{d} \mode{\latentFunc}(\expertInducingInput) \nonumber \\
# p(\mode{\gatingFunc}(\state_{\timeInd}) &\mid \state_{\timeInd}, \dataset_{0:i})
# \approx q(\mode{\gatingFunc}(\state_{\timeInd})) \\
# &= \int p(\mode{\gatingFunc}(\state_{\timeInd}) \mid \mode{\gatingFunc}(\gatingInducingInput))
# q(\mode{\gatingFunc}(\gatingInducingInput))
# \text{d} \mode{\gatingFunc}(\gatingInducingInput),
# \end{align}
# #+END_EXPORT

** FIGURE 2 - two column :ignore:
#+begin_export latex
\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.91\textwidth]{../experiments/figures/moderl_four_iterations_in_row.pdf}
  \caption{Visualisation of four episodes $i$ of ModeRL in the quadcopter navigation task from
  \cref{fig-problem-statement}.
  The goal is to navigate to the black star, whilst avoiding the turbulent dynamic mode (dashed red line).
  The contour plots indicate the agent's belief of being in the desired dynamic mode $\Pr(\modeVar=\desiredMode \mid \state, \dataset_{0:i})$ at each episode,
  i.e. after training on $\dataset_{0:i}$.
  The black lines show the $\delta\text{-mode constraint}$ (see \cref{eq-expected-constraint}) expanding during training.
  At each episode $i$ we roll out the policy in the desired dynamic mode's GP (magenta) as well as the in the environment (cyan). Experiments used an exponentially decaying schedule on $\delta$ to tighten the mode constraint during training.}
  %The environment trajectories (cyan) deviate from the dynamics trajectories (magenta) when the trajectories leave the desired dynamic mode.}
  \label{fig-joint-entropy-four-episodes}
\end{figure*}
#+end_export
  # \caption{Visualisation of four iterations $i$ of ModeRL in the quadcopter navigation task from
  # \cref{fig-constraint-expanding}. % \cref{fig-problem-statement}.
  # The goal is to navigate to the target state $\targetState$, from the start state $\state_0$, whilst avoiding the undesired dynamic modeindicated
  # by the mode boundary (dashed blue line).
  # Reading left to right the contour plots indicate how the agent's belief of being in the desired dynamic mode$\Pr(\modeVar=\desiredMode \mid \state, \dataset_{0:i})$ changes as it collects data and updates its dynamic model, i.e. trains on $\dataset_{0:i}$.
  # The black contour lines ($\Pr(\modeVar=\desiredMode \mid \state, \dataset_{0:i})=0.8$) show how the $\delta\text{-mode constraint}$ expands during learning.
  # At each episode $i$ the optimised trajectory is rolled out in the desired mode's GP dynamics (magenta) as well as the in the environment (cyan).}
*** Figure 2 - two column :ignore:noexport:
#+begin_export latex
\begin{figure*}[!t]
    \input{figures/figure-3-0.tex}
    \hskip 5pt
    \input{figures/figure-3-1.tex}
    \hskip 5pt
    \begin{tikzpicture}
        \begin{axis}
        \addplot[color=red]{exp(x)};
        \end{axis}
    \end{tikzpicture}
    %Here ends the 2D plot
    \hskip 5pt
    %Here begins the 3D plot
    \begin{tikzpicture}
        \begin{axis}
        \addplot3[
            surf,
        ]
        {exp(-x^2-y^2)*x};
        \end{axis}
    \end{tikzpicture}
\end{figure*}
#+end_export
*** yoyo :ignore:noexport:
#+begin_export latex
\begin{figure}
  \centering\footnotesize

  % Set your figure size here
  %\setlength{\figurewidth}{.33\textwidth}
  %\setlength{\figureheight}{.75\figurewidth}

  % Customize your plot here
  % (scale only axis applies the size to the axis box and not entire figure)
  %\pgfplotsset{grid style={dotted},title={Foo},scale only axis}

  % Use the subcaption package (= subfigure) for sub-plots, that is
  % plot the separate plots separately in Python
  \begin{subfigure}{.48\textwidth}
    \centering
    %\input{../python/simplefigure.tex}
    %\input{./figures/figure-3-no-width}
    \include{./figures/figure-3-no-width}
    %\include{simplefig.tex}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.48\textwidth}
    \centering
    %\input{../python/simplefigure.tex}
    \input{simplefig}
    % \input{./figures/figure-3-no-width}
  \end{subfigure}
  \caption{Foo}
\end{figure}
#+end_export

*** Figure 2 - two column :ignore:noexport:
#+begin_export latex
\begin{figure*}[!t]
\centering
\includegraphics[width=0.7\textwidth]{./images/mode-opt/exploration/mvn-full-cov/data_over_mixing_probs_step_0_epoch_0.pdf}
\caption{The goal is to navigate from the start state $\state_0$ to target state $\targetState$ whilst avoiding the undesired dynamic modeindicated by the mode boundary (purple line). Initially only a small region around the start state is believed to belong to the desired dynamic mode.}
\label{fig-explorative-traj-opt-7-initial}
\end{figure*}
#+end_export
*** Tikz fig :noexport:
#+begin_export latex
\begin{figure*}
\begin{tikzpicture}

\definecolor{brown1315655}{RGB}{131,56,55}
\definecolor{darkgray176}{RGB}{176,176,176}
\definecolor{indianred16512094}{RGB}{165,120,94}
\definecolor{lightgray204}{RGB}{204,204,204}
\definecolor{linen234234233}{RGB}{234,234,233}
\definecolor{maroon941819}{RGB}{94,18,19}
\definecolor{rosybrown174149110}{RGB}{174,149,110}
\definecolor{sienna1559181}{RGB}{155,91,81}
\definecolor{silver202199187}{RGB}{202,199,187}
\definecolor{tan187177144}{RGB}{187,177,144}

\begin{axis}[
legend style={fill opacity=0.8, draw opacity=1, text opacity=1, draw=lightgray204},
tick align=outside,
tick pos=left,
x grid style={darkgray176},
xlabel={\(\displaystyle x\)},
xmin=-3, xmax=3,
xtick style={color=black},
y grid style={darkgray176},
ylabel={\(\displaystyle y\)},
ymin=-3, ymax=3,
ytick style={color=black}
]
\addplot [draw=none, fill=linen234234233, forget plot]
table{%
x  y
0.245607915584267 0.333333333333333
0.167871114910802 0.167871114910802
-0.157104640405243 -0.333333333333333
0.333333333333333 -0.611396491220371
0.907325876548355 -0.426007456784978
1 -0.409911281813271
1.06438394000584 -0.333333333333333
1 -0.218190296371186
0.399268199834164 0.333333333333333
0.333333333333333 0.351296711045954
};
\addplot [draw=none, fill=silver202199187, forget plot]
table{%
x  y
0.333333333333333 0.351296711045954
0.399268199834164 0.333333333333333
1 -0.218190296371186
1.06438394000584 -0.333333333333333
1 -0.409911281813271
0.907325876548355 -0.426007456784978
0.333333333333333 -0.611396491220371
-0.157104640405243 -0.333333333333333
0.167871114910802 0.167871114910802
0.245607915584267 0.333333333333333

0.333333333333333 -1.16927749346174
0.884808062925314 -1.11519193707469
1 -1.1131954564453
1.08869106554495 -1.08869106554495
1.22173705199636 -1
1.66666666666667 -0.42348152084083
1.70029205524435 -0.366958721911018
1.70845791049426 -0.333333333333333
1.69544183757911 -0.304558162420885
1.66666666666667 -0.254337584278635
1.32449561343265 0.333333333333333
1 0.505353961974804
0.750224743996082 0.583108589337251
0.333333333333333 0.696687008045173
-0.0480812427607085 0.618585423905958
-0.333333333333333 0.626442909281786
-0.547690848065048 0.547690848065047
-0.738791963799092 0.333333333333333
-0.7587732689089 -0.0921066022422331
-0.795644842375952 -0.333333333333333
-0.333333333333333 -0.898072316081978
-0.145380724128233 -1
};
\addplot [draw=none, fill=tan187177144, forget plot]
table{%
x  y
-0.145380724128233 -1
-0.333333333333333 -0.898072316081978
-0.795644842375952 -0.333333333333333
-0.7587732689089 -0.0921066022422331
-0.738791963799092 0.333333333333333
-0.547690848065048 0.547690848065047
-0.333333333333333 0.626442909281786
-0.0480812427607084 0.618585423905958
0.333333333333333 0.696687008045173
0.750224743996082 0.583108589337251
1 0.505353961974804
1.32449561343265 0.333333333333333
1.66666666666667 -0.254337584278635
1.69544183757911 -0.304558162420885
1.70845791049426 -0.333333333333333
1.70029205524435 -0.366958721911018
1.66666666666667 -0.42348152084083
1.22173705199636 -1
1.08869106554495 -1.08869106554495
1 -1.1131954564453
0.884808062925314 -1.11519193707469
0.333333333333333 -1.16927749346174

0.240059021505137 -1.57339235483847
0.333333333333333 -1.60637497037805
0.400181082939876 -1.59981891706012
1 -1.58942299109984
1.46182554299426 -1.46182554299426
1.66666666666667 -1.32527443002579
2.04279617046063 -1
2.15116690167517 -0.817833568341835
2.26882676985741 -0.333333333333333
2.08128126769298 0.0812812676929764
1.93686155896136 0.333333333333333
1.79059729737652 0.457263964043187
1.66666666666667 0.501005360099536
1.3569019629225 0.643098037077497
1 0.832297821246682
0.461281520451922 1
0.360965124526025 1.02763179119269
0.333333333333333 1.03522017732103
-0.287043190246849 1.04629014308648
-0.333333333333333 1.05681106725162
-0.531308322300692 1
-0.874072910874658 0.874072910874658
-1 0.732820894848367
-1.3156624683615 0.333333333333333
-1.32968997976312 -0.00364335357021733
-1.3260112843089 -0.333333333333333
-1.23587129738103 -0.569204630714362
-1 -0.886088802862259
-0.911771275825894 -1
-0.616724520301798 -1.28339118696847
-0.333333333333333 -1.41412628176216
};
\addplot [draw=none, fill=rosybrown174149110, forget plot]
table{%
x  y
-0.333333333333333 -1.41412628176216
-0.616724520301798 -1.28339118696847
-0.911771275825894 -1
-1 -0.886088802862259
-1.23587129738103 -0.569204630714362
-1.3260112843089 -0.333333333333333
-1.32968997976312 -0.00364335357021739
-1.3156624683615 0.333333333333333
-1 0.732820894848367
-0.874072910874658 0.874072910874658
-0.531308322300692 1
-0.333333333333333 1.05681106725162
-0.287043190246849 1.04629014308648
0.333333333333333 1.03522017732103
0.360965124526025 1.02763179119269
0.461281520451922 1
1 0.832297821246682
1.3569019629225 0.643098037077497
1.66666666666667 0.501005360099536
1.79059729737652 0.457263964043187
1.93686155896136 0.333333333333333
2.08128126769298 0.0812812676929764
2.26882676985741 -0.333333333333333
2.15116690167517 -0.817833568341835
2.04279617046063 -1
1.66666666666667 -1.32527443002579
1.46182554299426 -1.46182554299426
1 -1.58942299109984
0.400181082939876 -1.59981891706012
0.333333333333333 -1.60637497037805
0.240059021505137 -1.57339235483847

2.75880393719645 0.0921372705297835
2.60580889722152 0.333333333333333
2.47084835297079 0.470848352970787
2.33333333333333 0.511644092460395
2.06897224345392 0.73563891012059
1.66666666666667 0.877632922973863
1.39990414118536 1
1.11838216297327 1.11838216297327
1 1.14090196121318
0.587779854143226 1.25444652080989
0.333333333333333 1.32432394697643
-0.00300773419639971 1.33032559913693
-0.333333333333333 1.4054027180784
-0.699742304624778 1.30025769537522
-1 1.30782264071896
-1.25828481278235 1.25828481278235
-1.51970038566465 1
-1.66666666666667 0.782435918575453
-1.85916213206403 0.525828798730701
-1.90975352109638 0.333333333333333
-1.90150151546255 0.0984984845374523
-1.89533894530008 -0.333333333333333
-1.66666666666667 -0.801189086022037
-1.61169674982749 -0.945030083160827
-1.57078014135854 -1
-1 -1.57047075629521
-0.950932867357416 -1.61759953402408
-0.844571274223885 -1.66666666666667
-0.333333333333333 -1.95339717672668
-0.0493089604644225 -2.04930896046442
0.333333333333333 -2.11857324719435
0.795948655891358 -2.12928198922469
1 -2.12521741963483
1.30494885722388 -2.02838447610946
1.66666666666667 -1.96790171517326
1.90088704403025 -1.90088704403025
2.19792663917209 -1.66666666666667
2.33333333333333 -1.5588669137967
2.80213596056312 -1
2.85243921705272 -0.852439217052719
2.92013170367181 -0.333333333333333
};
\addplot [draw=none, fill=indianred16512094, forget plot]
table{%
x  y
3 0.732367423567039
2.86743824782233 0.867438247822325
2.42059668588589 1
2.33333333333333 1.02404491142374
2.30763876360418 1.02569456972915
1.66666666666667 1.23308432701589
1.36143612413088 1.36143612413088
1 1.43019199399856
0.814594583760426 1.48126125042709
0.333333333333333 1.61342771663183
0.28102772185405 1.61436105518738
0.0508925928863343 1.66666666666667
-0.333333333333333 1.78845881389742
-0.497208645702679 1.83054197903601
-1 1.84654922127264
-1.64249780589116 1.69083552744217
-1.66666666666667 1.69451589936635
-1.69284490559702 1.69284490559702
-1.7240748786139 1.66666666666667
-2.33333333333333 1.18387238064036
-2.48801395057628 1
-2.66162215513959 0.661622155139589
-2.71765337588091 0.333333333333333
-2.70182789412858 0.0351612274619113
-2.68290574051516 -0.333333333333333
-2.61632222893749 -0.616322228937494
-2.40581448357539 -1
-2.33333333333333 -1.10082495408046
-2.1531309250703 -1.48646425840364
-1.9259883315226 -1.66666666666667
-1.66666666666667 -1.91093678147532
-1.47346697554492 -2.14013364221158
-1.01831504134129 -2.33333333333333
-1 -2.34552730023033
-0.603703307327851 -2.60370330732785
-0.333333333333333 -2.67077463045355
0.077759999309006 -2.74442666597567
0.333333333333333 -2.81668693407283
0.8411350590742 -2.8411350590742
1 -2.84694875301965
1.19311933540681 -2.80688066459319
1.66666666666667 -2.76926955659765
2.03734061897897 -2.70400728564564
2.33333333333333 -2.54417071759324
2.69194244369326 -2.33333333333333
2.90688633441036 -2.2402196677437
3 -2.12799135782494
3 -1.66666666666667
3 -1
3 -0.333333333333333
3 0.333333333333333

2.92013170367181 -0.333333333333333
2.85243921705272 -0.852439217052719
2.80213596056312 -1
2.33333333333333 -1.5588669137967
2.19792663917209 -1.66666666666667
1.90088704403025 -1.90088704403025
1.66666666666667 -1.96790171517326
1.30494885722388 -2.02838447610946
1 -2.12521741963483
0.795948655891358 -2.12928198922469
0.333333333333333 -2.11857324719435
-0.0493089604644225 -2.04930896046442
-0.333333333333333 -1.95339717672668
-0.844571274223885 -1.66666666666667
-0.950932867357416 -1.61759953402408
-1 -1.57047075629521
-1.57078014135854 -1
-1.61169674982749 -0.945030083160827
-1.66666666666667 -0.801189086022037
-1.89533894530008 -0.333333333333333
-1.90150151546255 0.0984984845374523
-1.90975352109638 0.333333333333333
-1.85916213206403 0.525828798730701
-1.66666666666667 0.782435918575453
-1.51970038566465 1
-1.25828481278235 1.25828481278235
-1 1.30782264071896
-0.699742304624778 1.30025769537522
-0.333333333333333 1.4054027180784
-0.00300773419639971 1.33032559913693
0.333333333333333 1.32432394697643
0.587779854143226 1.25444652080989
1 1.14090196121318
1.11838216297327 1.11838216297327
1.39990414118536 1
1.66666666666667 0.877632922973863
2.06897224345392 0.73563891012059
2.33333333333333 0.511644092460395
2.47084835297079 0.470848352970787
2.60580889722152 0.333333333333333
2.75880393719645 0.0921372705297835
};
\addplot [draw=none, fill=sienna1559181, forget plot]
table{%
x  y
3 1.49283121320794
2.56107293731372 1.43892706268628
2.33333333333333 1.50167939164462
1.79723503448227 1.53609829885106
1.66666666666667 1.57834435330489
1.6044900852885 1.6044900852885
1.27763999047793 1.66666666666667
1 1.75461616632251
0.859454303344676 1.80721236332199
0.333333333333333 2.02944015122116
-0.24737837337928 2.24737837337928
-0.333333333333333 2.27462442426641
-0.561950662389267 2.33333333333333
-0.833736880167515 2.49959645316582
-1 2.62160512838037
-1.51179054229897 2.8451238756323
-1.66666666666667 2.88525001544689
-2.14175593726264 3
-2.33333333333333 3
-3 3
-3 2.33333333333333
-3 1.66666666666667
-3 1
-3 0.333333333333333
-3 -0.333333333333333
-3 -1
-3 -1.66666666666667
-3 -2.33333333333333
-3 -3
-2.33333333333333 -3
-1.66666666666667 -3
-1 -3
-0.333333333333333 -3
0.333333333333333 -3
1 -3
1.66666666666667 -3
2.33333333333333 -3
3 -3
3 -2.33333333333333
3 -2.12799135782494
2.90688633441036 -2.2402196677437
2.69194244369326 -2.33333333333333
2.33333333333333 -2.54417071759324
2.03734061897897 -2.70400728564564
1.66666666666667 -2.76926955659765
1.19311933540681 -2.80688066459319
1 -2.84694875301965
0.8411350590742 -2.8411350590742
0.333333333333333 -2.81668693407283
0.077759999309006 -2.74442666597567
-0.333333333333333 -2.67077463045355
-0.603703307327851 -2.60370330732785
-1 -2.34552730023033
-1.01831504134129 -2.33333333333333
-1.47346697554492 -2.14013364221158
-1.66666666666667 -1.91093678147532
-1.9259883315226 -1.66666666666667
-2.15313092507031 -1.48646425840364
-2.33333333333333 -1.10082495408046
-2.40581448357539 -1
-2.61632222893749 -0.616322228937494
-2.68290574051516 -0.333333333333333
-2.70182789412858 0.0351612274619113
-2.71765337588091 0.333333333333333
-2.66162215513959 0.661622155139589
-2.48801395057628 1
-2.33333333333333 1.18387238064036
-1.7240748786139 1.66666666666667
-1.69284490559702 1.69284490559702
-1.66666666666667 1.69451589936635
-1.64249780589116 1.69083552744217
-1 1.84654922127264
-0.497208645702679 1.83054197903601
-0.333333333333333 1.78845881389742
0.0508925928863344 1.66666666666667
0.28102772185405 1.61436105518738
0.333333333333333 1.61342771663183
0.814594583760426 1.48126125042709
1 1.43019199399856
1.36143612413088 1.36143612413088
1.66666666666667 1.23308432701589
2.30763876360418 1.02569456972915
2.33333333333333 1.02404491142374
2.42059668588589 1
2.86743824782233 0.867438247822325
3 0.732367423567039
3 1
};
\addplot [draw=none, fill=brown1315655, forget plot]
table{%
x  y
2.89904626738403 3
2.74951326002755 2.58382007330578
2.64901655929164 2.33333333333333
2.33333333333333 2.19246921673204
2.20619671624256 2.20619671624256
1.66666666666667 2.10921753678532
1.1419502962439 2.19138303708943
1 2.23634938905177
0.740842935591537 2.33333333333333
0.586269753416008 2.58626975341601
0.333333333333333 2.76988780285031
0.226992102662133 2.8936587693288
0.0968784535384012 3
-0.333333333333333 3
-1 3
-1.66666666666667 3
-2.14175593726264 3
-1.66666666666667 2.88525001544689
-1.51179054229897 2.8451238756323
-1 2.62160512838037
-0.833736880167515 2.49959645316582
-0.561950662389267 2.33333333333333
-0.333333333333333 2.27462442426642
-0.24737837337928 2.24737837337928
0.333333333333333 2.02944015122116
0.859454303344676 1.80721236332199
1 1.75461616632251
1.27763999047793 1.66666666666667
1.6044900852885 1.6044900852885
1.66666666666667 1.57834435330489
1.79723503448227 1.53609829885106
2.33333333333333 1.50167939164462
2.56107293731372 1.43892706268628
3 1.49283121320794
3 1.66666666666667
3 2.33333333333333
3 3
};
\addplot [draw=none, fill=maroon941819, forget plot]
table{%
x  y
0.0968784535384012 3
0.226992102662133 2.8936587693288
0.333333333333333 2.76988780285031
0.586269753416008 2.58626975341601
0.740842935591537 2.33333333333333
1 2.23634938905177
1.1419502962439 2.19138303708943
1.66666666666667 2.10921753678532
2.20619671624256 2.20619671624256
2.33333333333333 2.19246921673204
2.64901655929164 2.33333333333333
2.74951326002755 2.58382007330578
2.89904626738403 3
2.33333333333333 3
1.66666666666667 3
1 3
0.333333333333333 3
};
\end{axis}

\end{tikzpicture}
%
\hskip 10pt
%
\begin{tikzpicture}

\definecolor{brown1315655}{RGB}{131,56,55}
\definecolor{darkgray176}{RGB}{176,176,176}
\definecolor{indianred16512094}{RGB}{165,120,94}
\definecolor{lightgray204}{RGB}{204,204,204}
\definecolor{linen234234233}{RGB}{234,234,233}
\definecolor{maroon941819}{RGB}{94,18,19}
\definecolor{rosybrown174149110}{RGB}{174,149,110}
\definecolor{sienna1559181}{RGB}{155,91,81}
\definecolor{silver202199187}{RGB}{202,199,187}
\definecolor{tan187177144}{RGB}{187,177,144}

\begin{axis}[
legend style={fill opacity=0.8, draw opacity=1, text opacity=1, draw=lightgray204},
tick align=outside,
tick pos=left,
x grid style={darkgray176},
xlabel={\(\displaystyle x\)},
xmin=-3, xmax=3,
xtick style={color=black},
y grid style={darkgray176},
% ylabel={\(\displaystyle y\)},
ymin=-3, ymax=3,
ytick style={color=black}
]
\addplot [draw=none, fill=linen234234233, forget plot]
table{%
x  y
0.245607915584267 0.333333333333333
0.167871114910802 0.167871114910802
-0.157104640405243 -0.333333333333333
0.333333333333333 -0.611396491220371
0.907325876548355 -0.426007456784978
1 -0.409911281813271
1.06438394000584 -0.333333333333333
1 -0.218190296371186
0.399268199834164 0.333333333333333
0.333333333333333 0.351296711045954
};
\addplot [draw=none, fill=silver202199187, forget plot]
table{%
x  y
0.333333333333333 0.351296711045954
0.399268199834164 0.333333333333333
1 -0.218190296371186
1.06438394000584 -0.333333333333333
1 -0.409911281813271
0.907325876548355 -0.426007456784978
0.333333333333333 -0.611396491220371
-0.157104640405243 -0.333333333333333
0.167871114910802 0.167871114910802
0.245607915584267 0.333333333333333

0.333333333333333 -1.16927749346174
0.884808062925314 -1.11519193707469
1 -1.1131954564453
1.08869106554495 -1.08869106554495
1.22173705199636 -1
1.66666666666667 -0.42348152084083
1.70029205524435 -0.366958721911018
1.70845791049426 -0.333333333333333
1.69544183757911 -0.304558162420885
1.66666666666667 -0.254337584278635
1.32449561343265 0.333333333333333
1 0.505353961974804
0.750224743996082 0.583108589337251
0.333333333333333 0.696687008045173
-0.0480812427607085 0.618585423905958
-0.333333333333333 0.626442909281786
-0.547690848065048 0.547690848065047
-0.738791963799092 0.333333333333333
-0.7587732689089 -0.0921066022422331
-0.795644842375952 -0.333333333333333
-0.333333333333333 -0.898072316081978
-0.145380724128233 -1
};
\addplot [draw=none, fill=tan187177144, forget plot]
table{%
x  y
-0.145380724128233 -1
-0.333333333333333 -0.898072316081978
-0.795644842375952 -0.333333333333333
-0.7587732689089 -0.0921066022422331
-0.738791963799092 0.333333333333333
-0.547690848065048 0.547690848065047
-0.333333333333333 0.626442909281786
-0.0480812427607084 0.618585423905958
0.333333333333333 0.696687008045173
0.750224743996082 0.583108589337251
1 0.505353961974804
1.32449561343265 0.333333333333333
1.66666666666667 -0.254337584278635
1.69544183757911 -0.304558162420885
1.70845791049426 -0.333333333333333
1.70029205524435 -0.366958721911018
1.66666666666667 -0.42348152084083
1.22173705199636 -1
1.08869106554495 -1.08869106554495
1 -1.1131954564453
0.884808062925314 -1.11519193707469
0.333333333333333 -1.16927749346174

0.240059021505137 -1.57339235483847
0.333333333333333 -1.60637497037805
0.400181082939876 -1.59981891706012
1 -1.58942299109984
1.46182554299426 -1.46182554299426
1.66666666666667 -1.32527443002579
2.04279617046063 -1
2.15116690167517 -0.817833568341835
2.26882676985741 -0.333333333333333
2.08128126769298 0.0812812676929764
1.93686155896136 0.333333333333333
1.79059729737652 0.457263964043187
1.66666666666667 0.501005360099536
1.3569019629225 0.643098037077497
1 0.832297821246682
0.461281520451922 1
0.360965124526025 1.02763179119269
0.333333333333333 1.03522017732103
-0.287043190246849 1.04629014308648
-0.333333333333333 1.05681106725162
-0.531308322300692 1
-0.874072910874658 0.874072910874658
-1 0.732820894848367
-1.3156624683615 0.333333333333333
-1.32968997976312 -0.00364335357021733
-1.3260112843089 -0.333333333333333
-1.23587129738103 -0.569204630714362
-1 -0.886088802862259
-0.911771275825894 -1
-0.616724520301798 -1.28339118696847
-0.333333333333333 -1.41412628176216
};
\addplot [draw=none, fill=rosybrown174149110, forget plot]
table{%
x  y
-0.333333333333333 -1.41412628176216
-0.616724520301798 -1.28339118696847
-0.911771275825894 -1
-1 -0.886088802862259
-1.23587129738103 -0.569204630714362
-1.3260112843089 -0.333333333333333
-1.32968997976312 -0.00364335357021739
-1.3156624683615 0.333333333333333
-1 0.732820894848367
-0.874072910874658 0.874072910874658
-0.531308322300692 1
-0.333333333333333 1.05681106725162
-0.287043190246849 1.04629014308648
0.333333333333333 1.03522017732103
0.360965124526025 1.02763179119269
0.461281520451922 1
1 0.832297821246682
1.3569019629225 0.643098037077497
1.66666666666667 0.501005360099536
1.79059729737652 0.457263964043187
1.93686155896136 0.333333333333333
2.08128126769298 0.0812812676929764
2.26882676985741 -0.333333333333333
2.15116690167517 -0.817833568341835
2.04279617046063 -1
1.66666666666667 -1.32527443002579
1.46182554299426 -1.46182554299426
1 -1.58942299109984
0.400181082939876 -1.59981891706012
0.333333333333333 -1.60637497037805
0.240059021505137 -1.57339235483847

2.75880393719645 0.0921372705297835
2.60580889722152 0.333333333333333
2.47084835297079 0.470848352970787
2.33333333333333 0.511644092460395
2.06897224345392 0.73563891012059
1.66666666666667 0.877632922973863
1.39990414118536 1
1.11838216297327 1.11838216297327
1 1.14090196121318
0.587779854143226 1.25444652080989
0.333333333333333 1.32432394697643
-0.00300773419639971 1.33032559913693
-0.333333333333333 1.4054027180784
-0.699742304624778 1.30025769537522
-1 1.30782264071896
-1.25828481278235 1.25828481278235
-1.51970038566465 1
-1.66666666666667 0.782435918575453
-1.85916213206403 0.525828798730701
-1.90975352109638 0.333333333333333
-1.90150151546255 0.0984984845374523
-1.89533894530008 -0.333333333333333
-1.66666666666667 -0.801189086022037
-1.61169674982749 -0.945030083160827
-1.57078014135854 -1
-1 -1.57047075629521
-0.950932867357416 -1.61759953402408
-0.844571274223885 -1.66666666666667
-0.333333333333333 -1.95339717672668
-0.0493089604644225 -2.04930896046442
0.333333333333333 -2.11857324719435
0.795948655891358 -2.12928198922469
1 -2.12521741963483
1.30494885722388 -2.02838447610946
1.66666666666667 -1.96790171517326
1.90088704403025 -1.90088704403025
2.19792663917209 -1.66666666666667
2.33333333333333 -1.5588669137967
2.80213596056312 -1
2.85243921705272 -0.852439217052719
2.92013170367181 -0.333333333333333
};
\addplot [draw=none, fill=indianred16512094, forget plot]
table{%
x  y
3 0.732367423567039
2.86743824782233 0.867438247822325
2.42059668588589 1
2.33333333333333 1.02404491142374
2.30763876360418 1.02569456972915
1.66666666666667 1.23308432701589
1.36143612413088 1.36143612413088
1 1.43019199399856
0.814594583760426 1.48126125042709
0.333333333333333 1.61342771663183
0.28102772185405 1.61436105518738
0.0508925928863343 1.66666666666667
-0.333333333333333 1.78845881389742
-0.497208645702679 1.83054197903601
-1 1.84654922127264
-1.64249780589116 1.69083552744217
-1.66666666666667 1.69451589936635
-1.69284490559702 1.69284490559702
-1.7240748786139 1.66666666666667
-2.33333333333333 1.18387238064036
-2.48801395057628 1
-2.66162215513959 0.661622155139589
-2.71765337588091 0.333333333333333
-2.70182789412858 0.0351612274619113
-2.68290574051516 -0.333333333333333
-2.61632222893749 -0.616322228937494
-2.40581448357539 -1
-2.33333333333333 -1.10082495408046
-2.1531309250703 -1.48646425840364
-1.9259883315226 -1.66666666666667
-1.66666666666667 -1.91093678147532
-1.47346697554492 -2.14013364221158
-1.01831504134129 -2.33333333333333
-1 -2.34552730023033
-0.603703307327851 -2.60370330732785
-0.333333333333333 -2.67077463045355
0.077759999309006 -2.74442666597567
0.333333333333333 -2.81668693407283
0.8411350590742 -2.8411350590742
1 -2.84694875301965
1.19311933540681 -2.80688066459319
1.66666666666667 -2.76926955659765
2.03734061897897 -2.70400728564564
2.33333333333333 -2.54417071759324
2.69194244369326 -2.33333333333333
2.90688633441036 -2.2402196677437
3 -2.12799135782494
3 -1.66666666666667
3 -1
3 -0.333333333333333
3 0.333333333333333

2.92013170367181 -0.333333333333333
2.85243921705272 -0.852439217052719
2.80213596056312 -1
2.33333333333333 -1.5588669137967
2.19792663917209 -1.66666666666667
1.90088704403025 -1.90088704403025
1.66666666666667 -1.96790171517326
1.30494885722388 -2.02838447610946
1 -2.12521741963483
0.795948655891358 -2.12928198922469
0.333333333333333 -2.11857324719435
-0.0493089604644225 -2.04930896046442
-0.333333333333333 -1.95339717672668
-0.844571274223885 -1.66666666666667
-0.950932867357416 -1.61759953402408
-1 -1.57047075629521
-1.57078014135854 -1
-1.61169674982749 -0.945030083160827
-1.66666666666667 -0.801189086022037
-1.89533894530008 -0.333333333333333
-1.90150151546255 0.0984984845374523
-1.90975352109638 0.333333333333333
-1.85916213206403 0.525828798730701
-1.66666666666667 0.782435918575453
-1.51970038566465 1
-1.25828481278235 1.25828481278235
-1 1.30782264071896
-0.699742304624778 1.30025769537522
-0.333333333333333 1.4054027180784
-0.00300773419639971 1.33032559913693
0.333333333333333 1.32432394697643
0.587779854143226 1.25444652080989
1 1.14090196121318
1.11838216297327 1.11838216297327
1.39990414118536 1
1.66666666666667 0.877632922973863
2.06897224345392 0.73563891012059
2.33333333333333 0.511644092460395
2.47084835297079 0.470848352970787
2.60580889722152 0.333333333333333
2.75880393719645 0.0921372705297835
};
\addplot [draw=none, fill=sienna1559181, forget plot]
table{%
x  y
3 1.49283121320794
2.56107293731372 1.43892706268628
2.33333333333333 1.50167939164462
1.79723503448227 1.53609829885106
1.66666666666667 1.57834435330489
1.6044900852885 1.6044900852885
1.27763999047793 1.66666666666667
1 1.75461616632251
0.859454303344676 1.80721236332199
0.333333333333333 2.02944015122116
-0.24737837337928 2.24737837337928
-0.333333333333333 2.27462442426641
-0.561950662389267 2.33333333333333
-0.833736880167515 2.49959645316582
-1 2.62160512838037
-1.51179054229897 2.8451238756323
-1.66666666666667 2.88525001544689
-2.14175593726264 3
-2.33333333333333 3
-3 3
-3 2.33333333333333
-3 1.66666666666667
-3 1
-3 0.333333333333333
-3 -0.333333333333333
-3 -1
-3 -1.66666666666667
-3 -2.33333333333333
-3 -3
-2.33333333333333 -3
-1.66666666666667 -3
-1 -3
-0.333333333333333 -3
0.333333333333333 -3
1 -3
1.66666666666667 -3
2.33333333333333 -3
3 -3
3 -2.33333333333333
3 -2.12799135782494
2.90688633441036 -2.2402196677437
2.69194244369326 -2.33333333333333
2.33333333333333 -2.54417071759324
2.03734061897897 -2.70400728564564
1.66666666666667 -2.76926955659765
1.19311933540681 -2.80688066459319
1 -2.84694875301965
0.8411350590742 -2.8411350590742
0.333333333333333 -2.81668693407283
0.077759999309006 -2.74442666597567
-0.333333333333333 -2.67077463045355
-0.603703307327851 -2.60370330732785
-1 -2.34552730023033
-1.01831504134129 -2.33333333333333
-1.47346697554492 -2.14013364221158
-1.66666666666667 -1.91093678147532
-1.9259883315226 -1.66666666666667
-2.15313092507031 -1.48646425840364
-2.33333333333333 -1.10082495408046
-2.40581448357539 -1
-2.61632222893749 -0.616322228937494
-2.68290574051516 -0.333333333333333
-2.70182789412858 0.0351612274619113
-2.71765337588091 0.333333333333333
-2.66162215513959 0.661622155139589
-2.48801395057628 1
-2.33333333333333 1.18387238064036
-1.7240748786139 1.66666666666667
-1.69284490559702 1.69284490559702
-1.66666666666667 1.69451589936635
-1.64249780589116 1.69083552744217
-1 1.84654922127264
-0.497208645702679 1.83054197903601
-0.333333333333333 1.78845881389742
0.0508925928863344 1.66666666666667
0.28102772185405 1.61436105518738
0.333333333333333 1.61342771663183
0.814594583760426 1.48126125042709
1 1.43019199399856
1.36143612413088 1.36143612413088
1.66666666666667 1.23308432701589
2.30763876360418 1.02569456972915
2.33333333333333 1.02404491142374
2.42059668588589 1
2.86743824782233 0.867438247822325
3 0.732367423567039
3 1
};
\addplot [draw=none, fill=brown1315655, forget plot]
table{%
x  y
2.89904626738403 3
2.74951326002755 2.58382007330578
2.64901655929164 2.33333333333333
2.33333333333333 2.19246921673204
2.20619671624256 2.20619671624256
1.66666666666667 2.10921753678532
1.1419502962439 2.19138303708943
1 2.23634938905177
0.740842935591537 2.33333333333333
0.586269753416008 2.58626975341601
0.333333333333333 2.76988780285031
0.226992102662133 2.8936587693288
0.0968784535384012 3
-0.333333333333333 3
-1 3
-1.66666666666667 3
-2.14175593726264 3
-1.66666666666667 2.88525001544689
-1.51179054229897 2.8451238756323
-1 2.62160512838037
-0.833736880167515 2.49959645316582
-0.561950662389267 2.33333333333333
-0.333333333333333 2.27462442426642
-0.24737837337928 2.24737837337928
0.333333333333333 2.02944015122116
0.859454303344676 1.80721236332199
1 1.75461616632251
1.27763999047793 1.66666666666667
1.6044900852885 1.6044900852885
1.66666666666667 1.57834435330489
1.79723503448227 1.53609829885106
2.33333333333333 1.50167939164462
2.56107293731372 1.43892706268628
3 1.49283121320794
3 1.66666666666667
3 2.33333333333333
3 3
};
\addplot [draw=none, fill=maroon941819, forget plot]
table{%
x  y
0.0968784535384012 3
0.226992102662133 2.8936587693288
0.333333333333333 2.76988780285031
0.586269753416008 2.58626975341601
0.740842935591537 2.33333333333333
1 2.23634938905177
1.1419502962439 2.19138303708943
1.66666666666667 2.10921753678532
2.20619671624256 2.20619671624256
2.33333333333333 2.19246921673204
2.64901655929164 2.33333333333333
2.74951326002755 2.58382007330578
2.89904626738403 3
2.33333333333333 3
1.66666666666667 3
1 3
0.333333333333333 3
};
\end{axis}

\end{tikzpicture}
%
\hskip 10pt
%
\begin{tikzpicture}

\definecolor{brown1315655}{RGB}{131,56,55}
\definecolor{darkgray176}{RGB}{176,176,176}
\definecolor{indianred16512094}{RGB}{165,120,94}
\definecolor{lightgray204}{RGB}{204,204,204}
\definecolor{linen234234233}{RGB}{234,234,233}
\definecolor{maroon941819}{RGB}{94,18,19}
\definecolor{rosybrown174149110}{RGB}{174,149,110}
\definecolor{sienna1559181}{RGB}{155,91,81}
\definecolor{silver202199187}{RGB}{202,199,187}
\definecolor{tan187177144}{RGB}{187,177,144}

\begin{axis}[
legend style={fill opacity=0.8, draw opacity=1, text opacity=1, draw=lightgray204},
tick align=outside,
tick pos=left,
x grid style={darkgray176},
xlabel={\(\displaystyle x\)},
xmin=-3, xmax=3,
xtick style={color=black},
y grid style={darkgray176},
% ylabel={\(\displaystyle y\)},
ymin=-3, ymax=3,
ytick style={color=black}
]
\addplot [draw=none, fill=linen234234233, forget plot]
table{%
x  y
0.245607915584267 0.333333333333333
0.167871114910802 0.167871114910802
-0.157104640405243 -0.333333333333333
0.333333333333333 -0.611396491220371
0.907325876548355 -0.426007456784978
1 -0.409911281813271
1.06438394000584 -0.333333333333333
1 -0.218190296371186
0.399268199834164 0.333333333333333
0.333333333333333 0.351296711045954
};
\addplot [draw=none, fill=silver202199187, forget plot]
table{%
x  y
0.333333333333333 0.351296711045954
0.399268199834164 0.333333333333333
1 -0.218190296371186
1.06438394000584 -0.333333333333333
1 -0.409911281813271
0.907325876548355 -0.426007456784978
0.333333333333333 -0.611396491220371
-0.157104640405243 -0.333333333333333
0.167871114910802 0.167871114910802
0.245607915584267 0.333333333333333

0.333333333333333 -1.16927749346174
0.884808062925314 -1.11519193707469
1 -1.1131954564453
1.08869106554495 -1.08869106554495
1.22173705199636 -1
1.66666666666667 -0.42348152084083
1.70029205524435 -0.366958721911018
1.70845791049426 -0.333333333333333
1.69544183757911 -0.304558162420885
1.66666666666667 -0.254337584278635
1.32449561343265 0.333333333333333
1 0.505353961974804
0.750224743996082 0.583108589337251
0.333333333333333 0.696687008045173
-0.0480812427607085 0.618585423905958
-0.333333333333333 0.626442909281786
-0.547690848065048 0.547690848065047
-0.738791963799092 0.333333333333333
-0.7587732689089 -0.0921066022422331
-0.795644842375952 -0.333333333333333
-0.333333333333333 -0.898072316081978
-0.145380724128233 -1
};
\addplot [draw=none, fill=tan187177144, forget plot]
table{%
x  y
-0.145380724128233 -1
-0.333333333333333 -0.898072316081978
-0.795644842375952 -0.333333333333333
-0.7587732689089 -0.0921066022422331
-0.738791963799092 0.333333333333333
-0.547690848065048 0.547690848065047
-0.333333333333333 0.626442909281786
-0.0480812427607084 0.618585423905958
0.333333333333333 0.696687008045173
0.750224743996082 0.583108589337251
1 0.505353961974804
1.32449561343265 0.333333333333333
1.66666666666667 -0.254337584278635
1.69544183757911 -0.304558162420885
1.70845791049426 -0.333333333333333
1.70029205524435 -0.366958721911018
1.66666666666667 -0.42348152084083
1.22173705199636 -1
1.08869106554495 -1.08869106554495
1 -1.1131954564453
0.884808062925314 -1.11519193707469
0.333333333333333 -1.16927749346174

0.240059021505137 -1.57339235483847
0.333333333333333 -1.60637497037805
0.400181082939876 -1.59981891706012
1 -1.58942299109984
1.46182554299426 -1.46182554299426
1.66666666666667 -1.32527443002579
2.04279617046063 -1
2.15116690167517 -0.817833568341835
2.26882676985741 -0.333333333333333
2.08128126769298 0.0812812676929764
1.93686155896136 0.333333333333333
1.79059729737652 0.457263964043187
1.66666666666667 0.501005360099536
1.3569019629225 0.643098037077497
1 0.832297821246682
0.461281520451922 1
0.360965124526025 1.02763179119269
0.333333333333333 1.03522017732103
-0.287043190246849 1.04629014308648
-0.333333333333333 1.05681106725162
-0.531308322300692 1
-0.874072910874658 0.874072910874658
-1 0.732820894848367
-1.3156624683615 0.333333333333333
-1.32968997976312 -0.00364335357021733
-1.3260112843089 -0.333333333333333
-1.23587129738103 -0.569204630714362
-1 -0.886088802862259
-0.911771275825894 -1
-0.616724520301798 -1.28339118696847
-0.333333333333333 -1.41412628176216
};
\addplot [draw=none, fill=rosybrown174149110, forget plot]
table{%
x  y
-0.333333333333333 -1.41412628176216
-0.616724520301798 -1.28339118696847
-0.911771275825894 -1
-1 -0.886088802862259
-1.23587129738103 -0.569204630714362
-1.3260112843089 -0.333333333333333
-1.32968997976312 -0.00364335357021739
-1.3156624683615 0.333333333333333
-1 0.732820894848367
-0.874072910874658 0.874072910874658
-0.531308322300692 1
-0.333333333333333 1.05681106725162
-0.287043190246849 1.04629014308648
0.333333333333333 1.03522017732103
0.360965124526025 1.02763179119269
0.461281520451922 1
1 0.832297821246682
1.3569019629225 0.643098037077497
1.66666666666667 0.501005360099536
1.79059729737652 0.457263964043187
1.93686155896136 0.333333333333333
2.08128126769298 0.0812812676929764
2.26882676985741 -0.333333333333333
2.15116690167517 -0.817833568341835
2.04279617046063 -1
1.66666666666667 -1.32527443002579
1.46182554299426 -1.46182554299426
1 -1.58942299109984
0.400181082939876 -1.59981891706012
0.333333333333333 -1.60637497037805
0.240059021505137 -1.57339235483847

2.75880393719645 0.0921372705297835
2.60580889722152 0.333333333333333
2.47084835297079 0.470848352970787
2.33333333333333 0.511644092460395
2.06897224345392 0.73563891012059
1.66666666666667 0.877632922973863
1.39990414118536 1
1.11838216297327 1.11838216297327
1 1.14090196121318
0.587779854143226 1.25444652080989
0.333333333333333 1.32432394697643
-0.00300773419639971 1.33032559913693
-0.333333333333333 1.4054027180784
-0.699742304624778 1.30025769537522
-1 1.30782264071896
-1.25828481278235 1.25828481278235
-1.51970038566465 1
-1.66666666666667 0.782435918575453
-1.85916213206403 0.525828798730701
-1.90975352109638 0.333333333333333
-1.90150151546255 0.0984984845374523
-1.89533894530008 -0.333333333333333
-1.66666666666667 -0.801189086022037
-1.61169674982749 -0.945030083160827
-1.57078014135854 -1
-1 -1.57047075629521
-0.950932867357416 -1.61759953402408
-0.844571274223885 -1.66666666666667
-0.333333333333333 -1.95339717672668
-0.0493089604644225 -2.04930896046442
0.333333333333333 -2.11857324719435
0.795948655891358 -2.12928198922469
1 -2.12521741963483
1.30494885722388 -2.02838447610946
1.66666666666667 -1.96790171517326
1.90088704403025 -1.90088704403025
2.19792663917209 -1.66666666666667
2.33333333333333 -1.5588669137967
2.80213596056312 -1
2.85243921705272 -0.852439217052719
2.92013170367181 -0.333333333333333
};
\addplot [draw=none, fill=indianred16512094, forget plot]
table{%
x  y
3 0.732367423567039
2.86743824782233 0.867438247822325
2.42059668588589 1
2.33333333333333 1.02404491142374
2.30763876360418 1.02569456972915
1.66666666666667 1.23308432701589
1.36143612413088 1.36143612413088
1 1.43019199399856
0.814594583760426 1.48126125042709
0.333333333333333 1.61342771663183
0.28102772185405 1.61436105518738
0.0508925928863343 1.66666666666667
-0.333333333333333 1.78845881389742
-0.497208645702679 1.83054197903601
-1 1.84654922127264
-1.64249780589116 1.69083552744217
-1.66666666666667 1.69451589936635
-1.69284490559702 1.69284490559702
-1.7240748786139 1.66666666666667
-2.33333333333333 1.18387238064036
-2.48801395057628 1
-2.66162215513959 0.661622155139589
-2.71765337588091 0.333333333333333
-2.70182789412858 0.0351612274619113
-2.68290574051516 -0.333333333333333
-2.61632222893749 -0.616322228937494
-2.40581448357539 -1
-2.33333333333333 -1.10082495408046
-2.1531309250703 -1.48646425840364
-1.9259883315226 -1.66666666666667
-1.66666666666667 -1.91093678147532
-1.47346697554492 -2.14013364221158
-1.01831504134129 -2.33333333333333
-1 -2.34552730023033
-0.603703307327851 -2.60370330732785
-0.333333333333333 -2.67077463045355
0.077759999309006 -2.74442666597567
0.333333333333333 -2.81668693407283
0.8411350590742 -2.8411350590742
1 -2.84694875301965
1.19311933540681 -2.80688066459319
1.66666666666667 -2.76926955659765
2.03734061897897 -2.70400728564564
2.33333333333333 -2.54417071759324
2.69194244369326 -2.33333333333333
2.90688633441036 -2.2402196677437
3 -2.12799135782494
3 -1.66666666666667
3 -1
3 -0.333333333333333
3 0.333333333333333

2.92013170367181 -0.333333333333333
2.85243921705272 -0.852439217052719
2.80213596056312 -1
2.33333333333333 -1.5588669137967
2.19792663917209 -1.66666666666667
1.90088704403025 -1.90088704403025
1.66666666666667 -1.96790171517326
1.30494885722388 -2.02838447610946
1 -2.12521741963483
0.795948655891358 -2.12928198922469
0.333333333333333 -2.11857324719435
-0.0493089604644225 -2.04930896046442
-0.333333333333333 -1.95339717672668
-0.844571274223885 -1.66666666666667
-0.950932867357416 -1.61759953402408
-1 -1.57047075629521
-1.57078014135854 -1
-1.61169674982749 -0.945030083160827
-1.66666666666667 -0.801189086022037
-1.89533894530008 -0.333333333333333
-1.90150151546255 0.0984984845374523
-1.90975352109638 0.333333333333333
-1.85916213206403 0.525828798730701
-1.66666666666667 0.782435918575453
-1.51970038566465 1
-1.25828481278235 1.25828481278235
-1 1.30782264071896
-0.699742304624778 1.30025769537522
-0.333333333333333 1.4054027180784
-0.00300773419639971 1.33032559913693
0.333333333333333 1.32432394697643
0.587779854143226 1.25444652080989
1 1.14090196121318
1.11838216297327 1.11838216297327
1.39990414118536 1
1.66666666666667 0.877632922973863
2.06897224345392 0.73563891012059
2.33333333333333 0.511644092460395
2.47084835297079 0.470848352970787
2.60580889722152 0.333333333333333
2.75880393719645 0.0921372705297835
};
\addplot [draw=none, fill=sienna1559181, forget plot]
table{%
x  y
3 1.49283121320794
2.56107293731372 1.43892706268628
2.33333333333333 1.50167939164462
1.79723503448227 1.53609829885106
1.66666666666667 1.57834435330489
1.6044900852885 1.6044900852885
1.27763999047793 1.66666666666667
1 1.75461616632251
0.859454303344676 1.80721236332199
0.333333333333333 2.02944015122116
-0.24737837337928 2.24737837337928
-0.333333333333333 2.27462442426641
-0.561950662389267 2.33333333333333
-0.833736880167515 2.49959645316582
-1 2.62160512838037
-1.51179054229897 2.8451238756323
-1.66666666666667 2.88525001544689
-2.14175593726264 3
-2.33333333333333 3
-3 3
-3 2.33333333333333
-3 1.66666666666667
-3 1
-3 0.333333333333333
-3 -0.333333333333333
-3 -1
-3 -1.66666666666667
-3 -2.33333333333333
-3 -3
-2.33333333333333 -3
-1.66666666666667 -3
-1 -3
-0.333333333333333 -3
0.333333333333333 -3
1 -3
1.66666666666667 -3
2.33333333333333 -3
3 -3
3 -2.33333333333333
3 -2.12799135782494
2.90688633441036 -2.2402196677437
2.69194244369326 -2.33333333333333
2.33333333333333 -2.54417071759324
2.03734061897897 -2.70400728564564
1.66666666666667 -2.76926955659765
1.19311933540681 -2.80688066459319
1 -2.84694875301965
0.8411350590742 -2.8411350590742
0.333333333333333 -2.81668693407283
0.077759999309006 -2.74442666597567
-0.333333333333333 -2.67077463045355
-0.603703307327851 -2.60370330732785
-1 -2.34552730023033
-1.01831504134129 -2.33333333333333
-1.47346697554492 -2.14013364221158
-1.66666666666667 -1.91093678147532
-1.9259883315226 -1.66666666666667
-2.15313092507031 -1.48646425840364
-2.33333333333333 -1.10082495408046
-2.40581448357539 -1
-2.61632222893749 -0.616322228937494
-2.68290574051516 -0.333333333333333
-2.70182789412858 0.0351612274619113
-2.71765337588091 0.333333333333333
-2.66162215513959 0.661622155139589
-2.48801395057628 1
-2.33333333333333 1.18387238064036
-1.7240748786139 1.66666666666667
-1.69284490559702 1.69284490559702
-1.66666666666667 1.69451589936635
-1.64249780589116 1.69083552744217
-1 1.84654922127264
-0.497208645702679 1.83054197903601
-0.333333333333333 1.78845881389742
0.0508925928863344 1.66666666666667
0.28102772185405 1.61436105518738
0.333333333333333 1.61342771663183
0.814594583760426 1.48126125042709
1 1.43019199399856
1.36143612413088 1.36143612413088
1.66666666666667 1.23308432701589
2.30763876360418 1.02569456972915
2.33333333333333 1.02404491142374
2.42059668588589 1
2.86743824782233 0.867438247822325
3 0.732367423567039
3 1
};
\addplot [draw=none, fill=brown1315655, forget plot]
table{%
x  y
2.89904626738403 3
2.74951326002755 2.58382007330578
2.64901655929164 2.33333333333333
2.33333333333333 2.19246921673204
2.20619671624256 2.20619671624256
1.66666666666667 2.10921753678532
1.1419502962439 2.19138303708943
1 2.23634938905177
0.740842935591537 2.33333333333333
0.586269753416008 2.58626975341601
0.333333333333333 2.76988780285031
0.226992102662133 2.8936587693288
0.0968784535384012 3
-0.333333333333333 3
-1 3
-1.66666666666667 3
-2.14175593726264 3
-1.66666666666667 2.88525001544689
-1.51179054229897 2.8451238756323
-1 2.62160512838037
-0.833736880167515 2.49959645316582
-0.561950662389267 2.33333333333333
-0.333333333333333 2.27462442426642
-0.24737837337928 2.24737837337928
0.333333333333333 2.02944015122116
0.859454303344676 1.80721236332199
1 1.75461616632251
1.27763999047793 1.66666666666667
1.6044900852885 1.6044900852885
1.66666666666667 1.57834435330489
1.79723503448227 1.53609829885106
2.33333333333333 1.50167939164462
2.56107293731372 1.43892706268628
3 1.49283121320794
3 1.66666666666667
3 2.33333333333333
3 3
};
\addplot [draw=none, fill=maroon941819, forget plot]
table{%
x  y
0.0968784535384012 3
0.226992102662133 2.8936587693288
0.333333333333333 2.76988780285031
0.586269753416008 2.58626975341601
0.740842935591537 2.33333333333333
1 2.23634938905177
1.1419502962439 2.19138303708943
1.66666666666667 2.10921753678532
2.20619671624256 2.20619671624256
2.33333333333333 2.19246921673204
2.64901655929164 2.33333333333333
2.74951326002755 2.58382007330578
2.89904626738403 3
2.33333333333333 3
1.66666666666667 3
1 3
0.333333333333333 3
};
\end{axis}

\end{tikzpicture}

\caption{insert caption}
\label{fig-}
\end{figure*}
#+end_export
*** Tikz fig :noexport:
#+begin_export latex
\begin{figure*}
\include{fig}
\caption{insert caption}
\label{fig-}
\end{figure*}
#+end_export
*** Tikz fig :noexport:
#+begin_export latex
\begin{figure}
% This file was created with tikzplotlib v0.10.1.
\begin{tikzpicture}

\definecolor{brown1315655}{RGB}{131,56,55}
\definecolor{darkgray176}{RGB}{176,176,176}
\definecolor{darkkhaki179162122}{RGB}{179,162,122}
\definecolor{indianred16210990}{RGB}{162,109,90}
\definecolor{indianred16512195}{RGB}{165,121,95}
\definecolor{lightgray204}{RGB}{204,204,204}
\definecolor{lightgray207205196}{RGB}{207,205,196}
\definecolor{linen236236235}{RGB}{236,236,235}
\definecolor{maroon931718}{RGB}{93,17,18}
\definecolor{maroon941819}{RGB}{94,18,19}
\definecolor{rosybrown169133100}{RGB}{169,133,100}
\definecolor{rosybrown174149110}{RGB}{174,149,110}
\definecolor{saddlebrown1254949}{RGB}{125,49,49}
\definecolor{sienna1518276}{RGB}{151,82,76}
\definecolor{sienna1559281}{RGB}{155,92,81}
\definecolor{silver203200189}{RGB}{203,200,189}
\definecolor{tan187178146}{RGB}{187,178,146}
\definecolor{tan190184156}{RGB}{190,184,156}
\definecolor{whitesmoke237237236}{RGB}{237,237,236}

\begin{groupplot}[group style={group size=2 by 1}]
\nextgroupplot[
tick align=outside,
tick pos=left,
x grid style={darkgray176},
xlabel={\(\displaystyle x\)},
xmin=-3, xmax=3,
xtick style={color=black},
y grid style={darkgray176},
ylabel={\(\displaystyle y\)},
ymin=-3, ymax=3,
ytick style={color=black}
]
\addplot [draw=none, fill=whitesmoke237237236]
table{%
x  y
0.333333333333333 -0.368038706048651
0.440847180080572 -0.333333333333333
0.333333333333333 -0.238853314235074
0.272127446203486 -0.333333333333333
};
\addplot [draw=none, fill=lightgray207205196]
table{%
x  y
0.272127446203486 -0.333333333333333
0.333333333333333 -0.238853314235074
0.440847180080572 -0.333333333333333
0.333333333333333 -0.368038706048651

-0.333333333333333 0.409390158910839
-0.388952669259613 0.388952669259612
-0.438531761539921 0.333333333333333
-0.443708637055047 0.222958029611619
-0.528661970961193 -0.333333333333333
-0.333333333333333 -0.571932098661895
0.187997465164982 -0.854664131831649
0.333333333333333 -0.937073450774879
0.426254871103182 -0.907078462230151
1 -0.807508006514957
1.39875309869465 -0.333333333333333
1.05920338843175 0.274129944901582
1.02474039380187 0.333333333333333
1 0.346448839839771
0.980956825940404 0.352376507392929
0.333333333333333 0.528772166097216
-0.259310236102152 0.407356430564514
};
\addplot [draw=none, fill=tan190184156]
table{%
x  y
-0.259310236102152 0.407356430564514
0.333333333333333 0.528772166097216
0.980956825940404 0.352376507392929
1 0.346448839839771
1.02474039380187 0.333333333333333
1.05920338843175 0.274129944901582
1.39875309869465 -0.333333333333333
1 -0.807508006514957
0.426254871103182 -0.907078462230151
0.333333333333333 -0.937073450774879
0.187997465164982 -0.854664131831649
-0.333333333333333 -0.571932098661895
-0.528661970961193 -0.333333333333333
-0.443708637055047 0.222958029611619
-0.438531761539921 0.333333333333333
-0.388952669259613 0.388952669259612
-0.333333333333333 0.409390158910839

-0.158373704928818 -1.17495962840452
0.333333333333333 -1.34884535220588
0.685691573183665 -1.31430842681633
1 -1.30886787812456
1.24200836881553 -1.24200836881553
1.6050933587395 -1
1.66666666666667 -0.920241166986417
1.8856142495475 -0.552280916214171
1.93879250520407 -0.333333333333333
1.85404185731079 -0.145958142689208
1.66666666666667 0.181151652027312
1.57807990491972 0.333333333333333
1 0.639788065417753
0.555040378989119 0.778292954344215
0.333333333333333 0.838680140424201
0.130448082247853 0.79711474891452
-0.333333333333333 0.809856790880426
-0.681808543294009 0.681808543294009
-0.992439447174989 0.333333333333333
-1 0.172136041274382
-1.02017569586964 -0.313157637463689
-1.01995028790239 -0.333333333333333
-1.01443402837836 -0.347767361711695
-1 -0.367158628674457
-0.509826944344476 -1
-0.419801488057808 -1.08646815472447
-0.333333333333333 -1.12635930195796
};
\addplot [draw=none, fill=darkkhaki179162122]
table{%
x  y
-0.333333333333333 -1.12635930195796
-0.419801488057808 -1.08646815472447
-0.509826944344476 -1
-1 -0.367158628674457
-1.01443402837836 -0.347767361711695
-1.01995028790239 -0.333333333333333
-1.02017569586964 -0.313157637463689
-1 0.172136041274382
-0.992439447174989 0.333333333333333
-0.681808543294009 0.681808543294009
-0.333333333333333 0.809856790880426
0.130448082247853 0.79711474891452
0.333333333333333 0.838680140424201
0.555040378989119 0.778292954344215
1 0.639788065417753
1.57807990491972 0.333333333333333
1.66666666666667 0.181151652027312
1.85404185731079 -0.145958142689208
1.93879250520407 -0.333333333333333
1.8856142495475 -0.552280916214171
1.66666666666667 -0.920241166986417
1.6050933587395 -1
1.24200836881553 -1.24200836881553
1 -1.30886787812456
0.685691573183665 -1.31430842681633
0.333333333333333 -1.34884535220588
-0.158373704928818 -1.17495962840452

2.42621147383435 -0.240455192832315
2.33333333333333 -0.0940163247387412
2.20023990643481 0.200239906434805
2.12400126533815 0.333333333333333
1.87643930059474 0.543105967261411
1.66666666666667 0.617147520252526
1.14232027488244 0.857679725117562
1 0.933127290995734
0.785165307267185 1
0.430903844190603 1.09757051085727
0.333333333333333 1.12436458767343
-0.199487058871557 1.13384627446178
-0.333333333333333 1.16426632536619
-0.905715230928947 1
-0.974664417328406 0.974664417328406
-1 0.946242188951725
-1.48428023701721 0.333333333333333
-1.49156015014393 0.158226816810592
-1.48606831606074 -0.333333333333333
-1.35167030682316 -0.685003640156498
-1.11720051143575 -1
-1 -1.11713685757981
-0.719697095420377 -1.38636376208704
-0.333333333333333 -1.56460850164703
0.0340728125310822 -1.66666666666667
0.257785396056402 -1.7422146039436
0.333333333333333 -1.75589057891316
0.4246721927192 -1.75800552605253
1 -1.74655997988459
1.25161883765478 -1.66666666666667
1.57682276850538 -1.57682276850538
1.66666666666667 -1.51693878584173
2.26444832195249 -1
2.29014606506265 -0.956812731729314
2.33333333333333 -0.779000430773824
2.44664935714459 -0.446649357144589
2.46142713608542 -0.333333333333333
};
\addplot [draw=none, fill=rosybrown169133100]
table{%
x  y
3 -0.18455174060839
2.85762257457452 0.19095590790785
2.76732038435005 0.333333333333333
2.55236564674895 0.552365646748953
2.33333333333333 0.617345494506401
2.12619680468867 0.792863471355334
1.66666666666667 0.955059676018694
1.56869934150772 1
1.16835558269872 1.16835558269872
1 1.20037990446773
0.634405176881039 1.30107184354771
0.333333333333333 1.38374991945735
0.0553631428276088 1.38869647616094
-0.333333333333333 1.47703787108379
-0.599287629714551 1.40071237028545
-1 1.41080596143929
-1.34469419263328 1.34469419263328
-1.66666666666667 1.02657260103729
-1.69653337963704 1
-1.96947314734425 0.636139814010921
-2.0490566900603 0.333333333333333
-2.03607414598085 -0.0360741459808461
-2.03182674490221 -0.333333333333333
-1.74356653026749 -0.92310013639918
-1.71614072620156 -1
-1.7003845550982 -1.03371788843153
-1.66666666666667 -1.06046831229158
-1.05995441875879 -1.66666666666667
-1.03253158477074 -1.69919825143741
-1 -1.71300738966959
-0.920737264133327 -1.74592940253334
-0.333333333333333 -2.07537828816142
-0.140498710730602 -2.1404987107306
0.333333333333333 -2.22627358967458
0.906205075504508 -2.23953840883784
1 -2.23767245672316
1.14016599256647 -2.19316734076686
1.66666666666667 -2.10513525661994
2.0075912045791 -2.0075912045791
2.33333333333333 -1.75074805524753
2.3920002979587 -1.72533363129203
2.44067587865624 -1.66666666666667
2.77525443123656 -1.22474556876344
2.9637844803103 -1
2.97299269584845 -0.972992695848454
3 -0.765900660396603
3 -0.333333333333333

2.46142713608542 -0.333333333333333
2.44664935714459 -0.446649357144589
2.33333333333333 -0.779000430773824
2.29014606506265 -0.956812731729314
2.26444832195249 -1
1.66666666666667 -1.51693878584173
1.57682276850538 -1.57682276850538
1.25161883765478 -1.66666666666667
1 -1.74655997988459
0.4246721927192 -1.75800552605253
0.333333333333333 -1.75589057891316
0.257785396056402 -1.7422146039436
0.0340728125310822 -1.66666666666667
-0.333333333333333 -1.56460850164703
-0.719697095420377 -1.38636376208704
-1 -1.11713685757981
-1.11720051143575 -1
-1.35167030682316 -0.685003640156498
-1.48606831606074 -0.333333333333333
-1.49156015014393 0.158226816810592
-1.48428023701721 0.333333333333333
-1 0.946242188951725
-0.974664417328406 0.974664417328406
-0.905715230928947 1
-0.333333333333333 1.16426632536619
-0.199487058871557 1.13384627446178
0.333333333333333 1.12436458767343
0.430903844190603 1.09757051085727
0.785165307267185 1
1 0.933127290995734
1.14232027488244 0.857679725117562
1.66666666666667 0.617147520252526
1.87643930059474 0.543105967261411
2.12400126533815 0.333333333333333
2.20023990643481 0.200239906434805
2.33333333333333 -0.0940163247387412
2.42621147383435 -0.240455192832315
};
\addplot [draw=none, fill=indianred16210990]
table{%
x  y
3 0.814645473940835
2.90819349697443 0.908193496974432
2.59873461951833 1
2.33333333333333 1.07313189129808
2.255184171458 1.07814916187534
1.66666666666667 1.26857487222597
1.38642444779073 1.38642444779073
1 1.45992947006023
0.837906509571476 1.50457317623814
0.333333333333333 1.64313525124127
0.310213344526775 1.64354667786011
0.208486875459382 1.66666666666667
-0.333333333333333 1.83841064879258
-0.564419958718273 1.89775329205161
-1 1.91161740893376
-1.55661582893558 1.77671750439776
-1.66666666666667 1.7934759569967
-1.78586727929585 1.78586727929585
-1.92807149797511 1.66666666666667
-2.33333333333333 1.3455238103493
-2.62400102975534 1
-2.75149567446022 0.751495674460219
-2.82286665031643 0.333333333333333
-2.81293717908284 0.146270512416168
-2.78830857729871 -0.333333333333333
-2.70164759546276 -0.70164759546276
-2.53795474447778 -1
-2.33333333333333 -1.28463898722258
-2.21166768591876 -1.5450010192521
-2.05831278033158 -1.66666666666667
-1.66666666666667 -2.0355805632361
-1.53047612470331 -2.19714279136998
-1.20963827904178 -2.33333333333333
-1 -2.47290810182295
-0.68083409567217 -2.68083409567217
-0.333333333333333 -2.76704110435922
0.195040872221037 -2.8617075388877
0.333333333333333 -2.90080853958865
0.929513244949565 -2.92951324494956
1 -2.9320933670446
1.08568327463099 -2.91431672536901
1.66666666666667 -2.86817366838688
2.12143816743891 -2.78810483410558
2.33333333333333 -2.67368368218011
2.91223148388086 -2.33333333333333
2.97347110560083 -2.30680443893417
3 -2.2748300960098
3 -1.66666666666667
3 -1
3 -0.765900660396603
2.97299269584845 -0.972992695848454
2.9637844803103 -1
2.77525443123656 -1.22474556876344
2.44067587865624 -1.66666666666667
2.3920002979587 -1.72533363129203
2.33333333333333 -1.75074805524753
2.0075912045791 -2.0075912045791
1.66666666666667 -2.10513525661994
1.14016599256647 -2.19316734076686
1 -2.23767245672316
0.906205075504508 -2.23953840883784
0.333333333333333 -2.22627358967458
-0.140498710730602 -2.1404987107306
-0.333333333333333 -2.07537828816142
-0.920737264133327 -1.74592940253334
-1 -1.71300738966959
-1.03253158477074 -1.69919825143741
-1.05995441875879 -1.66666666666667
-1.66666666666667 -1.06046831229158
-1.7003845550982 -1.03371788843153
-1.71614072620156 -1
-1.74356653026749 -0.92310013639918
-2.03182674490221 -0.333333333333333
-2.03607414598085 -0.036074145980846
-2.0490566900603 0.333333333333333
-1.96947314734425 0.636139814010921
-1.69653337963704 1
-1.66666666666667 1.02657260103729
-1.34469419263328 1.34469419263328
-1 1.41080596143929
-0.599287629714551 1.40071237028545
-0.333333333333333 1.47703787108379
0.0553631428276087 1.38869647616094
0.333333333333333 1.38374991945735
0.634405176881039 1.30107184354771
1 1.20037990446773
1.16835558269872 1.16835558269872
1.56869934150772 1
1.66666666666667 0.955059676018694
2.12619680468867 0.792863471355334
2.33333333333333 0.617345494506401
2.55236564674895 0.552365646748953
2.76732038435005 0.333333333333333
2.85762257457452 0.19095590790785
3 -0.18455174060839
3 0.333333333333333
};
\addplot [draw=none, fill=sienna1518276]
table{%
x  y
3 1.49283247897013
2.56107134378741 1.43892865621259
2.33333333333333 1.5016823514353
1.7972326719609 1.53610066137243
1.66666666666667 1.57834770432932
1.60449331288274 1.60449331288274
1.27764076374447 1.66666666666667
1 1.75461859787807
0.859451419676838 1.80721524698983
0.333333333333333 2.02944433510387
-0.247379745487206 2.2473797454872
-0.333333333333333 2.27462496671584
-0.561950017364068 2.33333333333333
-0.833736593557157 2.49959673977618
-1 2.62160473410902
-1.51178993508344 2.84512326841678
-1.66666666666667 2.88524978672734
-2.14175573421001 3
-2.33333333333333 3
-3 3
-3 2.33333333333333
-3 1.66666666666667
-3 1
-3 0.333333333333333
-3 -0.333333333333333
-3 -1
-3 -1.66666666666667
-3 -2.33333333333333
-3 -3
-2.33333333333333 -3
-1.66666666666667 -3
-1 -3
-0.333333333333333 -3
0.333333333333333 -3
1 -3
1.66666666666667 -3
2.33333333333333 -3
3 -3
3 -2.33333333333333
3 -2.2748300960098
2.97347110560084 -2.30680443893417
2.91223148388086 -2.33333333333333
2.33333333333333 -2.67368368218011
2.12143816743891 -2.78810483410558
1.66666666666667 -2.86817366838688
1.08568327463099 -2.91431672536901
1 -2.9320933670446
0.929513244949565 -2.92951324494956
0.333333333333333 -2.90080853958865
0.195040872221037 -2.8617075388877
-0.333333333333333 -2.76704110435922
-0.68083409567217 -2.68083409567217
-1 -2.47290810182295
-1.20963827904178 -2.33333333333333
-1.53047612470331 -2.19714279136998
-1.66666666666667 -2.0355805632361
-2.05831278033158 -1.66666666666667
-2.21166768591876 -1.5450010192521
-2.33333333333333 -1.28463898722258
-2.53795474447778 -1
-2.70164759546276 -0.70164759546276
-2.78830857729871 -0.333333333333333
-2.81293717908284 0.146270512416168
-2.82286665031643 0.333333333333333
-2.75149567446022 0.751495674460219
-2.62400102975534 1
-2.33333333333333 1.3455238103493
-1.92807149797511 1.66666666666667
-1.78586727929585 1.78586727929585
-1.66666666666667 1.7934759569967
-1.55661582893558 1.77671750439776
-1 1.91161740893376
-0.564419958718273 1.89775329205161
-0.333333333333333 1.83841064879258
0.208486875459382 1.66666666666667
0.310213344526775 1.64354667786011
0.333333333333333 1.64313525124127
0.837906509571476 1.50457317623814
1 1.45992947006023
1.38642444779073 1.38642444779073
1.66666666666667 1.26857487222597
2.255184171458 1.07814916187534
2.33333333333333 1.07313189129808
2.59873461951833 1
2.90819349697443 0.908193496974432
3 0.814645473940835
3 1
};
\addplot [draw=none, fill=saddlebrown1254949]
table{%
x  y
3 2.78159864998554
2.88150961264722 2.45182372068612
2.83396890997888 2.33333333333333
2.33333333333333 2.10994247997489
2.13171268938288 2.13171268938288
1.66666666666667 2.04812593733216
1.21438243012329 2.11895090321005
1 2.18686365670071
0.608604246491689 2.33333333333333
0.504188776152056 2.50418877615206
0.333333333333333 2.62821794149216
0.161521769926228 2.8281884365929
-0.0487013187421088 3
-0.333333333333333 3
-1 3
-1.66666666666667 3
-2.14175573421001 3
-1.66666666666667 2.88524978672734
-1.51178993508344 2.84512326841678
-1 2.62160473410902
-0.833736593557157 2.49959673977618
-0.561950017364068 2.33333333333333
-0.333333333333333 2.27462496671584
-0.247379745487206 2.2473797454872
0.333333333333333 2.02944433510387
0.859451419676838 1.80721524698983
1 1.75461859787807
1.27764076374447 1.66666666666667
1.60449331288274 1.60449331288274
1.66666666666667 1.57834770432932
1.7972326719609 1.53610066137243
2.33333333333333 1.5016823514353
2.56107134378741 1.43892865621259
3 1.49283247897013
3 1.66666666666667
3 2.33333333333333
};
\addplot [draw=none, fill=maroon931718]
table{%
x  y
-0.0487013187421088 3
0.161521769926228 2.8281884365929
0.333333333333333 2.62821794149215
0.504188776152056 2.50418877615206
0.608604246491689 2.33333333333333
1 2.18686365670071
1.21438243012329 2.11895090321005
1.66666666666667 2.04812593733216
2.13171268938288 2.13171268938288
2.33333333333333 2.10994247997489
2.83396890997888 2.33333333333333
2.88150961264722 2.45182372068612
3 2.78159864998554
3 3
2.33333333333333 3
1.66666666666667 3
1 3
0.333333333333333 3
};

\nextgroupplot[
legend style={fill opacity=0.8, draw opacity=1, text opacity=1, draw=lightgray204},
tick align=outside,
tick pos=left,
x grid style={darkgray176},
xlabel={\(\displaystyle x\)},
xmin=-3, xmax=3,
xtick style={color=black},
y grid style={darkgray176},
ylabel={\(\displaystyle y\)},
ymin=-3, ymax=3,
ytick style={color=black}
]
\addplot [draw=none, fill=linen236236235, forget plot]
table{%
x  y
-0.0630724848915591 -1.27026084844177
0.333333333333333 -1.07567144466552
0.384491261153592 -1
0.355263439790612 -0.978069893542721
0.333333333333333 -0.939217513597454
-0.173618898758508 -0.333333333333333
-0.287044817995094 -0.287044817995094
-0.333333333333333 -0.236109647072395
-0.965577278375191 -0.298910611708524
-1 -0.271618020736679
-1.04430074984461 -0.289032583488723
-1.06278009091027 -0.333333333333333
-1.05051215403613 -0.383845487369459
-1 -0.633077971296663
-0.87523920669654 -1
-0.53977484794468 -1.20644151461135
-0.333333333333333 -1.28987831703544
};
\addplot [draw=none, fill=silver203200189, forget plot]
table{%
x  y
-0.333333333333333 -1.28987831703544
-0.53977484794468 -1.20644151461135
-0.87523920669654 -1
-1 -0.633077971296663
-1.05051215403613 -0.383845487369459
-1.06278009091027 -0.333333333333333
-1.04430074984461 -0.289032583488723
-1 -0.271618020736679
-0.965577278375191 -0.298910611708524
-0.333333333333333 -0.236109647072395
-0.287044817995094 -0.287044817995094
-0.173618898758508 -0.333333333333333
0.333333333333333 -0.939217513597454
0.355263439790612 -0.978069893542721
0.384491261153592 -1
0.333333333333333 -1.07567144466552
-0.0630724848915591 -1.27026084844177

0.568073165300202 -1.66666666666667
0.697137441656976 -1.30286255834302
0.901888665419881 -1
0.577058587478089 -0.756274745855245
0.338330606982072 -0.333333333333334
0.333333333333333 -0.330310262039653
-0.137972751498896 -0.137972751498896
-0.333333333333333 0.0769990304773301
-0.617937440265623 0.0487292264010434
-0.976893757616138 0.333333333333333
-1 0.349094151229702
-1.04660445023867 0.333333333333333
-1.49170016445143 0.158366831118092
-1.66666666666667 -0.261082499801506
-1.69801592783714 -0.333333333333334
-1.66666666666667 -0.457425001248814
-1.56064140073195 -0.893974734065286
-1.53915317223049 -1
-1 -1.55664192645978
-0.931889686503533 -1.5985563531702
-0.763369370883144 -1.66666666666667
-0.333333333333333 -1.83784661198519
0.20030323772973 -1.79969676227027
0.333333333333333 -1.79889991027601
0.440904146139909 -1.77423747947324
};
\addplot [draw=none, fill=tan187178146, forget plot]
table{%
x  y
0.7818870050558 3
0.333333333333333 2.37363022399301
0.31726700988068 2.33333333333333
0.333333333333333 2.3289035646469
0.337643931076443 2.32902273559022
1 2.20732389812948
1.14822027697615 2.18511305635718
1.66666666666667 2.23916756292435
1.91261552640079 2.33333333333333
1.86949052892589 2.53615719559256
1.66666666666667 2.83865231283657
1.3564299710663 3
1 3

0.440904146139909 -1.77423747947324
0.333333333333333 -1.79889991027601
0.20030323772973 -1.79969676227027
-0.333333333333333 -1.83784661198519
-0.763369370883144 -1.66666666666667
-0.931889686503533 -1.5985563531702
-1 -1.55664192645978
-1.53915317223049 -1
-1.56064140073195 -0.893974734065286
-1.66666666666667 -0.457425001248814
-1.69801592783714 -0.333333333333333
-1.66666666666667 -0.261082499801506
-1.49170016445143 0.158366831118092
-1.04660445023867 0.333333333333333
-1 0.349094151229702
-0.976893757616138 0.333333333333333
-0.617937440265623 0.0487292264010434
-0.333333333333333 0.07699903047733
-0.137972751498896 -0.137972751498896
0.333333333333333 -0.330310262039653
0.338330606982072 -0.333333333333333
0.577058587478089 -0.756274745855245
0.901888665419881 -1
0.697137441656976 -1.30286255834302
0.568073165300202 -1.66666666666667

1 -2.17303667569649
1.49670763463347 -1.66666666666667
1.53535966584848 -1.53535966584848
1.44714369162161 -1
1.19751296606599 -0.80248703393401
1 -0.685402889594183
0.798853735165155 -0.534479598168179
0.685317344150591 -0.333333333333333
0.333333333333333 -0.120402676793323
0.0110993149970272 0.0110993149970272
-0.281738306435499 0.333333333333333
-0.300499405392186 0.36616726127448
-0.333333333333333 0.409133792404326
-0.639175114280038 0.639175114280037
-1 0.885294549099179
-1.17331890158994 0.826681098410059
-1.66666666666667 0.895412504974207
-2.04668663371916 0.713353300385824
-2.22458033422243 0.333333333333333
-2.33333333333333 -0.175987633296314
-2.36866501955439 -0.298001647112274
-2.37137705541412 -0.333333333333334
-2.36733152103271 -0.367331521032711
-2.33333333333333 -0.486253919714299
-2.24361015223387 -1
-2.08648630193627 -1.4198196352696
-1.80499731415514 -1.66666666666667
-1.66666666666667 -1.79880553553134
-1.41744441173102 -2.08411107839769
-1 -2.22800407149996
-0.57168417376075 -2.33333333333333
-0.403293824085566 -2.40329382408557
-0.333333333333333 -2.41227431973896
-0.254032401862712 -2.41263426480396
0.333333333333333 -2.40701067282019
0.398654146246429 -2.39865414624643
0.534388422531692 -2.33333333333333
0.913160163081194 -2.24649349641453
};
\addplot [draw=none, fill=rosybrown174149110, forget plot]
table{%
x  y
1.3564299710663 3
1.66666666666667 2.83865231283657
1.86949052892589 2.53615719559256
1.91261552640079 2.33333333333333
1.66666666666667 2.23916756292435
1.14822027697615 2.18511305635718
1 2.20732389812948
0.337643931076443 2.32902273559022
0.333333333333333 2.3289035646469
0.31726700988068 2.33333333333333
0.333333333333333 2.37363022399301
0.7818870050558 3
0.333333333333333 3
-0.333333333333333 3
-1 3
-1.66666666666667 3
-1.98957130524925 3
-2.20207363052919 2.86874029719586
-2.33333333333333 2.70828741165832
-2.74049897408993 2.33333333333333
-2.91118199548157 2.2445153288149
-3 2.08794939455656
-3 1.66666666666667
-3 1
-3 0.333333333333333
-3 -0.333333333333333
-3 -1
-3 -1.66666666666667
-3 -2.33333333333333
-3 -2.52421175135401
-2.44572771025006 -3
-2.33333333333333 -3
-1.66666666666667 -3
-1 -3
-0.333333333333333 -3
0.333333333333333 -3
1 -3
1.66666666666667 -3
2.33333333333333 -3
2.38354021043149 -3
2.47666770640128 -2.85666562693205
2.91407272583199 -2.33333333333333
2.94494827948616 -2.27828161281949
2.96059664604585 -1.66666666666667
2.7169693310977 -1.2830306689023
2.33333333333333 -1.03013652569965
2.32352698884838 -1.00980634448495
2.32034785430537 -1
1.66666666666667 -0.737116467259514
1.44124320978289 -0.558756790217107
1.06096921247088 -0.333333333333333
1 -0.31397608127392
0.950533165632076 -0.28386649896541
0.333333333333333 0.0895049084533956
0.160171381493226 0.160171381493226
0.00280645037491101 0.333333333333333
-0.119421448796791 0.547245217869875
-0.333333333333333 0.827170655437313
-0.563111331955089 1
-0.333655058808008 1.66634494119199
-0.333997430006581 1.66666666666667
-0.334067544515263 1.6674008778486
-0.333333333333333 1.66708960255648
0.0348971259689736 1.96510287403103
0.333333333333333 1.88281874909255
0.771728041394448 1.89493862527222
1 1.8529967856929
1.5650022636627 1.76833106967064
1.66666666666667 1.77893085250085
1.84857921448515 1.84857921448515
2.33333333333333 1.8015479145634
2.66245788454069 1.99579121787403
3 2.03662708640596
3 2.33333333333333
3 3
2.33333333333333 3
1.66666666666667 3

0.913160163081194 -2.24649349641453
0.534388422531692 -2.33333333333333
0.398654146246429 -2.39865414624643
0.333333333333333 -2.40701067282019
-0.254032401862712 -2.41263426480396
-0.333333333333333 -2.41227431973896
-0.403293824085566 -2.40329382408557
-0.57168417376075 -2.33333333333333
-1 -2.22800407149996
-1.41744441173102 -2.08411107839769
-1.66666666666667 -1.79880553553134
-1.80499731415514 -1.66666666666667
-2.08648630193627 -1.4198196352696
-2.24361015223387 -1
-2.33333333333333 -0.486253919714299
-2.36733152103271 -0.367331521032711
-2.37137705541412 -0.333333333333333
-2.36866501955439 -0.298001647112274
-2.33333333333333 -0.175987633296314
-2.22458033422243 0.333333333333333
-2.04668663371916 0.713353300385824
-1.66666666666667 0.895412504974207
-1.17331890158994 0.826681098410059
-1 0.885294549099179
-0.639175114280038 0.639175114280037
-0.333333333333333 0.409133792404326
-0.300499405392186 0.36616726127448
-0.281738306435499 0.333333333333333
0.0110993149970272 0.0110993149970272
0.333333333333333 -0.120402676793323
0.685317344150591 -0.333333333333333
0.798853735165155 -0.534479598168179
1 -0.685402889594183
1.19751296606599 -0.80248703393401
1.44714369162161 -1
1.53535966584848 -1.53535966584848
1.49670763463347 -1.66666666666667
1 -2.17303667569649
};
\addplot [draw=none, fill=indianred16512195, forget plot]
table{%
x  y
3 2.03662708640596
2.66245788454069 1.99579121787403
2.33333333333333 1.8015479145634
1.84857921448515 1.84857921448515
1.66666666666667 1.77893085250085
1.5650022636627 1.76833106967064
1 1.8529967856929
0.771728041394448 1.89493862527222
0.333333333333333 1.88281874909255
0.0348971259689736 1.96510287403103
-0.333333333333333 1.66708960255648
-0.334067544515263 1.6674008778486
-0.333997430006581 1.66666666666667
-0.333655058808008 1.66634494119199
-0.563111331955089 1
-0.333333333333333 0.827170655437313
-0.119421448796791 0.547245217869875
0.00280645037491103 0.333333333333333
0.160171381493226 0.160171381493226
0.333333333333333 0.0895049084533956
0.950533165632076 -0.28386649896541
1 -0.31397608127392
1.06096921247088 -0.333333333333333
1.44124320978289 -0.558756790217107
1.66666666666667 -0.737116467259514
2.32034785430537 -1
2.32352698884838 -1.00980634448495
2.33333333333333 -1.03013652569965
2.7169693310977 -1.2830306689023
2.96059664604585 -1.66666666666667
2.94494827948616 -2.27828161281949
2.91407272583199 -2.33333333333333
2.47666770640128 -2.85666562693205
2.38354021043149 -3
3 -3
3 -2.33333333333333
3 -1.66666666666667
3 -1
3 -0.333333333333333
3 0.333333333333333
3 1
3 1.66666666666667

-3 2.08794939455656
-2.91118199548157 2.2445153288149
-2.74049897408993 2.33333333333333
-2.33333333333333 2.70828741165832
-2.20207363052919 2.86874029719586
-1.98957130524925 3
-2.33333333333333 3
-3 3
-3 2.33333333333333

-2.44572771025006 -3
-3 -2.52421175135401
-3 -3

2.33333333333333 -0.275928578974349
1.71904305877713 -0.280956941222871
1.66666666666667 -0.309445698193303
1.64378465008293 -0.310451316749599
1 -0.10605468378358
0.419196729053516 0.24746993761315
0.333333333333333 0.299412493699726
0.309243447989149 0.309243447989149
0.287351207184795 0.333333333333333
0.0616565077982685 0.728323174464935
-0.145951997667086 1
0.333333333333333 1.49619601412554
0.839912494940085 1.50657916160675
1 1.56467608196889
1.51758907649558 1.51758907649558
1.66666666666667 1.47154158667019
2.04592461721031 1.28740871612303
2.33333333333333 1.27127169550956
2.82282460796764 1
2.87296010860285 0.872960108602854
2.96835920897643 0.333333333333333
};
\addplot [draw=none, fill=sienna1559281, forget plot]
table{%
x  y
2.96835920897643 0.333333333333333
2.87296010860285 0.872960108602854
2.82282460796764 1
2.33333333333333 1.27127169550956
2.04592461721031 1.28740871612303
1.66666666666667 1.47154158667019
1.51758907649558 1.51758907649558
1 1.56467608196889
0.839912494940085 1.50657916160675
0.333333333333333 1.49619601412554
-0.145951997667086 1
0.0616565077982685 0.728323174464935
0.287351207184795 0.333333333333333
0.309243447989149 0.309243447989149
0.333333333333333 0.299412493699726
0.419196729053516 0.24746993761315
1 -0.10605468378358
1.64378465008293 -0.310451316749599
1.66666666666667 -0.309445698193303
1.71904305877713 -0.280956941222871
2.33333333333333 -0.275928578974349

0.173501117970493 1
0.333333333333333 1.16547159502327
0.502267503480482 1.16893417014715
1 1.34956491457107
1.32041552154508 1.32041552154508
1.66666666666667 1.21346453002641
2.10633895294017 1
2.1367450830721 0.803411749738767
2.26371243752013 0.333333333333333
1.66666666666667 0.00858603439383443
1.33914137599769 -0.00580804266435342
1 0.101866713707145
0.619725015862617 0.333333333333333
0.333333333333333 0.750843429233686
0.242734464393663 0.90940113106033
};
\addplot [draw=none, fill=brown1315655, forget plot]
table{%
x  y
0.242734464393663 0.90940113106033
0.333333333333333 0.750843429233686
0.619725015862617 0.333333333333333
1 0.101866713707145
1.33914137599769 -0.00580804266435342
1.66666666666667 0.00858603439383443
2.26371243752013 0.333333333333333
2.1367450830721 0.803411749738767
2.10633895294017 1
1.66666666666667 1.21346453002641
1.32041552154508 1.32041552154508
1 1.34956491457107
0.502267503480482 1.16893417014715
0.333333333333333 1.16547159502327
0.173501117970493 1

0.87682701219519 0.456506321138143
0.629509364750844 1
1 1.13445374717365
1.12324196659496 1.12324196659496
1.52223473379233 1
1.66666666666667 0.405904041959122
1.67638758753889 0.343054254205559
1.67901319118899 0.333333333333333
1.66666666666667 0.326617766980384
1.034498101913 0.298835231420329
1 0.309788111197485
0.96131770970397 0.333333333333333
};
\addplot [draw=none, fill=maroon941819, forget plot]
table{%
x  y
0.96131770970397 0.333333333333333
1 0.309788111197485
1.034498101913 0.298835231420329
1.66666666666667 0.326617766980384
1.67901319118899 0.333333333333333
1.67638758753889 0.343054254205559
1.66666666666667 0.405904041959122
1.52223473379233 1
1.12324196659496 1.12324196659496
1 1.13445374717365
0.629509364750844 1
0.87682701219519 0.456506321138143
};
\end{groupplot}

\end{tikzpicture}
\end{figure}
#+end_export
** Planning label:sec-planning
We now detail our planning algorithm which leverages the latent structure of our dynamic model to:
1) Enforce the mode constraint with high probability,
2) Escape local optima induced by the mode constraint, by targeting exploration where the agent has high /epistemic uncertainty/ in its belief of the mode constraint.
# 2) target exploration where the agent is not confident in its belief of the mode constraint, i.e. where the gating network's /epistemic uncertainty/ is high.

*Multi-step predictions with dynamic model*
Let us first observe that if the controlled system satisfies the mode constraint, the system will be fully
governed by the desired dynamic mode $\dynamicsFunc_{\desiredMode}$.
We leverage this observation and simplify making multi-step predictions by using only the desired
dynamic mode $\dynamicsFunc_{\desiredMode}$.
This enables us to approximate the GP dynamics integration in closed form using moment matching
[cite:@girardGaussian2003;@deisenrothPILCO2011;@kamtheDataEfficient2018].
See cref:sec-moment-matching for more details.
This approximation has been shown to work well in RL
contexts [cite:@panSample2015;@panProbabilistic2014;@cutlerEfficient2015;@deisenrothPILCO2011;@deisenrothGaussian2015].

# The mode-constrained problem in cref:eq-main-problem can then be formulated as,

# Given this approach for making multi-step dynamics predictions, the mode-constrained problem in cref:eq-main-problem can
# be approximately formulated as,
*Objective*
Given this approach for making multi-step predictions, we use cref:def-delta-mode-remaining
to formulate a relaxed version of the mode-constrained problem in cref:eq-main-problem,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-greedy-objective}
\begin{align}
%&\pi_{\text{greedy}}=
&\argmax_{\pi \in \Pi}
\underbrace{\E_{p(\dynamicsFunc_{\desiredMode} \mid \dataset_{0:i})} \left[ J(\pi, \dynamicsFunc_{\desiredMode}) \right]}_{\text{greedy exploitation}},
\label{eq-greedy-objective-term}
\\
&\text{s.t. }
\underbrace{\Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \action_{0:t}, \dataset_{0:i})
\geq 1-\delta}_{\delta\text{-mode constraint}}, \quad \forall \timeInd \label{eq-expected-constraint}.
%\underbrace{\E_{\dynamicsFunc_{\desiredMode} \sim p(\dynamicsFunc_{\desiredMode} \mid \dataset_{0:i})}
%\left[ \Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{\timeInd}) \right]
%\geq 1-\delta}_{\delta\text{-mode constraint}}, \quad \forall \timeInd \label{eq-expected-constraint},
\end{align}
\end{subequations}
#+END_EXPORT
The expectation is taken over the desired dynamic mode's $\dynamicsFunc_{\desiredMode}$ GP.
Note that the expected objective in cref:eq-greedy-objective-term, which we refer to as greedy exploitation, is widely adopted.
For example, in PILCO [cite:@deisenrothPILCO2011], PETS [cite:@chuaDeepReinforcementLearning2018] and GP-MPC [cite:@kamtheDataEfficient2018].
We calculate the $\delta\text{-mode constraint}$ in cref:eq-expected-constraint as,
#+BEGIN_EXPORT latex
\begin{align} \label{eq--delta-mode-constraint-approx}
&\Pr (\modeVar_{\timeInd}=\desiredMode \mid \state_{0}, \action_{0:\timeInd}, \dataset_{0:i}) =   \\
\ &\int \Pr(\modeVar_{\timeInd}=\desiredMode \mid \gatingFunc(\state_{\timeInd}))
\underbrace{p(\gatingFunc(\state_{\timeInd}) \mid \state_{0}, \action_{0:\timeInd}, \dataset_{0:i})}_{\text{gating posterior at \timeInd}}
\text{d} \gatingFunc(\state_{\timeInd}), \nonumber
\end{align}
#+END_EXPORT
where $\Pr(\modeVar_{\timeInd}=\desiredMode \mid \gatingFunc(\state_{\timeInd}))$ resembles a classification likelihood (see cref:sec-gating-network-app).
We approximate the gating function posterior predicted $\timeInd$ steps into the future,
#+BEGIN_EXPORT latex
\begin{align}
&\underbrace{p(\gatingFunc(\state_{\timeInd}) \mid \state_{0}, \action_{0:\timeInd}, \dataset_{0:i})}_{\text{gating posterior at \timeInd}}
= \nonumber \\
&\quad \int
\underbrace{p\left(\gatingFunc(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i} \right)}_{\text{gating posterior}}
\underbrace{p(\state_{\timeInd} \mid \state_{0}, \action_{0:\timeInd-1}, \mathcal{D}_{0:i})}_{\text{dynamics posterior}} \text{d} \state_{\timeInd},
\end{align}
#+END_EXPORT
by propagating the state's uncertainty using moment matching (see cref:sec-moment-matching),
where the state distribution
${p(\state_{\timeInd} \mid \state_{0}, \action_{0:\timeInd-1}, \mathcal{D}_{0:i})}$ is obtained by cascading single-step predictions using
moment matching (see cref:sec-moment-matching).
Importantly, the $\delta\text{-mode constraint}$ considers 1) both the /epistemic/ and /aleatoric uncertainties/ in the desired dynamic mode
and 2) the /epistemic uncertainty/ in the gating network.
This ensures that the controlled system is $\delta\text{-mode-constrained}$ (cref:def-delta-mode-remaining) under the uncertainty of the learned dynamic model.
Intuitively, it will remain where the dynamic model's /uncertainty/ is low because the expectation results in lower probabilities for more uncertain states.

# Importantly, the $\delta\text{-mode constraint}$ considers the 1) /epistemic uncertainty/ in both the desired dynamic mode and in the gating network and
# the /aleatoric uncertainty/ in the desired dynamic mode.

# Importantly, the expected mode constraint considers the /epistemic uncertainty/ in both the desired dynamic mode
# $p(\dynamicsFunc_{\desiredMode}(\singleInput) \mid \singleInput, \dataset_{0:i})$
# and in the gating network $p(\GatingFunc(\state_{\timeInd}) \mid \state_{0}, \action_{0:\timeInd-1}, \dataset_{0:i})$.


# Importantly, the expectated mode constraint ensures that the controlled system remains in regions of the learned dynamics
# model with low /epistemic uncertainty/ because the expectation results in lower probabilities for more uncertain states.
# These constraints enforce the system to remain in the desired dynamic modewith satisfaction probability $\satisfactionProb=1-\delta$, at each time step.


# Note that we have simplified $\dynamicsFunc_{\desiredMode}(\singleInput) \sim p(\dynamicsFunc_{\desiredMode}(\singleInput) \mid \singleInput \dataset_{0:i})$

# In practice,
# over the state distribution obtained from cascading single-step predictions using moment matching,
# \todo{cite moment matching approximation}
# $q_{\mode{\theta}}(\state_{\timeInd+1}) = \int \dynamicsModelK(\state_{\timeInd+1} \mid \state_{\timeInd}, \action_{\timeInd}) q_{\mode{\theta}}(\state_{\timeInd}) \text{d} \state_{\timeInd}$,
# with $q_{\mode{\theta}}(\state_{0}) = \delta(\state_{0})$.

# where $q_{\mode{\theta}}(\dynamicsFunc_{\mode{\theta}}(\state_{\timeInd}, \action_{\timeInd})) = \int p_{\mode{\theta}}( \dynamicsFunc_{\mode{\theta}}(\state_{\timeInd}, \action_{\timeInd})  \mid \state_{\timeInd}, \action_{\timeInd}) q_{\mode{\theta}}(\state_{\timeInd}) \text{d} \state_{\timeInd}$

** FIGURE 3 - Greedy exploration - two column :ignore:
#+begin_export latex
\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.88\textwidth]{../experiments/figures/greedy_and_myopic_comparisons.pdf}
  \caption{(Left) show trajectories found by the greedy exploitation strategy in \cref{eq-greedy-objective}, with and without
  the $\delta\text{-mode constraint}$ (black line).
  (Right) shows trajectories found by our strategy in \cref{eq-joint-entropy-objective} when using our
  non-myopic exploration term $\mathcal{H}[\gatingFunc_{\desiredMode}(\stateTraj) \mid \stateTraj, \dataset_{0}]$,
  compared to using the myopic exploration term
  $\frac{1}{\TimeInd}\sum_{\timeInd=1}^{\TimeInd} \mathcal{H}[\gatingFunc_{\desiredMode}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0}]$.
  We overlay the trajectories on the GP posterior variance associated with desired mode's gating function
  $\mathbb{V}[\gatingFunc_{\desiredMode}(\state) \mid \state, \dataset_{0}]$ at episode $i=0$.}
  \label{fig-ablations}
\end{figure*}
#+end_export
  # \caption{(left) \textbf{Greedy exploitation results}.  (right) \textbf{Evaluation of our non-myopic exploration strategy}.
  # (left) shows the trajectories found by the greedy exploitation strategy in \cref{eq-greedy-objective}, with and without
  # the $\delta\text{-mode constraint}$ (black).
  # (right) shows the trajectories found by our strategy in \cref{eq-joint-entropy-objective} when using our
  # non-myopic exploration term $\mathcal{H}[\gatingFunc_{\desiredMode}(\stateTraj) \mid \stateTraj, \dataset_{0:2}]$,
  # compared to using the myopic exploration term
  # $\frac{1}{\TimeInd}\sum_{\timeInd=1}^{\TimeInd} \mathcal{H}[\gatingFunc_{\desiredMode}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:2}]$.
  # We overlay the trajectories on the GP posterior variance associated with desired mode's gating function
  # $\mathbb{V}[\gatingFunc_{\desiredMode}(\state) \mid \state, \dataset_{0:2}]$ at episode $i=2$.}
** Exploration :ignore:
# In our experiments, the constraint in cref:eq-expected-constraint hinders exploration and prevents the agent exploring around the mode boundary.
# This is because the mode constraint restricts the optimisation landscape, which causes the agent to get stuck in a local optimum.
# Consequently, the greedy strategy in cref:eq-greedy-objective was not able to sovle task.

# We propose to solve this problem by encouraging the agent to explore where it has high /epistemic uncertainty/ in the mode constraints.
*Exploration*
In our experiments, the constraint in cref:eq-expected-constraint hinders exploration and prevents the agent from solving the task.
This can be seen in the second from the left plot in cref:fig-ablations, where the mode constraint has induced a local
optimum and stopped the agent from reaching the target state.
We name the approach in cref:eq-greedy-objective the greedy constrained strategy.
We propose to overcome the issue of local optima induced by the mode constraint,
by targeting exploration where the agent has high /epistemic uncertainty/ in the mode constraint.
We do this by augmenting our objective with an intrinsic exploration term that encourages the agent to explore where its gating network is /uncertain/.
We use the entropy of the desired mode's gating function $\gatingFunc_{\desiredMode}$ over a
trajectory $\stateTraj=\{\state_{0},\ldots, \state_{\TimeInd} \}$ as our intrinsic exploration term.
This leads to our strategy taking the form,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-joint-entropy-objective}
%\small
\begin{align}
%&\pi_{\text{entropy}}=
&\argmax_{\pi \in \Pi}
\underbrace{\E_{p(\dynamicsFunc_{\desiredMode} \mid \dataset_{0:i})} \left[ J(\pi, \dynamicsFunc_{\desiredMode}) \right]}_{\text{greedy exploitation}} +
\beta\underbrace{\mathcal{H} \left[
\desiredGatingFunction(\stateTraj) \mid \stateTraj, \dataset_{0:i} \right]}_{\text{exploration}} \\
%&\text{s.t. } \underbrace{\E_{\dynamicsFunc_{\desiredMode} \sim p(\dynamicsFunc_{\desiredMode} \mid \dataset_{0:i})} \left[ \Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{\timeInd}) \right]
%\geq 1-\delta}_{\delta \text{ mode constraint}} \quad \forall \timeInd.
&\text{s.t. } \underbrace{ \Pr \left(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \action_{0:\timeInd}, \mathcal{D}_{0:i} \right)
\geq 1-\delta}_{\delta\text{-mode constraint}} \quad \forall \timeInd.
%\in \{ 0, \ldots, \TimeInd \} \nonumber
\end{align}
\end{subequations}
%\normalsize
#+END_EXPORT
where $\beta$ is a hyperparameter that sets the level of exploration.
Intuitively, the entropy term should enable the agent to escape local optima induced by the mode constraint as it
encourages the agent to explore away from regions of the mode constraint that it has already observed.
This is because the gating network's /epistemic uncertainty/ will be low where it has observed the environment.

*Epistemic vs aleatoric uncertainty*
When adopting this approach, it is extremely important to disentangle the gating network's /epistemic uncertainty/ from its /aleatoric uncertainty/.
For example, what is the meaning of the agent's belief in the mode indicator variable
tending to a uniform distribution i.e. $\Pr(\modeVar=\modeInd \mid \state, \dataset_{0:i})=\frac{1}{\ModeInd}$?
Does it mean that the agent has not observed the environment near $\state$ (high /epistemic uncertainty/), or
that it has observed the environment near $\state$ but is still uncertain which mode governs the dynamics (high /aleatoric uncertainty/)?
We show the importance of disentangling these sources of uncertainty in our experiments.
This motivated our GP-based gating network in the MoGPE dynamic model, as it principally
disentangles the sources of uncertainty over $\alpha$, by representing the /epistemic uncertainty/ in the gating network's GPs.
# The eager reader can compare the results in cref:fig-joint-gating,fig-bernoulli-gating.
# This motivated our GP-based gating network in the MoGPE dynamic model as is principally
# disentangles the sources of uncertainty over $\alpha$ by representing the /epistemic uncertainty/ in the GPs over the gating functions.

# Does it mean that the agent has high /epistemic uncertainty/ because it has not observed the environment near $\state$?
# Or does it mean that the agent has high /aleatoric uncertainty/ because it has observed the environment near $\state$ but still cannot be confident
# which mode governs the dynamics?
# We show the importance of disentangling these sources of uncertainty in our ablation study.
# The eager reader can compare the results in cref:fig-joint-gating,fig-bernoulli-gating.
# This motivated our GP-based gating network in the MoGPE dynamic model as is principally
# disentangles the sources of uncertainty over $\alpha$ by representing the /epistemic uncertainty/ in the GPs over the gating functions.


*Non-myopic exploration*
Further to providing a principled approach for disentangling sources of uncertainty, our GP-based gating network enables us to deploy a non-myopic exploration strategy.
That is, we are able to steer the agent along a trajectory that will maximise uncertainty reduction over the entire trajectory,
by considering the joint entropy over a trajectory $\mathcal{H}[\gatingFunc_{\desiredMode}(\stateTraj) \mid \stateTraj, \dataset_{0}]$.
In contrast, myopic exploration considers uncertainty reduction of each state independently without considering the influence of other states in the trajectory,
e.g. $\frac{1}{\TimeInd}\sum_{\timeInd=0}^{\TimeInd}\mathcal{H}[\gatingFunc_{\desiredMode}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0}]$.

# \todo{cite more information based strategies, [cite:@houthooftVIME2017]}

# It is worth noting here that previous MoGPE methods do not obtain
# For this reason, we consier augmenting the objective with an extra term to promote exploration.

# Our method disentangles the sources of uncertainty over $\alpha$ as it represents /epistemic uncertainty/ in
# the GPs over the gating functions.
# Our method disentangles the sources of uncertainty over $\alpha$ as it represents /epistemic uncertainty/ in
# the GPs over the gating functions.
# Our intrinsic term enables the agent to escape the local optimum as it encourages the agent to explore
# away from regions of the mode boundary that it has already observed; because it will have reduced
# the gating network's /epistemic uncertainty/ in this region.

# corresponeds to reducing the gating network's /epistemic uncertainty/
# when collectecting data from the environment and retraining the model.


# In our experiments, many common exploration strategies performed poorly because they do not consider information gain over entire trajectories jointly but
# instead only consider information

# For example, summing the dynamic model's predictive entropy over a horizon (without conditioning on the rest of the trajectory), results in the
# trajectory collapsing on a point of maximum entropy.
# That is, the policy navigates to a point of high entropy and then remains at this location for the rest of the trajectory.


# It is common to see intrinsic exploration terms which favour actions that steer the system to states corresponding
# to model parameters with high entropy or to states maximising the information gain in the dynamic model's parameters.

# model is not updated with hallucinated observations during the planning phase.

# Following this intuition, we augmented the reward function with the gating function's entropy at a particular state
# $\mathcal{H} \left[\gatingFunc_{\desriedMode}(\state_{t}) \mid \state_{t}, \dataset_{0:i} \right]$, which is equivalent to adding the following objective
# $\sum_{\timeInd=0}^{\TimeInd}\mathcal{H} \left[\gatingFunc_{\desriedMode}(\state_{t}) \mid \state_{t}, \dataset_{0:i} \right]$, which is equivalent
# cref:

# Based on this observation, we augmented the objective by considering the gating entropy over all time steps jointly,
# $\mathcal{H} \left[\gatingFunc_{\desriedMode}(\state_{0:t}) \mid \state_{0:t}, \dataset_{0:i} \right]$.

# *Greedy Exploration*
# *Thompson sampling*
# *Upper Confidence Reinforcement Learning (UCRL)*
# *Intrinsic motivation*
# *VIME*


# *Policy $\pi$*
# ModeRL relies on its policy to explore the environment.
# The goal of the policy is to maximise the expected sum of rewards, whilst remaining in the desired dynamic mode.
# To promote exploration we augment the objective function in cref:eq-objective with an exploration term to encourage the policy to
# explore the environment.
# We us the explorative trajectory optimisation algorithm detailed in cref:sec-exploration.


# During the planning phase we exploit our mode constraint and make multi-step predictions using only the desired dynamic mode $p_{\mode{\theta}}$.
# This enables us to approximate GP dynamics integration in closed from using moment matching.
# Given a reward function with quadratic form, for example,
# $\rewardFunc(\state_{\timeInd}, \action_{\timeInd}) = \left(\state_{\timeInd} - \targetState \right)^{T}\mathbf{Q} \left(\state_{\timeInd}-\targetState \right)$,
# we can calculate our objective in closed form.
# \todo{cite moment matching approximation}
# #+BEGIN_EXPORT latex
# \begin{subequations} \label{eq-explorative-traj-opt-2}
# \begin{align}
# \max_{\pi \in \Pi}
# %&\underbrace{J(\pi, \dynamicsModelK)}_{\text{exploitation}} +
# &\underbrace{\E_{\dynamicsFunc_{\mode{\theta}} \sim q_{\mode{\theta}}}\left[ J(\pi, \dynamicsFunc_{\mode{\theta}}) \right]}_{\text{exploitation}} +
# \underbrace{\mathcal{H} \left[
# \desiredGatingFunction(\stateTraj) \mid \stateTraj, \dataset_{0:i-1} \right]}_{\text{exploration}} \\
# %\text{s.t. } &\state_{\timeInd+1} \sim q_{\mode{\theta}}(\state_{\timeInd+1}) \quad &&\forall \timeInd \in \{0, \ldots, \TimeInd-1\} \\
# \text{s.t. } &\E_{\state_{\timeInd}\sim q_{\mode{\theta}}(\state_{\timeInd})} \left[ \Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{\timeInd}) \right]
# \geq 1-\delta \quad \forall \timeInd \in \{ 0, \ldots, \TimeInd \}
# \end{align}
# \end{subequations}
# #+END_EXPORT
# where $q_{\mode{\theta}}(\state_{\timeInd+1}) = \int \dynamicsModelK(\state_{\timeInd+1} \mid \state_{\timeInd}, \action_{\timeInd}) q_{\mode{\theta}}(\state_{\timeInd}) \text{d} \state_{\timeInd}$


# *$\delta\text{-mode remaining}$ chance constraints*
# We enforce the controlled system to remain in the desired dynamic modeas it allows us to approximate
# our objective in closed form and as a result we can use a non deterministic optimiser such as LBFGS.
# This is possible because we can approximate multi-step predictions (integration) using the moment matching approximation.

** Optimisatoin :ignore:
# \todo[inline]{We solve the optimisation problem in ..... with SLSQP}

# *Trajectory optimisation*
# We solve the constrained optimisation problem in cref:eq-joint-entropy-objective using
# Sequential Least Squares Quadratic Programming (SLSQP) in SciPy [cite:@2020SciPy-NMeth].
# In particular, we use the TensorFlow [cite:@tensorflow2015-whitepaper] wrapper provided by GPflow [cite:@GPflow2017].

# *Trajectory optimisation*
# We solve the constrained optimisation problem in cref:eq-joint-entropy-objective using
# sequential least squares quadratic programming (SLSQP) in SciPy [cite:@2020SciPy-NMeth].
# In particular, we use the TensorFlow [cite:@tensorflow2015-whitepaper] wrapper provided by GPflow [cite:@GPflow2017].

# Given a start state $\state_{0}$, ModeRL solves the following optimal control problem,
*Trajectory optimisation*
We use an open-loop trajectory optimisation policy because it naturally handles the $\delta\text{-mode}$ constraint
in cref:eq-joint-entropy-objective.
Given a start state $\state_{0}$, ModeRL finds the action sequence $\bar{\action} = \{\action_{0},\ldots,\action_{\TimeInd-1}\}$ solving the following optimal control problem,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-trajectory-optimisation}
\begin{align}
&\argmax_{\action_{0}, \ldots, \action_{\TimeInd-1}}
\underbrace{\E_{p(\dynamicsFunc_{\desiredMode} \mid \dataset_{0:i})} \left[ J(\pi, \dynamicsFunc_{\desiredMode}) \right]}_{\text{greedy exploitation}}
+ \beta \underbrace{\ln \left( | (2\pi e) \bm\Sigma_{\desiredMode}(\bar{\state}, \bar{\state}) | \right)}_{\text{exploration}} \\
%+ \beta\underbrace{\mathcal{H} \left[
%\desiredGatingFunction(\stateTraj) \mid \stateTraj, \dataset_{0:i} \right]}_{\text{exploration}} \\
&\text{s.t. } \underbrace{ \Pr \left(\modeVar_{\timeInd} = \desiredMode \mid \state_{0}, \action_{0:t}, \mathcal{D}_{0:i} \right)
\geq 1-\delta}_{\delta\text{-mode constraint}} \quad \forall \timeInd \in \{0,\ldots, \TimeInd\}, \label{eq-constraint-approx}
\end{align}
\end{subequations}
#+END_EXPORT
where the greedy exploitation term is an expectation over the state distribution obtained from making multi-step predictions
in the desired dynamic mode's GP using moment-matching, see cref:eq-moment-matching-system-function.
$\bm\Sigma^{2}_{\desiredMode}(\bar{\state},\bar{\state})$ is the predictive covariance of the desired mode's
gating function posterior over the trajectory $\bar{\state}$, given in cref:eq-joint-gating-covariance.
It is worth noting a closed-loop policy can be obtained using MPC.
However, this would require our algorithm to be made faster,
for example, via locally linear dynamics approximations.
Alternatively, a closed-loop policy could be learned, for example, via guided policy search [cite:@levineGuided2013].

 # via MPC or using our trajectory opt.
# It is worth noting here that we leave obtaining a closed-loop policy for future work.

# policy optimisation by reformulating them as a sum of discounted rewards with the mode chance constraints implemented via Lagrange
# multipliers.

# where the $\delta\text{-mode constraint}$ is calculated using cref:eq--delta-mode-constraint-approx and the greedy
# exploitation is an expectation over the state disribution obtained from making multi-step predictions in the desired
# mode's dynamics GP using the moment-matching approximation, see cref:eq-moment-matching-system-function.
# epistemic and aleatoric uncertainties.

# #+BEGIN_EXPORT latex
# \begin{subequations} \label{eq-trajectory-optimisation}
# \begin{align}
# &\pi =
# \argmax_{\action_{0}, \ldots, \action_{H}}
# \underbrace{\E_{\dynamicsFunc_{\desiredMode} \sim p(\dynamicsFunc_{\desiredMode} \mid \dataset_{0:i})} \left[ J(\pi, \dynamicsFunc_{\desiredMode}) \mid \state_{0}=\state \right]}_{\text{greedy exploitation}} +
# \beta\underbrace{\mathcal{H} \left[
# \desiredGatingFunction(\stateTraj) \mid \stateTraj, \dataset_{0:i} \right]}_{\text{exploration}} \\
# &\text{s.t. } \underbrace{\E_{\dynamicsFunc_{\desiredMode} \sim p(\dynamicsFunc_{\desiredMode} \mid \dataset_{0:i})} \left[ \Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{\timeInd}) \mid \state_{0}=\state \right]
# \geq 1-\delta}_{\delta-\text{mode-constraint}} \quad \forall \timeInd. \label{eq-constraint-approx}
# \end{align}
# \end{subequations}
# #+END_EXPORT
# where cref:eq-constraint-approx is given by
# #+BEGIN_EXPORT latex
# \begin{align}
# &\text{s.t. } \underbrace{\E_{\dynamicsFunc_{\desiredMode} \sim p(\dynamicsFunc_{\desiredMode} \mid \dataset_{0:i})} \left[ \Pr(\modeVar_{\timeInd} = \desiredMode \mid \state_{\timeInd}) \mid \state_{0}=\state \right]
# \geq 1-\delta}_{\delta \text{ mode constraint}} \quad \forall \timeInd. \label{eq-constraint-approx}
# \end{align}
# #+END_EXPORT

** FIGURE  4 - Uncertainy comparison - two column :ignore:
#+begin_export latex
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{../experiments/figures/uncertainty_comparison.pdf}
    \caption{\textbf{Epistemic vs aleatoric uncertainty} Illustration of the shortcomings of using the entropy of the mode indicator variable
    (\textit{aleatoric uncertainty}) for exploration.
    (Left) shows that the entropy of the mode indicator variable $\mathcal{H}[\modeVar \mid \state, \dataset_{0:10}]$ (\textit{aleatoric uncertainty}) at a region of the mode boundary
    which has been observed (black crosses), is high (red), even though we have observed the environment at these states.
    In contrast, (middle) shows that the entropy of the desired mode's gating function $\mathcal{H}[\gatingFunc_{\desiredMode}(\state) \mid \state, \dataset_{0:10}]$
    (\textit{epistemic uncertainty}) is low (white).
    (Right) visualises ModeRL converging to a local optimum when we replace the intrinsic exploration term in \cref{eq-joint-entropy-objective} with the
    entropy of the mode indicator variable.}
    \label{fig-uncertainty-comparison}
\end{figure*}
#+end_export
    # \caption{\textbf{Epistemic vs aleatoric uncertainty} This figure demostrates the shortcomings of using the entropy of the mode indicator variable
    # (\textit{aleatoric uncertainty}) for exploration.
    # (left) visualises the entropy of the mode indicator variable $\mathcal{H}[\modeVar \mid \state, \dataset_{0:20}]$ (\textit{aleatoric uncertainty}) at a region of the mode boundary
    # which has been observed (black crosses). The entropy is high even though we have observed the environment at these states.
    # In contrast, (middle) shows that the entropy of the gating function $\mathcal{H}[\gatingFunc_{\desiredMode}(\state) \mid \state, \dataset_{0:20}]$
    # is low. This is because it represents the \textit{epistemic uncertainty}.
    # We use the trained model and data set collected at episode $i=20$ of the experiment from \cref{fig-joint-entropy-four-episodes} -- which uses the
    # joint gating function entropy -- to create the left and middle plots.
    # (right) visualises our method converging to a local optimum when we replace the eploration term in \cref{eq-joint-entropy-objective} with the
    # entropy of the mode indicator variable, i.e. we replace the \textit{epistemic uncertainty} with \textit{aleatoric uncertainty}.}

* EXPERIMENTAL RESULTS
** intro :ignore:

We test ModeRL on a 2D quadcopter navigation example, where the goal is to navigate to a target state $\state_{f}$, whilst avoiding a turbulent dynamic mode.
See cref:illustrative_example for a schematic of the environment and details of the problem.
Our experiments seek to answer the following questions:
1. Why does the greedy exploitation strategy in cref:eq-greedy-objective fail to solve the mode-constrained problem in cref:eq-main-problem?
2. Does ModeRL, our strategy in cref:eq-joint-entropy-objective, solve the mode-constrained problem in cref:eq-main-problem?
3. Is it important to disentangle the sources of uncertainty in the mode constraint?
4. Does our non-myopic exploration strategy help?
5. How does the constraint level $\delta$ influence training?
# 4. Does a non-myopic exploration strategy provide benefits over a myopic exploration strategy?


# In mode-constrained RL, the mode constraint function is /unknown a priori/ and its output is not directly observed from the environment.
# To this end, we presented a nonparametric dynamic model to learn the $\delta\text{-mode}$ constraint -- as a latent variable -- alongside the dynamic modes.
# The novelty of ModeRL arises from its nonparametric dynamic model which learns the $\delta\text{-mode}$ constraint -- as a latent variable -- alongside the dynamic modes.
To evaluate ModeRL, we compare against the greedy baseline strategy in cref:eq-greedy-objective
(whose objective is used by PILCO [cite:@deisenrothPILCO2011], PETS [cite:@chuaDeepReinforcementLearning2018], GP-MPC [cite:@kamtheDataEfficient2018]),
both with and without the $\delta\text{-mode}$ constraint in cref:eq-expected-constraint.
Our experiments' configurations are detailed in cref:sec-experiment-configuration.

# 1. Why does the greedy exploitation strategy in cref:eq-greedy-objective fail to solve the mode-constrained problem in cref:eq-main-problem?
# 2. Does our strategy in cref:eq-joint-entropy-objective, which targets exploration where the mode constraint's /epistemic uncertainty/ is high, solve the mode-constrained problem in cref:eq-main-problem?
# 3. Is it important to disentangle sources of uncertainty in the mode constraint?
# 4. Does our non-myopic exploration strategy provide benefits over a myopic exploration strategy?

# 5. How well does our approach satisfay the mode constraint during exploration?

# 4. Does our non-myopic exploration strategy that considers uncertainty reduction over entire trajectories) provide benefits over a myopic exploration strategy?

# 1. Can greedy exploitation with mode constraints (cref:eq-greedy-objective) solve the mode-constrained problem in cref:eq-main-problem? Does the mode constraint effect learning? Does it prevent an agent from solving the task?
# 2. Can our approach solve the mode-constrained problem in cref:eq-main-problem?
# 3. How do different exploration ojectives effect the mode-constrained learning?
# 4. How well does our approach satisfay the mode constraint during learning?

# 4. How sample efficient is our aproach?
# 4. How does the exploration term have on the constrained exploration?
# 3. Does our strategy improve upon the mode-constrained greedy exploitation from cref:eq-greedy-objective?
# 4. How well do our probabilistic mode constraints keep the controlled system in the desired dynamic mode.

# As there is no prior work considering our specific mode-constrained RL problem,
# we evaluate our method against the greedy strategy in cref:eq-greedy-objective, both with and without the mode constraint.

# cannot be learned with supervised learning and requires the dynamic model
# presented in this paper.

# As such, the $\delta\text{-mode}$ constraint cannot be learned with supervised learning and requires the dynamic model
# presented in this paper.
# As there is no prior work considering our specific mode-constrained RL problem,
# we evaluate our method against the greedy strategy in cref:eq-greedy-objective, both with and without
# the mode constraint.
# Our experiments configurations are detailed in cref:sec-experiment-configuration.

# However, we are able to use the greedy strategy in cref:eq-greedy-objective, both with and without the mode constraints, as baselines to evaluate our method.
# Our experiments configurations are detailed in cref:sec-experiment-configuration.

# We do not compare to a baseline from the literature because 1) there is no prior work considering mode-constrained RL
# and 2) our mode constraint is /unknown a priori/ and highly coupled to our dynamic model.
# As a result, any constrained RL method would still require our dynamic model to infer the mode constraint.
# However, we are able to use the greedy strategy in cref:eq-greedy-objective, both with and without the mode constraints, as baselines to evaluate our method.
# Our experiments configurations are detailed in cref:sec-experiment-configuration.

# #+BEGIN_EXPORT latex
# \begin{subequations} \label{eq-explorative-traj-opt}
# \begin{align}
# %\pi(\timeInd)
# &\argmax_{\action_{0}, \ldots, \action_{\TimeInd-1}}
# \sum_{\timeInd=1}^{\TimeInd}
# \underbrace{- (\state_{\timeInd} - \targetState)^T \mathbf{Q}
# (\state_{\timeInd} - \targetState)
# +\text{Tr} \left(\bm\mu^{\text{MM}}_{\state_{\timeInd}} \bm\Sigma^{\text{MM}}_{\state_{\timeInd}} \right)}_{\text{state difference cost}} \nonumber \\
# &\quad\quad\quad - \underbrace{\control_{\timeInd}^{\TimeInd} \mathbf{R} \control_{\timeInd}}_{\text{control cost}}
# + \beta \underbrace{\ln \left( | (2\pi e) \bm\Sigma^2_{\desiredMode}(\bar{\state}, \bar{\state}) | \right)}_{\text{joint gating entropy}} \\
# &\quad \text{s.t. } \underbrace{\Phi \left( \frac{\bm\mu_{\desiredMode}(\state_{\timeInd})}{\sqrt{1+ \bm\Sigma^2_{\desiredMode}(\state_{\timeInd}, \state_{\timeInd})}} \right)
# \geq 1-\delta}_{\delta\text{-mode constraint}} \quad \forall \timeInd \in \{ 0, \ldots, \TimeInd \}
# \end{align}
# \end{subequations}
# #+END_EXPORT

# #+BEGIN_EXPORT latex
# \begin{subequations} \label{eq-explorative-traj-opt}
# \begin{align}
# %\pi(\timeInd)
# &\argmax_{\action_{0}, \ldots, \action_{\TimeInd-1}}
# \underbrace{- (\state_{\TimeInd} - \targetState)^T \mathbf{H}
# (\state_{\TimeInd} - \targetState)
# +\text{Tr} \left(\mathbf{H} \bm\Sigma^{\text{MM}}_{\state_{\TimeInd}} \right)}_{\text{terminal state reward}} \nonumber \\
# &- \sum_{\timeInd=0}^{\TimeInd-1} \big(
# \underbrace{- (\state_{\timeInd} - \targetState)^T \mathbf{Q}
# (\state_{\timeInd} - \targetState)
# -\text{Tr} \left(\mathbf{Q} \bm\Sigma^{\text{MM}}_{\state_{\timeInd}} \right)}_{\text{state difference reward}} \big)
# %- \underbrace{\control_{\timeInd}^{\TimeInd} \mathbf{R} \control_{\timeInd}}_{\text{control reward}}
# \nonumber \\
# &- \sum_{\timeInd=0}^{\TimeInd-1}
# \big( \underbrace{\control_{\timeInd}^{\TimeInd} \mathbf{R} \control_{\timeInd}}_{\text{control reward}} \big)
# + \beta \underbrace{\ln \left( | (2\pi e) \bm\Sigma^2_{\desiredMode}(\bar{\state}, \bar{\state}) | \right)}_{\text{joint gating entropy}} \\
# &\quad \text{s.t. } \underbrace{\Phi \left( \frac{\bm\mu_{\desiredMode}(\state_{\timeInd})}{\sqrt{1+ \bm\Sigma^2_{\desiredMode}(\state_{\timeInd}, \state_{\timeInd})}} \right)
# \geq 1-\delta}_{\delta\text{-mode constraint}} \quad \forall \timeInd \in \{ 0, \ldots, \TimeInd \}
# \end{align}
# \end{subequations}
# #+END_EXPORT

# *What do we solve?*
# Given the quadratic reward function in cref:eq-reward-func our objective has closed form.
# Further to this, we use two dynamic modes so our $\delta\text{-mode}$ constraint also has closed form.
# so that our $\delta\text{-mode}$ constraint also has closed form.
# At each episode, we solve the following optimal control problem,
# *What do we solve?*
Given the quadratic reward function in cref:eq-reward-func our objective has closed form.
Further to this, we use two dynamic modes so our $\delta\text{-mode}$ constraint also has a closed form.
At each episode, we then solve,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-explorative-traj-opt}
\begin{align}
%\pi(\timeInd)
&\argmax_{\action_{0}, \ldots, \action_{\TimeInd-1}}
\underbrace{- \|\bm\mu^{\text{MM}}_{\state_{\TimeInd}} - \targetState \|_{\mathbf{H}}
+\text{Tr} \left(\mathbf{H} \bm\Sigma^{\text{MM}}_{\state_{\TimeInd}} \right)}_{\text{terminal state reward}} \nonumber \\
&\quad\quad - \sum_{\timeInd=0}^{\TimeInd-1} \big(
\underbrace{\|\bm\mu^{\text{MM}}_{\state_{\timeInd}} - \targetState \|_{\mathbf{Q}}
-\text{Tr} \left(\mathbf{Q} \bm\Sigma^{\text{MM}}_{\state_{\timeInd}} \right)}_{\text{state difference reward}}
+ \underbrace{\|\control_{\timeInd} \|_{\mathbf{R}}}_{\text{control reward}}
\big)
%- \underbrace{\control_{\timeInd}^{\TimeInd} \mathbf{R} \control_{\timeInd}}_{\text{control reward}}
\nonumber \\
&\quad\quad + \beta \underbrace{\ln \left( | (2\pi e) \bm\Sigma^2_{\desiredMode}(\bar{\state}, \bar{\state}) | \right)}_{\text{joint gating entropy}} \\
&\quad \text{s.t. } \underbrace{\Phi \bigg( \frac{\bm\mu_{\desiredMode}(\state_{\timeInd})}{\sqrt{1+ \bm\Sigma^2_{\desiredMode}(\state_{\timeInd}, \state_{\timeInd})}} \bigg)
\geq 1-\delta}_{\delta\text{-mode constraint}} \quad \forall \timeInd \in \{ 0, \ldots, \TimeInd \}
\end{align}
\end{subequations}
#+END_EXPORT
where $\bm\mu_{\desiredMode}(\state_{\timeInd})$ and $\bm\Sigma^{2}_{\desiredMode}(\state_{\timeInd},\state_{\timeInd})$
are the predictive mean and covariance
of the desired mode's gating function posterior at
$\state_{\timeInd}$, given in cref:eq-gating-predictive-eqns,eq-joint-gating-covariance.
$\bm\mu^{\text{MM}}_{\state_{\timeInd}}$ and
$\bm\Sigma^{\text{MM}}_{\state_{t}}$ are the mean and covariance of the state predicted $\timeInd$ steps into the future
$p(\state_{\timeInd} \mid \state_{0}, \action_{0:\timeInd-1}, \mathcal{D}_{0:i})$, obtained
by cascading single-step predictions through the desired dynamic mode's GP with moment matching, given in cref:eq-moment-matching-system-function.
$\mathbf{H}$ and $\mathbf{Q}$ are user-defined, real symmetric positive semi-definite
weight matrices and $\mathbf{R}$ is a user-defined, positive definite weight matrix.
We solve  cref:eq-explorative-traj-opt
using sequential least squares quadratic programming (SLSQP) in SciPy [cite:@2020SciPy-NMeth],
using the TensorFlow [cite:@tensorflow2015-whitepaper] wrapper provided by GPflow [cite:@GPflow2017].
# We solve this constrained optimisation
# using sequential least squares quadratic programming (SLSQP) in SciPy [cite:@2020SciPy-NMeth],
# using the TensorFlow [cite:@tensorflow2015-whitepaper] wrapper provided by GPflow [cite:@GPflow2017].

# TODO We provide open-source code for our experiments at \url{https://github.com/aidanscannell/ModeRL}.

# We treat the greedy strategy as with no constraints as our baseline.
# We consider both the constrained and unconstrained greedy strategy.

# The environment trajectories (cyan) deviate from the dynamics trajectories (magenta) when the trajectories leave the desired dynamic mode.


# For obvious reasons, we leave this as future work.
# As there is no prior work considering mode-constrained RL, we do not compare to a baseline.
# This is exacerbated by our mode constraint being /unknown a priori/ and highly coupled to our dynamic model.
# However, we validate our method by comparing to the greedy strategy in cref:eq-greedy-objective
# We consider both the constrained and unconstrained greedy strategy.

# and our constraint implementation is highly coupled to our dynamic model, there is no baseline from
# the literature that warrants comparison.
# However, we thorougly evaluate our method by comparing to the greedy strategy in cref:eq-greedy-objective and

# As our mode constraint is /unknown a priori/ and our constraint implementation is highly coupled to our dynamic model, there is no baseline from
# the literature that warrants comparison.
# However, we thorougly evaluate our method by comparing to the greedy strategy in cref:eq-greedy-objective and

# Instead, we compare our method to the greedy strategy in cref:eq-greedy-objective.
# We consider both the constrained and unconstrained greedy strategy.


# without the mode constraint, i.e. solving the greedy strategy in cref:eq-greedy-objective
# We show the

# Due to the novelty of our problem, in particular, our mode constraint which is /unknown a priori/,
# there is no obvious baseline that we can compare to.

** results :ignore:

# *Greedy exploitation*
*Why does greedy exploitation fail?*
Using the greedy strategy without the mode constraint results in the optimisation finding trajectories that leave the desired dynamic mode.
This is illustrated in cref:fig-ablations (left), which shows the unconstrained greedy strategy converged to a solution navigating
straight to the target state.
As a result, the trajectory leaves the desired dynamic mode and passes through the turbulent dynamic mode.
This is expected as the strategy is not aware of the turbulent dynamic mode.
In contrast, the constrained greedy strategy in cref:fig-ablations (second left), does not leave the desired dynamic mode.
However, the optimisation does not converge to the global optimum, i.e. the trajectory does not navigate to the target state $\targetState$.
Instead, it gets stuck at the mode boundary, i.e. a local optimum.
# cref:fig-ablations (second left) shows the local optimum induced by the mode constraint.
# as it
# is well-known that gradient-based optimisers find local solutions.

# Solving the greedy objective with the mode constraints results in the optimisation getting stuck in a local optimum and never solving the task,
# i.e. navigating to the target state.

# If the strategy searched more to the left it would be able to expand
# the mode chance constraints and explore around the mode boundary.
# However, the mode chance constraints have induced a local optimum that prevented ModeRL
# from exploring in this direction.
# Importantly, our results show that disentangling the sources of /uncertainty/ in the mode constraint is crucial for solving our quadcopter navigation task.

# *Uncertainty guided exploration*
*Does our uncertainty-guided exploration work?*
The failure of the greedy strategy motivated our nonparametric dynamic model which learns the $\delta\text{-mode}$ constraint -- as a latent variable -- alongside the
dynamic modes.
Importantly, this enabled ModeRL (in cref:eq-joint-entropy-objective), to adopt an intrinsic exploration term,
which targets exploration where the mode constraint's /epistemic uncertainty/ is high.
cref:fig-joint-entropy-four-episodes shows four episodes $i$ of ModeRL in the quadcopter navtigation task.
Reading from left to right, the contours show how the agent's belief of being in the desired dynamic mode $\Pr(\modeVar=\desiredMode \mid \state, \dataset_{0:i})$
changes as the agent interacts with the environment, collects data $\dataset_{0:i}$ and updates its dynamic model, i.e. trains on $\dataset_{0:i}$.
It shows the $\delta\text{-mode-constrained}$ region (black line) expanding as the agent trains on the new observations.
The middle two plots show that ModeRL escaped the local optimum induced by the constraint.
They also show that ModeRL provides some level of constraint satisfaction during training.
Finally, the right-hand plot shows that ModeRL successfully navigated to the target state, i.e. it solved the task.

# It shows the $\delta\text{-mode-constrained}$ region (black line) expanding as the agent trains on the new observations,
# reducing its /epistemic uncertainty/.

# As a result, the agent has been able to successfully navigate to the target state, as  visualised in the right-hand plot.
# This supports our hypothesis that the mode constraint's /epistemic uncertainty/ can be used to escape local optima induced by the mode constraint.

# This motivated a strategy which favours searching regions of the state space which have not previously been observed and
# could likely avoid the local optima in cref:fig-explorative-state-diff-traj-opt-over-prob-7.
# This intuition motivated adding the gating entropy term in cref:eq-explorative-traj-opt, which favours exploration in
# regions of high /epistemic uncertainty/.

# *Epistemic vs aleatoric uncertainty*
*Is it important to disentangle sources of uncertainty?*
We now evaluate the importance of disentangling the sources of uncertainty in the mode constraint.
In the left plot of cref:fig-uncertainty-comparison we visualise the entropy of the mode indicator variable $\mathcal{H}[\modeVar \mid \state, \dataset_{0:10}]$
(/aleatoric uncertainty/) at a region of the mode boundary that the agent has observed (black crosses).
The entropy is high (red) even though we have observed the environment at these states.
As such, the agent would keep exploring the mode boundary even though it has been observed.
In contrast,  the middle plot shows the entropy of the gating function $\mathcal{H}[\gatingFunc_{\desiredMode}(\state) \mid \state, \dataset_{0:10}]$.
The entropy is low (white) around the observations (black crosses), indicating that our GPs are capturing the
mode constraint's /epistemic uncertainty/.
The entropy of the desired mode's gating function (cref:fig-uncertainty-comparison middle) is a much better
objective for exploration because it encourages the exploration away from the mode boundary once it has been observed.
The right plot of cref:fig-uncertainty-comparison shows results when using the entropy of the mode indicator variable
for the intrinsic exploration term.
It shows that the agent is not able to escape the local optimum induced by the mode constraint.

# The right plot of cref:fig-uncertainty-comparison shows that the agent is not able to escape the local optimum induced by the mode constraint,
# when using the entropy of the mode indicator variable for exploration.

# is not able to escape the local optimum induced by the mode constraint.
# our algorithm (using the entropy  of
# mode indicator variable for exploration) is not able to escape the local optimum induced by the mode constraint.

# In summary, cref:fig-uncertainty-comparison, shows that the entropy of the mode indicator variable $\mathcal{H}[\modeVar \mid \state, \dataset_{0:20}]$ is not
# a good choice for exploration.

# (right) visualises our method converging to a local optimum when we replace the eploration term in \cref{eq-joint-entropy-objective} with the
# entropy of the mode indicator variable, i.e. we replace the \textit{epistemic uncertainty} with \textit{aleatoric uncertainty}.


# This figure demostrates the shortcomings of using the entropy of the mode indicator variable
# (\textit{aleatoric uncertainty}) for exploration.
# (left) visualises the entropy of the mode indicator variable $\mathcal{H}[\modeVar \mid \state, \dataset_{0:20}]$ (\textit{aleatoric uncertainty}) at a region of the mode boundary
# which has been observed (black crosses). The entropy is high even though we have observed the environment at these states.
# In contrast, (middle) shows that the entropy of the gating function $\mathcal{H}[\gatingFunc_{\desiredMode}(\state) \mid \state, \dataset_{0:20}]$
# is low. This is because it represents the \textit{epistemic uncertainty}.
# We use the trained model and data set collected at episode $i=20$ of the experiment from \cref{fig-joint-entropy-four-episodes} -- which uses the
# joint gating function entropy -- to create the left and middle plots.
# (right) visualises our method converging to a local optimum when we replace the eploration term in \cref{eq-joint-entropy-objective} with the
# entropy of the mode indicator variable, i.e. we replace the \textit{epistemic uncertainty} with \textit{aleatoric uncertainty}.

# *Myopic exploration*
*Non-myopic vs myopic exploration?*
We now test the importance of our non-myopic exploration term, i.e. using the joint entropy of the desired mode's gating function over a trajectory $\mathcal{H}[\gatingFunc_{\desiredMode}(\stateTraj) \mid \stateTraj, \dataset_{0}]$,
instead of taking the mean of the gating function's entropy at each time step
$\frac{1}{\TimeInd}\sum_{\timeInd=0}^{\TimeInd}\mathcal{H} \left[\gatingFunc_{\desiredMode}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i} \right]$.
In the right plots of cref:fig-ablations, we overlay the trajectories found with the non-myopic and myopic exploration terms,
over the GP posterior variance associated with
desired mode's gating function $\mathbb{V}[\gatingFunc_{\desiredMode}(\state) \mid \state, \dataset_{0}]$ at episode $i=0$.
The right-hand plot of cref:fig-ablations shows that using the myopic exploration term results in the trajectory
navigating to a single state of high entropy and remaining in that state for the rest of the trajectory.
This is an undesirable behaviour.
In contrast, the second from right plot in cref:fig-ablations shows our non-myopic exploration
term $\mathcal{H}[\gatingFunc_{\desiredMode}(\stateTraj) \mid \stateTraj, \dataset_{0}]$ spreading out.
This is a desirable behaviour because it reduces the number of environment interactions that are required to solve the task,
i.e. it improves sample efficiency.
\todo[inline]{add sample efficiency results}


# # *Constraint vs non constraint*
# *Constraint satisfaction during exploration*
# Compare cref:fig-greedy-with-constraint vs cref:fig-greedy-no-constraint




# do not consider uncertainty reduction over the entire trajectory.
# Instead, they only consider uncertainty reduction of each state independently without considering the influence of other states in the trajectory.
# For example, in our GP-based gating network, the myopic exploration strategy would correspond to taking the mean of the gating function's entropy at
# each time step
# $\frac{1}{\TimeInd}\sum_{\timeInd=0}^{\TimeInd}\mathcal{H} \left[\gatingFunc_{\desiredMode}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i} \right]$.
# Considering the entropy over a horizon (without conditioning on the rest of the trajectory), results in the trajectory collapsing to a single point of high entropy.


# Further to providing a principled approach for disentangling sources of uncertainty, our GP-based gating network enables us to deploy a non-myopic exploration strategy.
# That is, our strategy is able to steer the agent along a state trajectory that will maximise the information gain over the entire trajectory.
# In contrast, myopic exploration strategies do not consider uncertainty reduction over the entire trajectory.
# Instead, they only consider uncertainty reduction of each state independently without considering the influence of other states in the trajectory.
# For example, in our GP-based gating network, the myopic exploration strategy would correspond to taking the mean of the gating function's entropy at
# each time step
# $\frac{1}{\TimeInd}\sum_{\timeInd=0}^{\TimeInd}\mathcal{H} \left[\gatingFunc_{\desiredMode}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i} \right]$.
# Considering the entropy over a horizon (without conditioning on the rest of the trajectory), results in the trajectory collapsing to a single point of high entropy.
# This can be seen in the right hand plot in cref:fig-ablations as the trajectory navigates to a point of high entropy and then remains at this location
# for the rest of the trajectory.
# In contrast, the second from right plots in cref:fig-ablations shows our non-myopic exploration
# term $\mathcal{H}[\gatingFunc_{\desiredMode}(\stateTraj) \mid \stateTraj, \dataset_{0:2}]$, spreading out

# third  by considering the entropy over a trajectory, we are able to find trajectories that spread out.
# For example, see cref:fig-joint-gating.





# The right hand plots in cref:fig-ablations compares our non-myopic exploration term to its myopic counterpart of
# taking the mean of the gating function's entropy at each time step
# $\frac{1}{\TimeInd}\sum_{\timeInd=0}^{\TimeInd}\mathcal{H} \left[\gatingFunc_{\desiredMode}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i} \right]$.

# The trajectory found by the myopic exploration
# term  $\frac{1}{\TimeInd}\sum_{\timeInd=1}^{\TimeInd} \mathcal{H}[\gatingFunc_{\desiredMode}(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:2}]$,
# at episode $i=2$ navigates to a point of high entropy and then remains at this location

# non-myopic exploration term $\mathcal{H}[\gatingFunc_{\desiredMode}(\stateTraj) \mid \stateTraj, \dataset_{0:2}]$,
# compared to using the myopic exploration term



# non-myopic exploration term $\mathcal{H}[\gatingFunc_{\desiredMode}(\stateTraj) \mid \stateTraj, \dataset_{0:2}]$,
# compared to using the myopic exploration term
# at episode $i=2$, overlayed on the GP posterior variance associated with desired mode's gating function
# $\mathbb{V}[\gatingFunc_{\desiredMode}(\state) \mid \state, \dataset_{0:2}]$.}

# vs cref:fig-bernoulli-gating

# our \
# Compare cref:fig-joint-gating vs cref:fig-bernoulli-gating

# Maybe show gating functin variance and prob next to each other?

# *Informative states vs informative trajectories*
# *Non-myopic vs myopic exploration*
# Compare cref:fig-joint-gating} vs cref:fig-independent-gating

# It is common to see intrinsic exploration terms which favour actions that steer the system to states corresponding
# to model parameters with high entropy or to states maximising the information gain in the dynamic model's parameters.
# [cite:@houthooftVIME2017]
# \todo{cite more information based strategies}

# cref:fig-joint-gating vs cref:fig-independent-gating

# model is not updated with hallucinated observations during the planning phase.

# Following this intuition, we augmented the reward function with the gating function's entropy at a particular state
# $\mathcal{H} \left[\gatingFunc_{\desriedMode}(\state_{t}) \mid \state_{t}, \dataset_{0:i} \right]$, which is equivalent to adding the following objective
# $\sum_{\timeInd=0}^{\TimeInd}\mathcal{H} \left[\gatingFunc_{\desriedMode}(\state_{t}) \mid \state_{t}, \dataset_{0:i} \right]$, which is equivalent
# cref:

# Based on this observation, we augmented the objective by considering the gating entropy over all time steps jointly,
# $\mathcal{H} \left[\gatingFunc_{\desriedMode}(\state_{0:t}) \mid \state_{0:t}, \dataset_{0:i} \right]$.

# However, we observed that simply augmenting the reward function with the state entropy $\mathcal{H} \left[\gatingFunc_{\desriedMode}(\state_{t}) \mid \state_{t}, \dataset_{0:i} \right]$

** constraint satisfaction during training :ignore:

*Constraint satisfaction during training*
Finally, we evaluate how the constraint level $\delta$ influences training.
cref:fig-num_constrint_violations_constraint_levels_ablation confirms that tightening the constraint (i.e. decreasing $\delta$) leads to less constraint violations.
This is shown by the accumulated number of episodes with constraint violations $N^{\modeVar}_{i}$ increasing more slowly for lower $\delta\text{'s}$.
cref:fig-episode_return_constraint_levels_ablation shows the training curves for five constraint levels $\delta$.
It shows that relaxing the constraint results in higher sample efficiency.
This is indicated by the training curves for lower $\delta\text{'s}$ converging in fewer episodes, e.g. $\delta=0.5$ (blue).
cref:fig-episode_return_constraint_levels_ablation further shows that ModeRL is not able to solve the task when the constraint is too tight, e.g. $\delta \leq 0.2$ (red/purple).
This is indicated by the asymptotic performance not matching that of lower $\delta\text{'s}$.
Finally, we place an exponentially decaying schedule on $\delta$ ($\delta_{0}^{s}$ brown), which tightens the
constraint during training.
This strategy solved the task in fewer episodes than the fixed $\delta$ experiments, indicating that it
is more sample efficient.
It also resulted in fewer constraint violations both during and after training.

** FIGURE  5 - constraint level ablation - two column :ignore:
#+begin_export latex
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{../experiments/figures/episode_return_constraint_levels_ablation.pdf}
    \caption{\textbf{Constraint level ablation} Training curves for different constraint levels $\delta$. They show that looser constraints (high $\delta$) have better sample efficiency. They further show that if the constraint is too tight (i.e. $\delta\leq0.2$) then ModeRL gets stuck in local optima and cannot solve the task. Curves show the mean and 95\% confidence interval of the episode return for five random seeds, at each episode $i$ of training. The $\delta^{s}_{0}=4$ experiment used an exponential schedule to tighten the constraint during training. }
    \label{fig-episode_return_constraint_levels_ablation}
\end{figure}
#+end_export
** Practical Considerations

*Warm start trajectory optimisation*
In practice, the constrained optimisation in cref:eq-joint-entropy-objective fails if the initial trajectory does not satisfy the $\delta\text{-mode constraint}$.
We overcome this by solving an unconstrained optimisation to a "fake" target state in the initial
state domain $\stateDomain_{0}$ and using it to warm start our trajectory optimiser.
# We overcome this by warm starting our trajectory optimiser by solving an unconstrained optimisation to a "fake" target state in the
# This ensures our initial trajectory satisfies the mode constraints.
# *Warm start trajectory optimisation*
# In practice, the constrained optimisation in cref:eq-joint-entropy-objective fails if the initial trajectory does not satisfy the mode constraints.
# We solve this issue by warm starting our trajectory optimiser by sampling a "fake" target state from the initial data set $\dataset_0$ and optimising
# the trajectory without the mode constraint.
# This ensures our initial trajectory satisfies the mode constraints.

# the initial trajectory

# to ensure the initial trajectory satisfies the constraints.
# We sample a "fake" target state from the initial data set $\dataset_0$ and find a trajectory  the initial trajectory

# As such, this work deploys a simple strategy to ensure the initial trajectory satisfies the constraints.
# We sample a "fake" target state from the initial data set $\dataset_0$ and optimise the initial trajectory
# using the cost function in cref:eq-quadratic-cost-control.
# This finds a straight line trajectory between the start state $\state_0$ and the "fake" target state which is in the initial state domain.
# Experiments show that this procedure finds trajectories where all of the time steps are in the desired mode with high probability.

*Fixing model parameters during training*
Initially, ModeRL explores the desired dynamic mode and does not observe any state transitions from other modes.
As such, we fix the kernel hyperparameters (e.g. lengthscale and signal variance) associated with the gating network GP.
This prevents the $\delta\text{-mode-constrained}$ region from expanding significantly further than the observed data.

# This results in the $\delta\text{-mode-constrained}$ region expanding significantly further than the observed data because the
# gating function's GP learns a long lengthscale.
# For this reason, we fix the kernelâ€™s hyperparameters (e.g. lengthscale and signal variance).
# We also fix the noise variance in the non desired dynamic modes.

*Inducing points*
We initialise each of the sparse GPs with a fixed number of inducing points uniformly sampled from $\dataset_{0}$.
Although this approach worked well in our experiments, it is unlikely to scale to larger problems.
As such, an interesting direction for future work is to study methods for dynamically adding new inducing points to each GP.
# Although this approach worked well in the experiments, it is unlikely that this will always be the case.
# For example, when exploring environments with much larger state domains.
# In these environments, the \acrshort{mosvgpe}'s ability to accurately model an ever increasing data set with a fixed number of inducing points will decrease.
# This is due to the sparse approximationâ€™s ability to model the true nonparametric model deteriorating as the number of data points increases.
# See cite:burtRates2019 for details on rates of convergence for sparse GPs.
# As such, an interesting direction for future work is to study methods for dynamically adding new inducing points to each GP.

* CONCLUSION label:sec-conclusion
We introduced ModeRL, a Bayesian model-based RL algorithm for low-dimensional continuous control problems,
that constrains exploration to a single dynamic mode (up to a given probability).
Intuitively, ModeRL relaxes the mode-constrained RL problem in cref:def-mode-remaining-main, making it feasible,
whilst still providing "some level" of constraint satisfaction during training.
It uses a nonparametric dynamic model to learn the mode constraint -- as a latent variable -- alongside the dynamic modes.
Importantly, it disentangles the sources of uncertainty in the learned mode constraint.
Our experiments show that our nonparametric formulation of the mode constraint is essential for solving the quadcopter navigation task,
as it enabled our planning algorithm to escape local optima that other strategies could not.
# , which enabled ModeRL to formulate a non-myopic intrinsic exploration term.

** Limitations :ignore:
*Limitations*
The main limitation of ModeRL is that it is restricted to lower dimensional problems,
due to the difficulties of defining GP priors in high dimensions.
Another (potentially unavoidable) downside of ModeRL,
is that it must leave the desired dynamic mode in order to learn about the mode constraint.
This is because we used internal sensing.
However, in some applications, it may be possible to infer the mode constraint using external sensors.
For example, in autonomous driving, it may be possible to infer dynamic modes associated with different road surfaces
without leaving the desired dynamic mode, by using cameras.
Finally, ModeRL uses an open-loop policy.
An interesting direction for future work is to make our algorithm faster so that it can be used to formulate a closed-loop policy via MPC.

# This is because we use internal sensing to obtain information on the robot.

** old :noexport:
# This enabled ModeRL to escape local optima induced by the mode constraint via intrinsic exploration.
# Further to this, the nonparametric mode constraint enabled ModeRL to formulate a non-myopic intrinsic exploration term.
# Our experiments show that our nonparametric formulation of the mode constraint is essential
# for solving the quadcopter navigation task, as it enabled our planning algorithm to escape local optima induced by the mode constraint.

# to enable our planning algorithm to escape local
# optima induced by the mode constraint and solve the task.

# In our experiments, ModeRL optimises its closed-form objective, subject to its closed-form constraints, using
# sequential least squares quadratic programming (SLSQP).

# targeting exploration where the mode constraint's /epistemic uncertainty/ is high,
# enables ModeRL's planning algorithm to escape local optima induced by the mode constraint and solve the task.


# joint distributions over simulated trajectories enabled ModeRL to construct a

# Further to this, the GPs ability to model joint distributions over simulated trajectories enabled ModeRL to construct a
# non-myopic exploration strategy.
# For planning, ModeRL optimises its closed-form objective, subject to its closed-form constraints, using
# sequential least squares quadratic programming (SLSQP).
# Our experiments showed that targeting exploration where the mode constraint's /epistemic uncertainty/ is high,
# enables ModeRL's planning algorithm to escape local optima induced by the mode constraint and solve the task.


# We introduced ModeRL, a Bayesian model-based RL algorithm for low-dimensional continuous control problems, that constrains exploration to
# a single dynamic modeup to a given probability.
# It uses a MoGPE method to infer the mode constraint alongside the underlying dynamic modes.
# Importantly, the GP-based gating network disentangles the sources of uncertainty in the learned mode constraint,
# enabling ModeRL to escape local optima induced by the mode constraint by targeting exploration where the mode constraint's /epistemic uncertainty/ is high.
# Further to this, the GPs ability to model joint distributions over simulated trajectories enabled ModeRL to construct a
# non-myopic exploration strategy.
# For planning, ModeRL optimises its closed-form objective, subject to its closed-form constraints, using
# sequential least squares quadratic programming (SLSQP).
# Our experiments showed that targeting exploration where the mode constraint's /epistemic uncertainty/ is high,
# enables ModeRL's planning algorithm to escape local optima induced by the mode constraint and solve the task.

# For example, in autonomous driving, it may be possible to infer the friction coefficients associated with different road surfaces using cameras without ever leaving the
# desired dynamic mode.
# Although not applicable in all settings, this is a promising direction for future work, as the agent would never have to enter the undesired dynamic mode.

# It uses a MoGPE method to infer both the underlying dynamic modes and the mode constraint from observations.

# For planning, ModeRL optimises its closed-form objective, subject to its closed-form constraints, using
# Sequential Least Squares Quadratic Programming (SLSQP).

# In model-based RL there has been interesting progress learning better statistical models and scaling them
# up to higher-dimensional problems, for example, using Bayesian neural networks.
# This is an interesting direction for future work as it may lead to more practical algorithms.


# disentangling the sources of uncertainty enables ModeRL's planning algorithm to escape local optimas induced by
# the mode constraint, by targeting exploration where the mode constraints /epistemic uncertainty/ is high.
# For planning, ModeRL optimises its closed-form objective, subject to its closed-form constraints, using
# Sequential Least Squares Quadratic Programming (SLSQP).

# In our experiments, we show the importance of disentangling sources of uncertainty in the mode constraint when using an uncertainty based exploration strategy.
# Further to this, we showed that by targeting exploration where the mode constraint's /epistemic uncertainty/ is high, enabled
# ModeRL to escape local omptimas induced by the mode constraint.

# can approximately solves the mode-constrained problem.

# We
# solves the $\delta\text{-mode-constrained}$ \acrshort{} problem.

# We show that disentangling sources of uncertainty in the mode constraints is
# enabling the planning algorithm to escape local optimas induced by the mode constraint, via an uncertainty-guided exploration strategy,


# by target exploration in regions of the state space where the
# mode constraints /epistemic uncertainty/ is high.


# We relaxed the infeasible mode constraint by introducing the notion of a $\delta\text{-mode-constrained}$ controlled system.

# Our main goal was to solve the mode-constrained RL problem in cref:eq-main-problem.
# Based on well-established methods from Bayesian statistics and machine learning, we have proposed ModeRL,
# a framework based on GPs for approximately solving the mode-constrained problem.

** old :noexport:
We have presented a novel model-based RL algorithm for environments with multimodal dynamics,
that can constrain the exploration to a desired dynamic modewith high probability.


Moreover, it has proposed how this exploration strategy can be combined with the dynamic model from cref:chap-dynamics,
the $\delta\text{-mode remaining}$ trajectory optimisation algorithms from cref:chap-traj-opt-control and the
$\delta\text{-mode remaining}$ chance constraints from cref:eq-mode-chance-constraint, to
approximately solve the mode remaining navigation problem in cref:eq-main-problem.

That is, it can control multimodal dynamical systems -- where the underlying dynamic modes and how the
system switches between them, are /not fully known a priori/ --
from a start state $\state_0$, to a target state $\targetState$ whilst guaranteeing
that the system remains in the desired dynamic modewith high probability.

The algorithm, named ModeRL, was tested in a simulated version of the illustrative quadcopter
example, verifying that the algorithm can work in practice.
However, it has not been fully tested in environments with more than two modes, nor has it been tested
on a real-world system.
Further testing and analysis of ModeRL is left for future work.



At the core of \acrshort{modeopt} is a Bayesian approach to learning multimodal dynamical systems, named \acrfull{mosvgpe},
that accurately identifies the underlying dynamic modes, as well as how the system switches between them.
Further to this, it learns informative /latent structure/ that \acrshort{modeopt} leverages to encode mode remaining behaviour
into control strategies.
The method's ability to learn factorised representations of multimodal data sets whilst retaining well-calibrated
uncertainty estimates was validated on a real-world quadcopter data set, as well as on the motorcycle data set.
Further to this, its applicability to learning dynamic models for model-based control
was validated in two simulated environments.

As this thesis has focused on model-based techniques that leverage a learned dynamic model,
it had to relax the requirement of remaining in the desired dynamic mode,
to remaining in the desired dynamic modewith high probability.
Initially, when not much of the environment has been observed, it is not possible to find trajectories to the target
state that remain in the desired mode with high probability.
This is due to the learned dynamic model having high /epistemic uncertainty/.
In this scenario, \acrshort{modeopt} reduces the model's /epistemic uncertainty/ by exploring the environment and
updating the dynamic model with new data.
\acrshort{modeopt} sidesteps the exploration-exploitation trade-off which is common in model-based RL algorithms by
introducing a set of chance constraints.
That is, \acrshort{modeopt} does not need an objective function that changes its exploration-exploitation balance as it gathers
more data.
The mode chance constraints are a powerful tool that allow \acrshort{modeopt} to deploy separate controllers during the
explorative and exploration phases.


* ACKNOWLEDGEMENTS :ignore:
#+BEGIN_EXPORT latex
\subsubsection*{Acknowledgements}
We thank ST John, Martin Trapp, Arno Solin, and Paul Chang for valuable discussions and feedback.
This work was conducted whilst Aidan Scannell was a PhD student at the EPSRC Centre for Doctoral
Training in Future Autonomous and Robotic Systems (FARSCOPE) at the Bristol Robotics Laboratory.
It was finished whilst funded by the Finnish Center for Artificial Intelligence (FCAI).
#+END_EXPORT
# All acknowledgments go at the end of the paper, including thanks to reviewers who gave useful comments, to colleagues who contributed to the ideas, and to funding agencies and corporate sponsors that provided financial support.
# To preserve the anonymity, please include acknowledgments \emph{only} in the camera-ready papers.
* BIBLIOGRAPHY :ignore:
#+BEGIN_EXPORT latex
\subsubsection*{References}
\printbibliography[heading=none]
#+END_EXPORT

* SUPPLEMENT :ignore:
** maths :ignore:
#+BEGIN_EXPORT latex
\renewcommand{\gatingKernelnM}{\ensuremath{\mode{\hat{k}}(\singleInput, \gatingInducingInput)}}
\renewcommand{\gatingKernelMM}{\ensuremath{\mode{\hat{k}}(\gatingInducingInput, \gatingInducingInput)}}
\renewcommand{\gatingKernelMn}{\ensuremath{\mode{\hat{k}}(\gatingInducingInput, \singleInput)}}
\renewcommand{\gatingKernelnn}{\ensuremath{\mode{\hat{k}}(\singleInput, \singleInput)}}

\renewcommand{\expertKernelnM}{\ensuremath{\mode{k}(\singleInput, \gatingInducingInput)}}
\renewcommand{\expertKernelMM}{\ensuremath{\mode{k}(\gatingInducingInput, \gatingInducingInput)}}
\renewcommand{\expertKernelMn}{\ensuremath{\mode{k}(\gatingInducingInput, \singleInput)}}
\renewcommand{\expertKernelnn}{\ensuremath{\mode{k}(\singleInput, \singleInput)}}
#+END_EXPORT

** intro :ignore:
#+BEGIN_EXPORT latex
%\clearpage
\appendix

\onecolumn
%\aistatstitle{Mode-Constrained Model-Based Reinforcement Learning via \\Gaussian Processes: Supplementary Materials}
%\thispagestyle{empty}
#+END_EXPORT

** DYNAMIC MODEL label:sec-dynamics
This section provides details of our nonparametric dynamic model, including the sparse GP approximations it is
built upon, our GP-based gating network, and how we propagate uncertainty when making multi-step predictions.

*** Sparse Gaussian Processes label:sec-sparse-approximations
Each dynamic mode's predictions conditioned on its inducing variables follows from the properties of multivariate normals
and are given by,
#+BEGIN_EXPORT latex
\begin{subequations}  \label{eq-sparse-gp-methodologies}
\begin{align}
&\singleExpertGivenInducing = \E_{\singleLatentExpertGivenInducing} \left[ \singleExpertLikelihood \right] \\
&\singleLatentExpertGivenInducing = \mathcal{N}\left( \mode{\latentFunc}(\singleInput) \mid
\expertKernelnM \expertKernelMM^{-1} \expertInducingOutput,
\expertKernelnn - \expertKernelnM \expertKernelMM^{-1} \expertKernelMn \right),
\end{align}
\end{subequations}
%\normalsize
#+END_EXPORT
Similarly for the gating network we have,
#+BEGIN_EXPORT latex
\begin{subequations} \label{eq-sparse-gp-methodologies-gating}
\begin{align}
\singleGatingGivenInducing &= \E_{\singleLatentGatingsGivenInducing} \left[ \singleGatingLikelihood \right] \\
\singleLatentGatingsGivenInducing &= \prod_{\modeInd=1}^{\ModeInd} \mathcal{N}\left( \mode{\gatingFunc}(\singleInput) \mid
\gatingKernelnM \gatingKernelMM^{-1} \gatingInducingOutput,
\gatingKernelnn - \gatingKernelnM \gatingKernelMM^{-1} \gatingKernelMn \right),
\end{align}
\end{subequations}
#+END_EXPORT

*Predictive posteriors*
As each GP's inducing variables are normally distributed, the functional form of their predictive posteriors are given by,
#+BEGIN_EXPORT latex
\begin{align}
\label{eq--variational-posteriors-functional-experts}
p(\mode{\latentFunc}(\singleInput) \mid \singleInput, \dataset_{0:i})
&\approx \int p(\mode{\latentFunc}(\singleInput) \mid \mode{\latentFunc}(\expertInducingInput))
q(\mode{\latentFunc}(\expertInducingInput)) \text{d} \mode{\latentFunc}(\expertInducingInput)
= \mathcal{N} \left( \mode{\latentFunc}(\singleInput) \mid
\mode{\mathbf{A}} \mode{\mathbf{m}},
\expertKernelnn
+ \mode{\mathbf{A}}
(\mode{\mathbf{S}} - \expertKernelMM)
\mode{\mathbf{A}}^T
\right) \\
p(\GatingFunc(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i})
&\approx \prod_{\modeInd=1}^\ModeInd q(\mode{\gatingFunc}(\gatingInducingInput))
= \prod_{\modeInd=1}^\ModeInd \mathcal{N} \left( \mode{\gatingFunc}(\singleInput) \mid
\mode{\hat{\mathbf{A}}} \mode{\hat{\mathbf{m}}},
\gatingKernelnn
+ \mode{\hat{\mathbf{A}}}
(\mode{\hat{\mathbf{S}}} - \gatingKernelMM)
\mode{\hat{\mathbf{A}}}^T
\right), \label{eq--variational-posteriors-functional-gating}
\end{align}
#+END_EXPORT
# $\mode{\mathbf{A}} = \expertKernel(\singleInput, \expertInducingInput)\left(\expertKernel(\expertInducingInput, \expertInducingInput)\right)^{-1}$ and
where
$\mode{\mathbf{A}} = \expertKernelnM \expertKernelMM^{-1}$ and
$\mode{\hat{\mathbf{A}}} = \gatingKernelnM \gatingKernelMM^{-1}$.
Importantly, our predictive posteriors marginalise the inducing variables in closed form, with Gaussian convolutions.

Given our GP-based gating network, we are able to model the joint distribution over the gating function values $\desiredGatingFunction(\stateTraj)$
along a trajectory $\stateTraj$ with,
#+BEGIN_EXPORT latex
\begin{align}
p(\desiredGatingFunction(\stateTraj) \mid \stateTraj, \dataset_{0:i})
&\approx q(\desiredGatingFunction(\stateTraj))
= \mathcal{N} \left(\desiredGatingFunction(\stateTraj) \mid \desiredGatingMeanFunc(\stateTraj),
\desiredGatingCovFunc(\stateTraj, \stateTraj) \right)
\end{align}
#+END_EXPORT
where $\desiredGatingMeanFunc(\cdot)$ and $\desiredGatingCovFunc(\cdot, \cdot)$ are
sparse GP mean and covariance functions, given by,
#+BEGIN_EXPORT latex
%\small
\begin{align}  \label{eq-gating-predictive-eqns}
\desiredGatingMeanFunc(\stateTraj) &=
\desiredGatingKernel(\stateTraj, \gatingInducingInput)
\desiredGatingKernel(\gatingInducingInput, \gatingInducingInput)^{-1}
\hat{\mathbf{m}}_{\desiredMode}  \\
\desiredGatingCovFunc(\stateTraj, \stateTraj) &=
\desiredGatingKernel(\stateTraj, \stateTraj)
+ \desiredGatingKernel(\stateTraj, \gatingInducingInput)
\desiredGatingKernel(\gatingInducingInput, \gatingInducingInput)^{-1}
\left( \hat{\mathbf{S}}_{\desiredMode}
- \desiredGatingKernel(\gatingInducingInput, \gatingInducingInput) \right)
\desiredGatingKernel(\gatingInducingInput, \gatingInducingInput)^{-1}
\desiredGatingKernel(\gatingInducingInput, \stateTraj), \label{eq-joint-gating-covariance}
\end{align}
%\normalsize
#+END_EXPORT
where $\desiredGatingKernel$ and
$\gatingInducingInput$ are the kernel and inducing inputs
associated with the desired mode's gating function respectively.
This sparse approximation arises because our dynamical model uses sparse GPs and approximates the posterior with,
#+BEGIN_EXPORT latex
\begin{align}
q(\desiredGatingFunction(\stateTraj)) = \int p(\desiredGatingFunction(\stateTraj) \mid \desiredGatingFunction(\gatingInducingInput))
q(\desiredGatingFunction(\gatingInducingInput))
\text{d} \desiredGatingFunction(\gatingInducingInput),
\end{align}
#+END_EXPORT
where
$q(\desiredGatingFunction(\gatingInducingInput)) = \mathcal{N}\left( \desiredGatingFunction(\gatingInducingInput \mid \hat{\mathbf{m}}_{\desiredMode}, \hat{\mathbf{S}}_{\desiredMode} \right)$.

*** Gating Network label:sec-gating-network-app
*Bernoulli ($\ModeInd=2$)* Instantiating the model with two dynamic modes, $\singleModeVar \in \{1, 2\}$, is a special case
where only a single gating function is needed.
This is because the output of a function $\gatingFunc(\singleInput)$ can be mapped through a sigmoid
function $\text{sig} : \R \rightarrow [0, 1]$ and interpreted as a probability,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-sigmoid}
\Pr(\singleModeVar=1 \mid \gatingFunc(\singleInput)) = \text{sig}(\gatingFunc(\singleInput)).
\end{align}
#+END_EXPORT
If this sigmoid function satisfies the point symmetry condition then
the following holds,
$\Pr(\singleModeVar=2 \mid \gatingFunc(\singleInput)) = 1 - \Pr(\singleModeVar=1 \mid \gatingFunc(\singleInput))$.
This only requires a single gating function and no normalisation term needs to be calculated.
If the sigmoid function in cref:eq-sigmoid is selected
to be the Gaussian cumulative distribution function
$\Phi(\gatingFunc(\cdot)) = \int^{\gatingFunc(\cdot)}_{-\infty} \mathcal{N}(\tau | 0, 1) \text{d} \tau$,
then the mode probability can be calculated in closed form,
# then the integral in cref:eq-gating-posterior can be calculated in closed-form,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-mixing-probabilities-analytic}
\Pr(\modeVar_{\timeInd}=1 \mid \singleInput) &=
 \int \Phi\left(\gatingFunc(\singleInput)\right) \mathcal{N}\left(\gatingFunc(\singleInput) \mid \mu_h, \sigma^2_h \right) \text{d} \gatingFunc(\singleInput) \nonumber \\
&= \Phi \left(\frac{\mu_{h}}{\sqrt{1 + \sigma^2_{h} }}\right),
\end{align}
#+END_EXPORT
where $\mu_h$ and $\sigma^2_{h}$ represent the mean and variance of the gating GP at $\singleInput$ respectively.

*Softmax ($\ModeInd>2$)* In the general case, when there are more than two dynamic modes,
the gating network's likelihood is defined as the Softmax function,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-softmax}
\singleGatingLikelihood = \text{softmax}_{\modeInd}\left(\GatingFunc(\singleInput)\right) = \frac{\text{exp}\left(\mode{\gatingFunc}(\singleInput)\right)}{\sum_{j=1}^{\ModeInd} \text{exp}\left(\gatingFunc_j(\singleInput) \right)}.
\end{align}
#+END_EXPORT
Each mode's mode probability $\Pr(\modeVar_{\timeInd}=\modeInd \mid \singleInput)$ is then obtained by marginalising
*all* of the gating functions.
In the general case where $\singleGatingLikelihood$ uses the softmax function in
cref:eq-softmax, this integral is intractable, so we approximate it with Monte Carlo quadrature.

# Each gating function $\mode{\gatingFunc}$ describes how its corresponding mode's mixing
# probability varies over the input space.
# Modelling the gating network with input-dependent functions enables
# informative prior knowledge to be encoded through the placement of GP priors on each gating function.
# Further to this, if the modes are believed to only vary over a subset of the state-action input space,
# then the gating functions can depend only on this subset.

*** sparse graphical model :ignore:
#+BEGIN_EXPORT latex
\begin{figure}
\centering
   \resizebox{0.6\columnwidth}{!}{
    \begin{tikzpicture}[
    pre/.style={<-,shorten <=0.4pt,>=stealth',semithick},
    post/.style={->,shorten >=0.4pt,>=stealth',semithick}
    ]
        \node[const] (x) {$\singleInput$};
        \node[latent, left=of x, yshift=-1.4cm] (f) {$\mode{\latentFunc}(\singleInput)$};
        %\node[latent, right=of x, yshift=-1.7cm] (h) {${h}^{(k)}_n$};
        \node[latent, right=of x, yshift=-1.4cm, xshift=0.5cm] (h) {$\mode{\gatingFunc}(\singleInput)$};

        \node[latent, left=of f, xshift=0.4cm, yshift=0.6cm] (uk) {$\expertInducingOutput$};
        \node[latent, right=of h, xshift=-0.4cm, yshift=0.6cm] (uh) {$\gatingInducingOutput$};
        \node[const, left=of uk, xshift=0.4cm] (zk) {$\expertInducingInput$};
        \node[const, right=of uh, xshift=-0.4cm] (zh) {$\gatingInducingInput$};

        \node[const, left=of f, xshift=0.4cm, yshift=-0.4cm] (thetak) {$\expertParamsK$};
        \node[const, right=of h, xshift=-0.4cm, yshift=-0.4cm] (phik) {$\gatingParamsK$};

        \node[const, below=of thetak, yshift=0.4cm] (sigmak) {$\sigma_{\modeInd}$};

        \node[obs, right=of sigmak, yshift=0.cm, xshift=1.4cm] (y) {$\singleOutput$};
        %\node[latent, right=of y, below=of h] (a) {$\alpha_t$};
        \node[latent, right=of y, xshift=-0.4cm] (a) {$\modeVar_{\timeInd}$};

        %\node[obs, right=of sigmak] (y) {$\Delta\mathbf{x}_{t}$};

        \factor[above=of a] {h-a} {left:Cat} {h} {a};

        \draw[post] (a)--(y);
        \draw[post] (x)-|(f);
        %\draw[post] (f)--(yk);
        \draw[post] (f)--(y);
        %\draw[post] (yk)--(y);
        %\draw[post] (h)--(a);
        \draw[post] (x)-|(h);
        \draw[post] (uk)--(f);
        \draw[post] (uh)--(h);
        \draw[post] (zk)--(uk);
        \draw[post] (zh)--(uh);
        \draw[post] (thetak)--(f);
        \draw[post] (phik)--(h);
        \draw[post] (sigmak)|-(y);

        \plate {} {(x) (y) (a) (f) (h)} {$\TimeInd$};
        %\plate {} {(zk) (uk) (f) (sigmak) (thetak) (yk)} {$K$};
        \plate {} {(zk) (uk) (f) (sigmak) (thetak)} {$\ModeInd$};
        \plate {} {(uh) (h) (phik)} {$\ModeInd$};
    \end{tikzpicture}
    }
\caption{
Graphical model of our augmented dynamic model where each state difference output $\singleOutput$
is generated by mapping the state-action input $\singleInput$ through the latent processes.
The dynamic modes are shown on the left and the gating network on the right.
The generative processes involve evaluating the gating network and sampling a mode indicator variable $\singleModeVar$.
The indicated mode's latent function $\mode{f}$ and noise model $\mathcal{N}(0, \sigma_{\modeInd}^{2})$ are then evaluated to generate the output $\singleOutput$.}
\label{fig-graphical-model-sparse}
\end{figure}
#+END_EXPORT

*** Uncertainty Propagation: Moment Matching label:sec-moment-matching
To obtain the state distributions $p(\state_{1} \mid \state_{0}, \action_{0}, \mathcal{D}_{0:i}),\ldots, p(\state_{\TimeInd} \mid \state_{0}, \action_{0:\TimeInd-1}, \mathcal{D}_{0:i})$,
for a given set of actions $\action_{0},\ldots,\action_{\TimeInd-1}$,
we cascade single-step predictions through the desired dynamic mode's GP using moment matching,
[cite:@girardGaussian2003;@quinonero-candelaUnifying2005;@deisenrothPILCO2011;@kamtheDataEfficient2018].
That is, we iteratively calculate,
#+BEGIN_EXPORT latex
\begin{align}
p(\state_{\timeInd+1} \mid \state_{0}, \action_{0:\timeInd}, \mathcal{D}_{0:i}) = \int \int
\underbrace{p(\state_{\timeInd+1} \mid \dynamicsFunc_{\desiredMode}(\singleInput))}_{\text{Gaussian likelihood}}
\underbrace{p(\dynamicsFunc_{\desiredMode}(\singleInput) \mid \singleInput, \dataset_{0:i})}_{\text{dynamics posterior}}
p(\state_{\timeInd} \mid \state_{0}, \action_{0:\timeInd-1}, \mathcal{D}_{0:i})
\text{d} \dynamicsFunc_{\desiredMode}(\singleInput)
\text{d} \state_{\timeInd},
\end{align}
#+END_EXPORT
with $p(\state_{0}) = \delta(\state_{0})$.
Importantly, the moment-matching approximation allows us to formulate uncertainty propagation using a deterministic
system function,
#+BEGIN_EXPORT latex
\begin{align}
\mathbf{z}_{\timeInd+1} = \dynamicsFunc_{\desiredMode}^{\text{MM}}(\mathbf{z}_{\timeInd}, \action_{\timeInd}),
\quad \mathbf{z}_{\timeInd} = [\bm\mu_{\state_{\timeInd}}^{\text{MM}}, \bm\Sigma_{\state_{\timeInd}}^{\text{MM}}],
\end{align}
#+END_EXPORT
where $\bm\mu_{\state_{\timeInd}}^{\text{MM}}$ and $\bm\Sigma_{\state_{\timeInd}}^{\text{MM}}$ are the mean and covariance of
$p(\state_{\timeInd} \mid \state_{0}, \action_{0:\timeInd-1}, \mathcal{D}_{0:i})$.
As we use deterministic actions we define the deterministic system function as
#+BEGIN_EXPORT latex
\begin{align} \label{eq-moment-matching-system-function}
\mathbf{z}_{\timeInd+1} = \dynamicsFunc_{\desiredMode}^{\text{MM}}(\hat{\mathbf{z}}_{\timeInd}),
\quad \hat{\mathbf{z}}_{\timeInd} = [\bm\mu_{\hat{\state}_{\timeInd}}^{\text{MM}}, \bm\Sigma_{\hat{\state}_{\timeInd}}^{\text{MM}}],
\quad \bm\mu_{\hat{\state}_{\timeInd}}^{\text{MM}} = [\bm\mu_{\state_{\timeInd}}^{\text{MM}}, \action_{\timeInd}],
\quad \bm\Sigma_{\hat{\state}_{\timeInd}}^{\text{MM}} = \text{blkdiag}[\bm\Sigma_{\state_{\timeInd}}^{\text{MM}}, \bm0].
\end{align}
#+END_EXPORT
# $\hat{\bm\Sigma}_{\timeInd}^{\text{MM}}=\text{blkdiag[\bm\Sigma_{\timeInd}^{\text{MM}}, 0]}$.

# action-augmented distribution
# $p(\singleInput)=p(\state_{\timeInd}, \action_{\timeInd})$ as

# Note that we have simplified $\dynamicsFunc_{\desiredMode}(\singleInput) \sim p(\dynamicsFunc_{\desiredMode}(\singleInput) \mid \singleInput \dataset_{0:i})$

# In practice,
# over the state distribution obtained from cascading single-step predictions using moment matching,
# \todo{cite moment matching approximation}
# $q_{\mode{\theta}}(\state_{\timeInd+1}) = \int \dynamicsModelK(\state_{\timeInd+1} \mid \state_{\timeInd}, \action_{\timeInd}) q_{\mode{\theta}}(\state_{\timeInd}) \text{d} \state_{\timeInd}$,
# with $q_{\mode{\theta}}(\state_{0}) = \delta(\state_{0})$.

# where $q_{\mode{\theta}}(\dynamicsFunc_{\mode{\theta}}(\state_{\timeInd}, \action_{\timeInd})) = \int p_{\mode{\theta}}( \dynamicsFunc_{\mode{\theta}}(\state_{\timeInd}, \action_{\timeInd})  \mid \state_{\timeInd}, \action_{\timeInd}) q_{\mode{\theta}}(\state_{\timeInd}) \text{d} \state_{\timeInd}$

*** Mode constraint label:sec-mode-constraint :noexport:
To calculate the $\delta\text{-mode-constraint}$ in
$p(\state_{\timeInd+1} \mid \action_{0:\timeInd})$
#+BEGIN_EXPORT latex
\begin{align}
p(\gatingFunc(\state_{\timeInd}) \mid \state_{0}, \dataset_{0:i}) \approx
\int p(\gatingFunc(\state_{\timeInd}) \mid \state_{\timeInd}, \dataset_{0:i})
p(\state_{\timeInd} \mid \action_{0:\timeInd-1})
\text{d} \state_{\timeInd},
\end{align}
#+END_EXPORT

#+BEGIN_EXPORT latex
\begin{align}
\Pr(\modeVar_{\timeInd}=\desiredMode \mid \gatingFunc())
p(\state_{\timeInd} \mid \action_{0:\timeInd-1})
\text{d} \state_{\timeInd},
\end{align}
#+END_EXPORT

** ILLUSTRATIVE EXAMPLE label:illustrative_example
#+BEGIN_EXPORT latex
\begin{figure}[!t]
    \centering
    %\includegraphics[width=0.5\columnwidth]{./images/quadcopter-problem.png}
    \includegraphics[width=0.5\columnwidth]{./images/quadcopter-problem-blue-red.png}
    \caption{\label{fig-problem-statement} \textbf{Mode-constrained quadcopter navigation} -
    Diagram showing a top-down view of a quadcopter subject to two dynamic modes:
    1) an \textit{operable} dynamic mode (blue) and 2) an \textit{inoperable}, turbulent dynamic mode induced by a strong wind field (red).
    The goal is to navigate to the target state $\targetState$ (white star), whilst avoiding the turbulent dynamic mode (red).}
\end{figure}
#+END_EXPORT
ModeRL is tested on a 2D quadcopter navigation example shown in cref:fig-problem-statement.
The goal is to fly the quadcopter from an initial state $\state_0$, to a target state $\state_{f}$ (white star).
However, it considers a quadcopter operating in an environment subject to spatially varying wind --
induced by a fan -- where two dynamic modes can represent the system,
- Mode 1 :: is an /operable/ dynamic mode away from the fan,
- Mode 2 :: is an /inoperable/, turbulent dynamic mode in front of the fan.
The turbulent dynamic mode is subject to higher drift (in the negative $x$ direction) and
to higher diffusion (transition noise).
It is hard to know the exact turbulent dynamics due to complex and uncertain interactions between the
quadcopter and the wind field.
Further to this, controlling the system in the turbulent dynamic mode may be infeasible.
This is because the unpredictability of the turbulence may cause catastrophic failure.
Therefore, when flying the quadcopter to the target state $\state_{f}$,
it is desirable to find trajectories that avoid entering this turbulent dynamic mode.
# #+BEGIN_EXPORT latex
# \begin{figure}[!t]
# \centering
# \includegraphics[width=0.7\textwidth]{./images/quadcopter-problem.png}
# \caption[Quadcopter navigation problem]{\label{fig-problem-statement}\textbf{Quadcopter navigation problem}
# Diagram showing a top-down view of an environment, representing a quadcopter subject to two dynamic modes:
# 1) an \textit{operable} dynamic mode away from the fan (blue) and 2) an \textit{inoperable}, turbulent dynamic modein front of the fan (green).
# The goal is to find trajectories from a start state $\state_0$, to the target state $\targetState$ (red star),
# whilst avoiding the turbulent dynamic mode.}
# \end{figure}
# #+END_EXPORT
The state-space of the velocity-controlled quadcopter example consists of the 2D Cartesian coordinates $\state = (x, y)$
and the controls consist of the speed in each direction, given by $\control = (\velocity_x, \velocity_y)$.

The reward function is given by,
#+BEGIN_EXPORT latex
\begin{align} \label{eq-reward-func}
\rewardFunc(\bar{\state}, \bar{\action}) &=
-\left(\state_{\TimeInd} - \targetState \right)^{T}\mathbf{H} \left(\state_{\TimeInd}-\targetState \right)
- \sum_{\timeInd=0}^{\TimeInd-1} \bigg(
\left(\state_{\timeInd} - \targetState \right)^{T}\mathbf{Q} \left(\state_{\timeInd}-\targetState \right)
+\action_{\timeInd}^{T} \mathbf{R} \action_{\timeInd} \bigg)
\\
&=
- \|{\state_{\TimeInd}} - \targetState \|_{\mathbf{H}}
- \sum_{\timeInd=0}^{\TimeInd-1} \bigg( \|{\state_{\timeInd}} - \targetState \|_{\mathbf{Q}}
+ \|\control_{\timeInd} \|_{\mathbf{R}} \bigg)
\end{align}
#+END_EXPORT
where $\mathbf{H}$ and $\mathbf{Q}$ are user-defined, real symmetric positive semi-definite
weight matrices and $\mathbf{R}$ is a user-defined, positive definite weight matrix.
In our experiments we set both $\mathbf{Q}$ and $\mathbf{R}$ to be the identify matrix $\mathbf{I}$ and
$\mathbf{H}$ to be $100\mathbf{I}$.

** EXPERIMENT CONFIGURATION label:sec-experiment-configuration
This section details how the experiments were configured.

*Initial data set $\dataset_0$*
The initial data set was collected by simulating $50$ random trajectories with horizon $\TimeInd=15$ from
the start state $\state_0 = \{x_{0}, y_{0}\}$ and
terminating them when they left the initial state domain $\initialStateDomain = \{\state \in \stateDomain \mid x_{0}-1 < x < x_{0}+1, y_{0}-1 < y < y_{0}+1 \}$.

*Model learning*
In all experiments, the dynamic model was instantiated with $\ModeInd=2$ modes.
Each dynamic mode's GP used a separate RBF kernel with automatic relevance determination (ARD) for each output dimension $d$ but
shared its inducing variables for each output dimension $d$.
Further to this, each dynamic mode learned a separate constant mean function $c_{\latentFunc_{\modeInd}}$ and separate noise variances for each output dimension.
The gating network used a single gating function with an RBF kernel with ARD and a zero mean function.
An early stopping callback was used to terminate the dynamic model's training.
The early stopping callback used a min delta of $0$ and a patience of $50$.
This meant that training terminated after 50 epochs of no improvement.
All of the dynamic model's initial parameters are shown in cref:tab-params-quadcopter.
We also fix the kernel hyperparameters (e.g. lengthscale and signal variance) associated with the gating network, along with
the noise variance in the non-desired (turbulent) dynamic mode.


*Policy*
\todo[inline]{update $\delta$ here and in table}
In all experiments, ModeRL used a horizon of $\TimeInd=15$ and was configured with $\delta=0.3$.
At each episode, ModeRL uses the previous solution as the initial solution for the trajectory optimiser.

*Intrinsic schedule*
In our experiments, we used an exponential decay schedule (with decay rate $0.96$ and decay steps $10$) on the exploration weight $\beta$.

#+CAPTION: Experiment configuration and parameter settings.
#+LABEL: tab-params-quadcopter
#+ATTR_LATEX: :center t
|                           | *DESCRIPTION*                | *SYMBOL*                  | *VALUE*                                                                                   |
|---------------------------+------------------------------+---------------------------+-------------------------------------------------------------------------------------------|
|                           | Start state                  | $\state_{0}$              | $[2.0, -2.5]$                                                                             |
| Environment               | Target state                 | $\targetState$            | $[1.2, 3.0]$                                                                              |
|                           | Terminal state reward weight | $\mathbf{H}$              | $\diag([100, 100])$                                                                       |
|                           | State reward weight          | $\mathbf{Q}$              | $\diag([1, 1])$                                                                           |
|                           | Control reward weight        | $\mathbf{R}$              | $\diag([1, 1])$                                                                           |
|---------------------------+------------------------------+---------------------------+-------------------------------------------------------------------------------------------|
|                           | Constraint level             | $\delta$                       | $0.3$                                                                                     |
| Policy $\pi$                | Horizon                      | $\TimeInd$                | $15$                                                                                      |
|                           | Exploration weight (initial) | $\beta_{0}$                   | $10.0$                                                                                    |
|---------------------------+------------------------------+---------------------------+-------------------------------------------------------------------------------------------|
|                           | Batch size                   | $\NumData_b$              | $64$                                                                                      |
|                           | Num epochs                   | N/A                       | $20000$                                                                                   |
| Dynamics optimiser        | Num gating samples           | N/A                       | $1$                                                                                       |
|                           | Num expert samples           | N/A                       | $1$                                                                                       |
|                           | Learning rate                | N/A                       | $0.01$                                                                                    |
|                           | Epsilon                      | N/A                       | $1\times 10^{-8}$                                                                              |
|---------------------------+------------------------------+---------------------------+-------------------------------------------------------------------------------------------|
|                           | Constant mean function       | $c_{f_{1}}$               | $[0, 0]$                                                                                  |
|                           | Kernel variance ($d=1$)      | $\sigma^{2}_{f_{1}}$           | $1$                                                                                       |
|                           | Kernel lengthscales  ($d=1$) | $l_{f_{1}}$               | $[1, 1, 1, 1]$                                                                            |
| Dynamic mode 1 $f_{1}$    | Kernel variance ($d=2$)      | $\sigma^{2}_{f_{1}}$           | $1$                                                                                       |
|                           | Kernel lengthscales ($d=2$)  | $l_{f_{1}}$               | $[1, 1, 1, 1]$                                                                            |
|                           | Likelihood variance          | $\sigma^{2}_{1}$               | $\diag([1, 1])$                                                                           |
|                           | Num inducing points          | $\NumInducing_{f_{1}}$    | $50$                                                                                      |
|                           | Inducing inputs              | $\expertsInducingInput_1$ | $\expertsInducingInput_1 \subseteq \allInput_{0}$ with $\#\expertsInducingInput_1 = \NumInducing$ |
|---------------------------+------------------------------+---------------------------+-------------------------------------------------------------------------------------------|
|                           | Constant mean function       | $c_{f_{2}}$               | $[0,0]$                                                                                   |
|                           | Kernel variance ($d=1$)      | $\sigma^{2}_{f_{2}}$           | $1$                                                                                       |
|                           | Kernel lengthscales  ($d=1$) | $l_{f_{2}}$               | $[1, 1, 1,1]$                                                                             |
| Dynamic mode 2 $f_{2}$    | Kernel variance ($d=2$)      | $\sigma^{2}_{f_{2}}$           | $1$                                                                                       |
|                           | Kernel lengthscales ($d=2$)  | $l_{f_{2}}$               | $[1, 1, 1,1]$                                                                             |
|                           | Likelihood variance          | $\sigma^{2}_{2}$               | $\diag([1,1])$                                                                            |
|                           | Num inducing points          | $\NumInducing_{f_{2}}$    | $50$                                                                                      |
|                           | Inducing inputs              | $\expertsInducingInput_2$ | $\expertsInducingInput_2 \subseteq \allInput_{0}$ with $\#\expertsInducingInput_2 = \NumInducing$ |
|---------------------------+------------------------------+---------------------------+-------------------------------------------------------------------------------------------|
|                           | Kernel variance              | $\sigma^{2}_{h_{1}}$           | $1$                                                                                       |
|                           | Kernel lengthscales          | $l_{h_{1}}$               | $[0.8, 0.8]$                                                                              |
| Gating function 1 $h_{1}$ | Active dims                  | N/A                       | $[0, 1]$                                                                                  |
|                           | Num inducing points          | $\NumInducing_{h_{1}}$    | $90$                                                                                      |
|                           | Inducing inputs              | $\gatingInducingInput$    | $\gatingInducingInput \subseteq \allInput_{0}$ with $\#\gatingInducingInput = \NumInducing_{h}$   |

\newpage

** FURTHER EXPERIMENTS
#+begin_export latex
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{../experiments/figures/num_constrint_violations_constraint_levels_ablation.pdf}
    %\caption{\textbf{Constraint level ablation} Shows how the constraint level $\delta$ influences the number of constraint violations during training. Specifically, it shows the accumulated number of constraint violations ($N^{\modeVar}_{i}$) at each episode ($i$) during training, for five constraint levels $\delta$.
    %Results show mean and 95\% confidence interval for five random seeds.}
    \caption{\textbf{Constraint level ablation} Accumulated number of constraint violations at each episode $i$ of training, for different constraint levels $\delta$.
    It shows that tighter constraints (i.e. lower $\delta$) result in less constraint violations during training.
    Lines show the mean and 95\% confidence interval of the accumulated number of constraint violations for five random seeds, at each episode $i$ of training. The $\delta^{s}_{0}=4$ experiment used an exponential schedule to tighten the constraint during training. }
    \label{fig-num_constrint_violations_constraint_levels_ablation}
\end{figure}
#+end_export
