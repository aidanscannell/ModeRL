input_dim = 1
output_dim = 1
epochs = 8000
batch_size = 30
num_inducing_samples = 4 # num samples to integrate inducing outputs variational dists in ELBO
num_experts = 3
fast_tasks_period = 10 # write loss and model params to tensorboard if epoch_num % fast_tasks_period == 0
slow_tasks_period = 500 # plot model (images) to tensorboard if epoch_num % slow_tasks_period == 0
logging_epoch_freq = 100 # print loss (ELBO) if epoch_num % logging_epoch_freq == 0
log_dir = "../logs/mcycle/three_experts"
num_ckpts = 5 # max number of checkpoints to store (model is saved in log_dir @ logging_epoch_freq). Remove this line to disable checkpointing

[[experts]]
    name = "expert_1" # specify a name
    mean_function = "constant" # mean function e.g. options are "constant" or "zero"
    whiten = true # boolean, if true, use whitened representation of inducing points
    [experts.kernel]
        name = "rbf" # kernel type e.g. "rbf", "cosine", "sum", "product" 
        [experts.kernel.params] # set required parameters for the kernel type
            lengthscale = 10.0 # RBF lengthscale 
            variance = 0.1 # RBF signal variance
    [experts.likelihood]
        name = "gaussian" # likelihood type, "gaussian" is only valid option
        [experts.likelihood.params] # set required parameters for the likelihood type
            variance = 0.0011 # Guassian noise variance
    [experts.inducing_points]
        num_inducing = 30
        q_mu.mean = 0.0 # mean of variational Gaussian posterior, will be broadcast to correct shape
        q_mu.var = 2.0 # add Gaussian noise to mean of variational Gaussian posterior to aid training N(0,q_mu.var)
        q_sqrt = 1.0 # cholesky of covariance of variational Gaussian posterior, will be multiplied by identity of correct shape
        q_diag = false # boolean, if true, approximate covariance by diagonal matrix

[[experts]]
    name = "expert_2" # specify a name
    mean_function = "constant" # mean function e.g. options are "constant" or "zero"
    whiten = true # boolean, if true, use whitened representation of inducing points
    [experts.kernel]
        name = "rbf" # kernel type e.g. "rbf", "cosine", "sum", "product" 
        [experts.kernel.params] # set required parameters for the kernel type
            lengthscale = 0.5 # RBF lengthscale 
            variance = 20.0 # RBF signal variance
    [experts.likelihood]
        name = "gaussian" # likelihood type, "gaussian" is only valid option
        [experts.likelihood.params] # set required parameters for the likelihood type
            variance = 0.9 # Guassian noise variance
    [experts.inducing_points]
        num_inducing = 30
        q_mu.mean = 0.0 # mean of variational Gaussian posterior, will be broadcast to correct shape
        q_mu.var = 2.0 # add Gaussian noise to mean of variational Gaussian posterior to aid training N(0,q_mu.var)
        q_sqrt = 1.0 # cholesky of covariance of variational Gaussian posterior, will be multiplied by identity of correct shape
        q_diag = false # boolean, if true, approximate covariance by diagonal matrix

[[experts]]
    name = "expert_3" # specify a name
    mean_function = "constant" # mean function e.g. options are "constant" or "zero"
    whiten = true # boolean, if true, use whitened representation of inducing points
    [experts.kernel]
        name = "rbf" # kernel type e.g. "rbf", "cosine", "sum", "product" 
        [experts.kernel.params] # set required parameters for the kernel type
            lengthscale = 0.5 # RBF lengthscale 
            variance = 20.0 # RBF signal variance
    [experts.likelihood]
        name = "gaussian" # likelihood type, "gaussian" is only valid option
        [experts.likelihood.params] # set required parameters for the likelihood type
            variance = 0.9 # Guassian noise variance
    [experts.inducing_points]
        num_inducing = 30
        q_mu.mean = 0.0 # mean of variational Gaussian posterior, will be broadcast to correct shape
        q_mu.var = 2.0 # add Gaussian noise to mean of variational Gaussian posterior to aid training N(0,q_mu.var)
        q_sqrt = 1.0 # cholesky of covariance of variational Gaussian posterior, will be multiplied by identity of correct shape
        q_diag = false # boolean, if true, approximate covariance by diagonal matrix

[[gating_functions]]
    mean_function = "zero" # mean function e.g. options are "constant" or "zero"
    whiten = true # boolean, if true, use whitened representation of inducing points
    [gating_functions.kernel]
        name = "rbf" # kernel type e.g. "rbf", "cosine", "sum", "product" 
        [gating_functions.kernel.params] # set required parameters for the kernel type
            lengthscale = 0.5 # RBF lengthscale 
            variance = 3.0 # RBF signal variance
    [gating_functions.inducing_points]
        num_inducing = 30
        q_mu.mean = 0.0 # mean of variational Gaussian posterior, will be broadcast to correct shape
        q_mu.var = 10.0 # add Gaussian noise to mean of variational Gaussian posterior to aid training N(0,q_mu.var)
        q_sqrt = 10.0 # cholesky of covariance of variational Gaussian posterior, will be multiplied by identity of correct shape
        q_diag = false # boolean, if true, approximate covariance by diagonal matrix

[[gating_functions]]
    mean_function = "zero" # mean function e.g. options are "constant" or "zero"
    whiten = true # boolean, if true, use whitened representation of inducing points
    [gating_functions.kernel]
        name = "rbf" # kernel type e.g. "rbf", "cosine", "sum", "product" 
        [gating_functions.kernel.params] # set required parameters for the kernel type
            lengthscale = 0.5 # RBF lengthscale 
            variance = 3.0 # RBF signal variance
    [gating_functions.inducing_points]
        num_inducing = 30
        q_mu.mean = 0.0 # mean of variational Gaussian posterior, will be broadcast to correct shape
        q_mu.var = 10.0 # add Gaussian noise to mean of variational Gaussian posterior to aid training N(0,q_mu.var)
        q_sqrt = 10.0 # cholesky of covariance of variational Gaussian posterior, will be multiplied by identity of correct shape
        q_diag = false # boolean, if true, approximate covariance by diagonal matrix

[[gating_functions]]
    mean_function = "zero" # mean function e.g. options are "constant" or "zero"
    whiten = true # boolean, if true, use whitened representation of inducing points
    [gating_functions.kernel]
        name = "rbf" # kernel type e.g. "rbf", "cosine", "sum", "product" 
        [gating_functions.kernel.params] # set required parameters for the kernel type
            lengthscale = 0.5 # RBF lengthscale 
            variance = 3.0 # RBF signal variance
    [gating_functions.inducing_points]
        num_inducing = 30
        q_mu.mean = 0.0 # mean of variational Gaussian posterior, will be broadcast to correct shape
        q_mu.var = 10.0 # add Gaussian noise to mean of variational Gaussian posterior to aid training N(0,q_mu.var)
        q_sqrt = 10.0 # cholesky of covariance of variational Gaussian posterior, will be multiplied by identity of correct shape
        q_diag = false # boolean, if true, approximate covariance by diagonal matrix
